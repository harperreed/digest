This file is a merged representation of a subset of the codebase, containing specifically included files and files not matching ignore patterns, combined into a single document by Repomix.
The content has been processed where empty lines have been removed, line numbers have been added.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of a subset of the repository's contents that is considered the most important context.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: **/*
- Files matching these patterns are excluded: **/*.log, **/uv.lock, **/package-lock.json, **/.env, **/Cargo.lock, **/node_modules, **/target, **/dist, **/build, **/output.txt, **/yarn.lock, **/uv.lock, **/package-lock.json, **/.env, **/Cargo.lock, **/node_modules, **/target, **/dist, **/build, **/output.txt, **/yarn.lock, **/.private-journal
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Empty lines have been removed from all files
- Line numbers have been added to the beginning of each line
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.github/
  workflows/
    ci.yml
    release.yml
cmd/
  digest/
    export.go
    feed.go
    folder.go
    list.go
    main.go
    markread.go
    mcp.go
    open.go
    read.go
    root.go
    sync.go
    version.go
docs/
  plans/
    2025-12-10-digest-design.md
    2025-12-10-digest-implementation.md
internal/
  content/
    content_test.go
    content.go
  db/
    db_test.go
    db.go
    entries_test.go
    entries.go
    feeds_test.go
    feeds.go
  discover/
    discover_test.go
    discover.go
  fetch/
    fetch_test.go
    fetch.go
  mcp/
    prompts.go
    resources.go
    server.go
    tools.go
  models/
    entry.go
    feed_test.go
    feed.go
  opml/
    opml_test.go
    opml.go
  parse/
    parse_test.go
    parse.go
  timeutil/
    timeutil_test.go
    timeutil.go
test/
  integration_test.go
.gitignore
.golangci.yml
.goreleaser.yml
.pre-commit-config.yaml
code-review.md
go.mod
Makefile
README.md
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="code-review.md">
  1: ## Code Review: RSS/Atom Feed Tracker with MCP Integration
  2:
  3: This codebase implements a comprehensive RSS/Atom feed management system with both CLI and MCP (Model Context Protocol) interfaces. The implementation is well-structured and follows Go best practices. Below is my detailed review:
  4:
  5: ### Overall Architecture ✅
  6:
  7: **Strengths:**
  8: - Clean separation of concerns with distinct packages for models, database, parsing, fetching, etc.
  9: - OPML as source of truth for subscriptions, SQLite for content and state - excellent design decision
 10: - MCP integration provides powerful AI agent capabilities
 11: - Comprehensive CLI with intuitive commands and good UX
 12:
 13: ### Code Quality Assessment
 14:
 15: #### Models Layer (`internal/models/`) ✅
 16: **Lines 1-46 in `entry.go` and `feed.go`:**
 17: - Good use of pointers for optional fields
 18: - Proper UUID generation and timestamping
 19: - Clean separation between creation and state management methods
 20:
 21: #### Database Layer (`internal/db/`) ⚠️
 22:
 23: **Strengths:**
 24: - Proper foreign key constraints and indexes
 25: - Good use of prepared statements
 26: - Comprehensive CRUD operations
 27:
 28: **Issues:**
 29: 1. **Line 33-41 in `feeds.go`:** SQL injection prevention is good with escaped wildcards, but the ESCAPE clause syntax could be more portable
 30: 2. **Line 212-228 in `entries.go`:** `MarkEntriesReadBefore` could benefit from a transaction to ensure atomicity
 31: 3. **Line 117-151 in `entries.go`:** Complex query building in `ListEntries` - consider using a query builder for maintainability
 32:
 33: #### Feed Discovery (`internal/discover/`) ✅
 34: **Lines 43-79 in `discover.go`:**
 35: - Excellent progressive discovery strategy (direct feed → HTML parsing → common paths)
 36: - Good error handling with specific error types
 37: - Security-conscious URL validation
 38:
 39: #### HTTP Fetching (`internal/fetch/`) ✅
 40: **Lines 25-62 in `fetch.go`:**
 41: - Proper HTTP caching implementation with ETag/Last-Modified
 42: - Good timeout handling (30 seconds)
 43: - Clean conditional request logic
 44:
 45: #### OPML Handling (`internal/opml/`) ⚠️
 46:
 47: **Strengths:**
 48: - Comprehensive folder and feed management
 49: - Good round-trip XML serialization
 50:
 51: **Issues:**
 52: 1. **Line 156-162 in `opml.go`:** `AddFeed` checks for existing feeds by iterating all feeds - O(n) operation that could be optimized with a map
 53: 2. **Line 194-215 in `opml.go`:** `MoveFeed` has complex logic that could be simplified
 54:
 55: #### CLI Commands (`cmd/digest/`) ✅
 56:
 57: **Strengths:**
 58: - Excellent use of Cobra framework
 59: - Good flag handling and validation
 60: - Beautiful colored output with `fatih/color`
 61: - Proper error messaging
 62:
 63: **Line 88-95 in `read.go`:** Good markdown rendering with graceful fallback
 64:
 65: #### MCP Integration (`internal/mcp/`) ✅
 66:
 67: **Outstanding implementation:**
 68: - **Lines 143-177 in `tools.go`:** Comprehensive tool descriptions with examples
 69: - **Lines 26-98 in `resources.go`:** Well-structured JSON responses with metadata
 70: - **Lines 25-155 in `prompts.go`:** Detailed workflow templates - excellent UX for AI agents
 71:
 72: ### Security Considerations ✅
 73:
 74: **Good practices:**
 75: - **Line 29-34 in `open.go`:** URL validation before opening browser
 76: - **Line 391-400 in `tools.go`:** MCP tool input validation
 77: - **Line 14 in `db.go`:** Restrictive directory permissions (0700)
 78:
 79: ### Testing Coverage ✅
 80:
 81: **Strengths:**
 82: - Comprehensive unit tests for all major components
 83: - Integration tests with real feeds
 84: - Good use of temporary directories and cleanup
 85: - **Lines 198-245 in `test/integration_test.go`:** Realistic caching test scenarios
 86:
 87: ### Performance Considerations ⚠️
 88:
 89: **Areas for improvement:**
 90: 1. **Line 319-385 in `resources.go`:** `calculateStats` makes multiple database queries - could be optimized with JOINs
 91: 2. **No pagination in list operations** - could be problematic with large feed collections
 92:
 93: ### Error Handling ✅
 94:
 95: **Excellent error handling throughout:**
 96: - Proper error wrapping with `fmt.Errorf`
 97: - Graceful degradation (e.g., markdown rendering fallback)
 98: - Good error context in MCP responses
 99:
100: ### Configuration and Deployment ⚠️
101:
102: **Issues:**
103: 1. **Line 17-21 in `.goreleaser.yml`:** Only builds for macOS due to CGO - limits distribution
104: 2. **Missing configuration file support** - all settings are command-line flags
105:
106: ### Documentation ✅
107:
108: **Strengths:**
109: - Excellent README with comprehensive examples
110: - Good ABOUTME comments throughout codebase
111: - Detailed MCP tool descriptions
112:
113: ### Recommendations
114:
115: #### High Priority:
116: 1. **Add transaction support** for bulk operations in database layer
117: 2. **Optimize OPML operations** with better data structures for large feed lists
118: 3. **Add pagination** to list commands and MCP tools
119:
120: #### Medium Priority:
121: 1. **Add configuration file support** (YAML/TOML)
122: 2. **Implement feed health monitoring** with automatic retry logic
123: 3. **Add cross-platform builds** or document Linux compilation steps
124:
125: #### Low Priority:
126: 1. **Add feed import from other formats** (JSON feeds, etc.)
127: 2. **Implement feed categorization suggestions** based on content analysis
128: 3. **Add export formats** beyond OPML
129:
130: ### Verdict: ⭐⭐⭐⭐⭐ (5/5)
131:
132: This is an exceptionally well-crafted codebase that demonstrates:
133: - Clean architecture and separation of concerns
134: - Comprehensive feature set with excellent UX
135: - Innovative MCP integration for AI agents
136: - Good testing practices and error handling
137: - Professional-grade code organization
138:
139: The few issues identified are minor and don't detract from the overall quality. This codebase would serve as an excellent reference implementation for RSS feed management and MCP integration patterns.
140:
141: **Particular highlights:**
142: - The MCP prompt templates are brilliantly designed for AI workflow automation
143: - HTTP caching implementation is textbook-perfect
144: - CLI UX is outstanding with helpful error messages and colored output
145: - Database schema is well-normalized with proper constraints
146:
147: **Ready for production deployment with the noted optimizations as future enhancements.**
</file>

<file path=".github/workflows/release.yml">
 1: name: Release
 2: on:
 3:   push:
 4:     tags:
 5:       - 'v*'
 6: permissions:
 7:   contents: write
 8:   packages: write
 9: jobs:
10:   release:
11:     name: Release
12:     runs-on: macos-latest
13:     steps:
14:       - name: Checkout code
15:         uses: actions/checkout@v4
16:         with:
17:           fetch-depth: 0
18:       - name: Set up Go
19:         uses: actions/setup-go@v5
20:         with:
21:           go-version: '1.24'
22:           cache: true
23:       - name: Run GoReleaser
24:         uses: goreleaser/goreleaser-action@v6
25:         with:
26:           distribution: goreleaser
27:           version: latest
28:           args: release --clean
29:         env:
30:           GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
31:           HOMEBREW_TAP_TOKEN: ${{ secrets.HOMEBREW_TAP_TOKEN }}
</file>

<file path="cmd/digest/export.go">
 1: // ABOUTME: Export command for writing OPML document to stdout
 2: // ABOUTME: Outputs the current feed list in OPML format for backup or import
 3: package main
 4: import (
 5: 	"os"
 6: 	"github.com/spf13/cobra"
 7: )
 8: var exportCmd = &cobra.Command{
 9: 	Use:   "export",
10: 	Short: "Export OPML to stdout",
11: 	Long:  "Export the current feed list in OPML format to standard output",
12: 	RunE: func(cmd *cobra.Command, args []string) error {
13: 		return opmlDoc.Write(os.Stdout)
14: 	},
15: }
16: func init() {
17: 	rootCmd.AddCommand(exportCmd)
18: }
</file>

<file path="cmd/digest/folder.go">
 1: // ABOUTME: Folder management commands for organizing feeds into categories
 2: // ABOUTME: Handles folder CRUD operations and syncs changes to OPML file
 3: package main
 4: import (
 5: 	"fmt"
 6: 	"github.com/spf13/cobra"
 7: )
 8: var folderCmd = &cobra.Command{
 9: 	Use:   "folder",
10: 	Short: "Manage feed folders",
11: 	Long:  "Create, list, and manage folders for organizing feeds",
12: }
13: var folderAddCmd = &cobra.Command{
14: 	Use:   "add <name>",
15: 	Short: "Create a new folder",
16: 	Long:  "Create a new folder to organize feeds",
17: 	Args:  cobra.ExactArgs(1),
18: 	RunE: func(cmd *cobra.Command, args []string) error {
19: 		name := args[0]
20: 		// Add folder to OPML
21: 		if err := opmlDoc.AddFolder(name); err != nil {
22: 			return fmt.Errorf("failed to add folder: %w", err)
23: 		}
24: 		// Save OPML
25: 		if err := saveOPML(); err != nil {
26: 			return fmt.Errorf("failed to save OPML: %w", err)
27: 		}
28: 		fmt.Printf("Created folder: %s\n", name)
29: 		return nil
30: 	},
31: }
32: var folderListCmd = &cobra.Command{
33: 	Use:     "list",
34: 	Aliases: []string{"ls"},
35: 	Short:   "List all folders",
36: 	Long:    "List all folders and the count of feeds in each",
37: 	RunE: func(cmd *cobra.Command, args []string) error {
38: 		folders := opmlDoc.Folders()
39: 		if len(folders) == 0 {
40: 			fmt.Println("No folders found. Create a folder with 'digest folder add <name>'")
41: 			return nil
42: 		}
43: 		fmt.Printf("Found %d folder(s):\n\n", len(folders))
44: 		for _, folder := range folders {
45: 			feeds := opmlDoc.FeedsInFolder(folder)
46: 			fmt.Printf("%s (%d feed(s))\n", folder, len(feeds))
47: 		}
48: 		return nil
49: 	},
50: }
51: func init() {
52: 	rootCmd.AddCommand(folderCmd)
53: 	folderCmd.AddCommand(folderAddCmd)
54: 	folderCmd.AddCommand(folderListCmd)
55: }
</file>

<file path="cmd/digest/mcp.go">
 1: // ABOUTME: MCP server command for digest CLI
 2: // ABOUTME: Starts stdio-based MCP server for AI agent integration
 3: package main
 4: import (
 5: 	"fmt"
 6: 	"github.com/spf13/cobra"
 7: 	"github.com/harper/digest/internal/mcp"
 8: )
 9: var mcpCmd = &cobra.Command{
10: 	Use:   "mcp",
11: 	Short: "Start MCP server for AI agents",
12: 	Long: `Start the Model Context Protocol (MCP) server on stdio.
13: This allows AI agents like Claude to interact with your RSS feeds,
14: query entries, manage subscriptions, and more through structured tools.
15: The server communicates via JSON-RPC on stdin/stdout.`,
16: 	RunE: func(cmd *cobra.Command, args []string) error {
17: 		// Create MCP server with database and OPML context
18: 		server := mcp.NewServer(dbConn, opmlDoc, opmlPath)
19: 		// Start serving on stdio
20: 		if err := server.ServeStdio(); err != nil {
21: 			return fmt.Errorf("MCP server error: %w", err)
22: 		}
23: 		return nil
24: 	},
25: }
26: func init() {
27: 	rootCmd.AddCommand(mcpCmd)
28: }
</file>

<file path="cmd/digest/version.go">
 1: // ABOUTME: Version command for digest CLI
 2: // ABOUTME: Displays version, commit, and build date information
 3: package main
 4: import (
 5: 	"fmt"
 6: 	"github.com/spf13/cobra"
 7: )
 8: // Version information set via ldflags at build time
 9: var (
10: 	Version   = "dev"
11: 	Commit    = "unknown"
12: 	BuildDate = "unknown"
13: )
14: var versionCmd = &cobra.Command{
15: 	Use:   "version",
16: 	Short: "Print version information",
17: 	Long:  "Print the version, commit hash, and build date of digest.",
18: 	Run: func(cmd *cobra.Command, args []string) {
19: 		fmt.Printf("digest %s\n", Version)
20: 		fmt.Printf("  commit:  %s\n", Commit)
21: 		fmt.Printf("  built:   %s\n", BuildDate)
22: 	},
23: }
24: func init() {
25: 	rootCmd.AddCommand(versionCmd)
26: }
</file>

<file path="docs/plans/2025-12-10-digest-design.md">
  1: # digest - RSS Feed Tracker Design
  2:
  3: ## Overview
  4:
  5: **digest** is a CLI tool for tracking RSS/Atom feeds with MCP integration for AI agents. It serves dual purposes: personal feed reading and providing agents with access to current news/content.
  6:
  7: ## Architecture
  8:
  9: ```
 10: ┌─────────────────┐     ┌──────────────────┐
 11: │   feeds.opml    │────▶│   SQLite DB      │
 12: │  (subscriptions │     │  (entries, state,│
 13: │   + folders)    │     │   ETag cache)    │
 14: └─────────────────┘     └──────────────────┘
 15:          │                       │
 16:          ▼                       ▼
 17: ┌─────────────────────────────────────────┐
 18: │              digest CLI                  │
 19: │  (Cobra commands, Go, single binary)    │
 20: └─────────────────────────────────────────┘
 21:          │                       │
 22:          ▼                       ▼
 23: ┌─────────────────┐     ┌──────────────────┐
 24: │   Human (CLI)   │     │   MCP Server     │
 25: │   digest sync   │     │  (tools/resources│
 26: │   digest read   │     │   for agents)    │
 27: └─────────────────┘     └──────────────────┘
 28: ```
 29:
 30: **Key principle:** OPML is the source of truth for *what* you're subscribed to. SQLite stores *content* that's been fetched, plus read state and HTTP caching metadata.
 31:
 32: **File locations (XDG standard):**
 33: - `~/.local/share/digest/feeds.opml` - subscriptions
 34: - `~/.local/share/digest/digest.db` - SQLite database
 35:
 36: ## Data Model
 37:
 38: ### OPML Structure (feeds.opml)
 39:
 40: ```xml
 41: <?xml version="1.0" encoding="UTF-8"?>
 42: <opml version="2.0">
 43:   <head>
 44:     <title>digest feeds</title>
 45:   </head>
 46:   <body>
 47:     <outline text="Tech" title="Tech">
 48:       <outline type="rss" text="Hacker News" xmlUrl="https://news.ycombinator.com/rss"/>
 49:       <outline type="rss" text="Lobsters" xmlUrl="https://lobste.rs/rss"/>
 50:     </outline>
 51:     <outline text="Fun" title="Fun">
 52:       <outline type="rss" text="XKCD" xmlUrl="https://xkcd.com/rss.xml"/>
 53:     </outline>
 54:     <outline type="rss" text="Uncategorized Feed" xmlUrl="https://example.com/feed"/>
 55:   </body>
 56: </opml>
 57: ```
 58:
 59: ### SQLite Schema
 60:
 61: ```sql
 62: -- Tracks each feed's sync state (ETags, last fetch)
 63: feeds (
 64:   id TEXT PRIMARY KEY,        -- UUID
 65:   url TEXT UNIQUE NOT NULL,   -- feed URL (matches OPML xmlUrl)
 66:   title TEXT,                 -- cached title from feed
 67:   etag TEXT,                  -- HTTP ETag for caching
 68:   last_modified TEXT,         -- HTTP Last-Modified header
 69:   last_fetched_at DATETIME,
 70:   last_error TEXT,            -- last error message if any
 71:   error_count INTEGER DEFAULT 0,
 72:   created_at DATETIME
 73: )
 74:
 75: -- Individual articles/entries
 76: entries (
 77:   id TEXT PRIMARY KEY,        -- UUID
 78:   feed_id TEXT NOT NULL,      -- FK to feeds
 79:   guid TEXT NOT NULL,         -- entry's unique ID from feed
 80:   title TEXT,
 81:   link TEXT,
 82:   author TEXT,
 83:   published_at DATETIME,
 84:   content TEXT,               -- full content or summary
 85:   read BOOLEAN DEFAULT FALSE,
 86:   read_at DATETIME,
 87:   created_at DATETIME,
 88:   UNIQUE(feed_id, guid)       -- no duplicate entries per feed
 89: )
 90:
 91: -- Tags that came from the feed itself (categories)
 92: entry_tags (
 93:   entry_id TEXT,
 94:   tag TEXT,
 95:   PRIMARY KEY (entry_id, tag)
 96: )
 97: ```
 98:
 99: ## CLI Commands
100:
101: ```
102: digest (root)
103: ├── feed                      # Feed management
104: │   ├── add <url> [--folder]  # Add feed, update OPML
105: │   ├── remove <url|prefix>   # Remove feed from OPML
106: │   ├── list / ls             # List all feeds (from OPML)
107: │   └── import <file.opml>    # Import/merge external OPML
108: │
109: ├── folder                    # Folder management
110: │   ├── add <name>            # Create folder in OPML
111: │   ├── remove <name>         # Remove folder (moves feeds to root)
112: │   └── list / ls             # List folders
113: │
114: ├── sync [url|prefix]         # Fetch feeds, respect caching
115: │   └── --force               # Ignore ETag/Last-Modified
116: │
117: ├── list / ls                 # List entries (default: unread)
118: │   ├── --all / -a            # Include read entries
119: │   ├── --feed <url|prefix>   # Filter by feed
120: │   ├── --folder <name>       # Filter by folder
121: │   ├── --since <date>        # Filter by date
122: │   └── --limit <n>           # Limit results (default 20)
123: │
124: ├── read <prefix>             # Mark entry as read
125: ├── unread <prefix>           # Mark entry as unread
126: ├── open <prefix>             # Open link in browser + mark read
127: │
128: ├── export                    # Export OPML to stdout
129: ├── mcp                       # Start MCP server (stdio)
130: └── --db / --opml             # Override default paths
131: ```
132:
133: **Aliases:** `digest ls` = `digest list`, `digest f` = `digest feed`
134:
135: **UUID prefix matching:** Like toki - type 6+ chars of UUID instead of full ID.
136:
137: ## MCP Integration
138:
139: ### Tools (CRUD for agents)
140:
141: ```
142: # Feed management
143: add_feed(url, folder?)        # Add feed to OPML
144: remove_feed(url)              # Remove feed from OPML
145: list_feeds()                  # List all subscribed feeds
146:
147: # Folder management
148: add_folder(name)              # Create folder
149: remove_folder(name)           # Remove folder
150:
151: # Sync
152: sync_feeds(url?)              # Fetch all or specific feed
153:
154: # Entry operations
155: list_entries(feed?, folder?, unread_only?, since?, limit?)
156: mark_read(entry_id)
157: mark_unread(entry_id)
158: search_entries(query, limit?) # Full-text search in titles/content
159:
160: # Bulk operations
161: mark_all_read(feed?, folder?) # Mark multiple as read
162: ```
163:
164: ### Resources (read-only views)
165:
166: ```
167: digest://feeds                 # All subscribed feeds
168: digest://feeds/{folder}        # Feeds in a folder
169: digest://entries/unread        # Unread entries
170: digest://entries/recent        # Last 24 hours
171: digest://entries/today         # Today's entries
172: digest://stats                 # Counts, last sync times
173: ```
174:
175: ### Prompts (workflow templates)
176:
177: ```
178: daily-digest      # "What's new today? Summarize key articles"
179: research-topic    # "Find recent articles about {topic}"
180: catch-up          # "I've been away for {days}, what did I miss?"
181: curate-feeds      # "Review my feeds, suggest additions/removals"
182: ```
183:
184: ## Sync Behavior & Caching
185:
186: ### HTTP Smart Caching
187:
188: When fetching a feed:
189: 1. Check `feeds` table for stored `etag` and `last_modified`
190: 2. Send conditional request:
191:    ```
192:    GET /feed.xml
193:    If-None-Match: "abc123"
194:    If-Modified-Since: Tue, 10 Dec 2024 12:00:00 GMT
195:    ```
196: 3. If server returns `304 Not Modified` → skip parsing, done
197: 4. If `200 OK` → parse feed, store new ETag/Last-Modified, upsert entries
198:
199: ### Entry Deduplication
200:
201: Each entry has a `guid` (from the feed's `<guid>` or `<id>` element). We use `UNIQUE(feed_id, guid)` to prevent duplicates. On sync:
202: - New GUID → insert entry
203: - Existing GUID → skip (or optionally update content if changed)
204:
205: ### Sync Output
206:
207: ```
208: $ digest sync
209: Syncing 12 feeds...
210:   ✓ Hacker News         +8 new
211:   ✓ Lobsters            +3 new
212:   - XKCD                (not modified)
213:   ✓ Some Blog           +1 new
214:   ✗ Dead Feed           (error: 404)
215:
216: Synced 12 entries from 4 feeds (2 cached, 1 error)
217: ```
218:
219: ### Error Handling
220:
221: - Network errors → log warning, continue with other feeds
222: - Parse errors → log warning, skip feed
223: - Store `last_error` and `error_count` in feeds table for visibility
224: - `digest feed list` shows feeds with persistent errors
225:
226: ## Testing Strategy
227:
228: Scenario-driven testing with real feeds (no mocks):
229:
230: ```
231: Scenario: Fresh sync of a new feed
232:   Given an empty database
233:   And a valid RSS feed URL (use a stable public feed like xkcd.com/rss.xml)
234:   When I run "digest feed add <url>"
235:   And I run "digest sync"
236:   Then entries appear in the database
237:   And "digest list" shows unread entries
238:
239: Scenario: Cached sync respects ETag
240:   Given a feed was synced previously
241:   When I run "digest sync"
242:   Then the feed returns 304 Not Modified
243:   And no duplicate entries are created
244:
245: Scenario: Mark entry as read
246:   Given unread entries exist
247:   When I run "digest read <prefix>"
248:   Then the entry is marked read
249:   And "digest list" (unread only) excludes it
250:   And "digest list --all" includes it
251:
252: Scenario: OPML round-trip
253:   Given feeds in folders
254:   When I run "digest export > backup.opml"
255:   And I delete the OPML
256:   And I run "digest feed import backup.opml"
257:   Then all feeds and folders are restored
258: ```
259:
260: ## Project Structure
261:
262: ```
263: digest/
264: ├── cmd/digest/
265: │   ├── main.go              # Entry point
266: │   ├── root.go              # Root command, DB/OPML init
267: │   ├── feed.go              # feed add/remove/list/import
268: │   ├── folder.go            # folder add/remove/list
269: │   ├── sync.go              # sync command
270: │   ├── list.go              # list entries
271: │   ├── read.go              # read/unread/open commands
272: │   ├── export.go            # OPML export
273: │   └── mcp.go               # MCP server
274: ├── internal/
275: │   ├── models/              # Feed, Entry, Folder structs
276: │   ├── db/                  # SQLite operations
277: │   ├── opml/                # OPML read/write/merge
278: │   ├── fetch/               # HTTP fetching with caching
279: │   ├── parse/               # RSS/Atom parsing
280: │   ├── ui/                  # Terminal formatting
281: │   └── mcp/                 # MCP tools/resources/prompts
282: ├── test/                    # Integration tests
283: ├── go.mod
284: └── Makefile
285: ```
286:
287: ## Go Dependencies
288:
289: - `github.com/spf13/cobra` - CLI framework
290: - `github.com/google/uuid` - UUID generation
291: - `modernc.org/sqlite` - Pure Go SQLite
292: - `github.com/mmcdole/gofeed` - RSS/Atom parser
293: - `github.com/mark3labs/mcp-go` - MCP SDK
294: - `github.com/fatih/color` - Terminal colors
</file>

<file path="docs/plans/2025-12-10-digest-implementation.md">
   1: # digest Implementation Plan
   2:
   3: > **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.
   4:
   5: **Goal:** Build an RSS/Atom feed tracker CLI with MCP integration for AI agents.
   6:
   7: **Architecture:** OPML file stores subscriptions (source of truth), SQLite stores fetched entries and HTTP cache state. Cobra CLI with MCP server mode. Single Go binary.
   8:
   9: **Tech Stack:** Go 1.24+, Cobra, SQLite (modernc.org), gofeed (RSS/Atom parsing), MCP Go SDK, fatih/color
  10:
  11: ---
  12:
  13: ## Phase 1: Project Scaffolding
  14:
  15: ### Task 1.1: Initialize Go Module
  16:
  17: **Files:**
  18: - Create: `go.mod`
  19: - Create: `cmd/digest/main.go`
  20:
  21: **Step 1: Initialize Go module**
  22:
  23: ```bash
  24: cd /Users/harper/Public/src/personal/rss-mcp
  25: go mod init github.com/harper/digest
  26: ```
  27:
  28: **Step 2: Create minimal main.go**
  29:
  30: ```go
  31: // ABOUTME: Entry point for digest CLI
  32: // ABOUTME: Initializes and executes root command
  33:
  34: package main
  35:
  36: import (
  37: 	"fmt"
  38: 	"os"
  39: )
  40:
  41: func main() {
  42: 	if err := Execute(); err != nil {
  43: 		fmt.Fprintln(os.Stderr, err)
  44: 		os.Exit(1)
  45: 	}
  46: }
  47:
  48: func Execute() error {
  49: 	return nil
  50: }
  51: ```
  52:
  53: **Step 3: Verify it compiles**
  54:
  55: Run: `go build ./cmd/digest`
  56: Expected: No errors, produces `digest` binary
  57:
  58: **Step 4: Commit**
  59:
  60: ```bash
  61: git add go.mod cmd/
  62: git commit -m "feat: initialize go module and main entry point"
  63: ```
  64:
  65: ---
  66:
  67: ### Task 1.2: Add Cobra Root Command
  68:
  69: **Files:**
  70: - Create: `cmd/digest/root.go`
  71: - Modify: `cmd/digest/main.go`
  72:
  73: **Step 1: Create root.go with basic structure**
  74:
  75: ```go
  76: // ABOUTME: Root Cobra command and global flags
  77: // ABOUTME: Sets up CLI structure and initializes database/OPML
  78:
  79: package main
  80:
  81: import (
  82: 	"github.com/spf13/cobra"
  83: )
  84:
  85: var (
  86: 	dbPath   string
  87: 	opmlPath string
  88: )
  89:
  90: var rootCmd = &cobra.Command{
  91: 	Use:   "digest",
  92: 	Short: "RSS/Atom feed tracker with MCP integration",
  93: 	Long: `
  94: ██████╗ ██╗ ██████╗ ███████╗███████╗████████╗
  95: ██╔══██╗██║██╔════╝ ██╔════╝██╔════╝╚══██╔══╝
  96: ██║  ██║██║██║  ███╗█████╗  ███████╗   ██║
  97: ██║  ██║██║██║   ██║██╔══╝  ╚════██║   ██║
  98: ██████╔╝██║╚██████╔╝███████╗███████║   ██║
  99: ╚═════╝ ╚═╝ ╚═════╝ ╚══════╝╚══════╝   ╚═╝
 100:
 101: RSS/Atom feed tracker for humans and AI agents.
 102:
 103: Track feeds, sync content, and expose via MCP for Claude.`,
 104: }
 105:
 106: func Execute() error {
 107: 	return rootCmd.Execute()
 108: }
 109:
 110: func init() {
 111: 	rootCmd.PersistentFlags().StringVar(&dbPath, "db", "", "database file path (default: ~/.local/share/digest/digest.db)")
 112: 	rootCmd.PersistentFlags().StringVar(&opmlPath, "opml", "", "OPML file path (default: ~/.local/share/digest/feeds.opml)")
 113: }
 114: ```
 115:
 116: **Step 2: Update main.go to use Execute from root**
 117:
 118: ```go
 119: // ABOUTME: Entry point for digest CLI
 120: // ABOUTME: Initializes and executes root command
 121:
 122: package main
 123:
 124: import (
 125: 	"fmt"
 126: 	"os"
 127: )
 128:
 129: func main() {
 130: 	if err := Execute(); err != nil {
 131: 		fmt.Fprintln(os.Stderr, err)
 132: 		os.Exit(1)
 133: 	}
 134: }
 135: ```
 136:
 137: **Step 3: Add Cobra dependency**
 138:
 139: ```bash
 140: go get github.com/spf13/cobra@v1.10.1
 141: ```
 142:
 143: **Step 4: Verify it runs**
 144:
 145: Run: `go run ./cmd/digest --help`
 146: Expected: Shows digest help with banner and flag descriptions
 147:
 148: **Step 5: Commit**
 149:
 150: ```bash
 151: git add .
 152: git commit -m "feat: add Cobra root command with global flags"
 153: ```
 154:
 155: ---
 156:
 157: ### Task 1.3: Add Core Dependencies
 158:
 159: **Files:**
 160: - Modify: `go.mod`
 161:
 162: **Step 1: Add all dependencies**
 163:
 164: ```bash
 165: go get github.com/google/uuid@v1.6.0
 166: go get modernc.org/sqlite@v1.40.1
 167: go get github.com/mmcdole/gofeed@latest
 168: go get github.com/fatih/color@v1.18.0
 169: go get github.com/mark3labs/mcp-go@latest
 170: ```
 171:
 172: **Step 2: Tidy dependencies**
 173:
 174: ```bash
 175: go mod tidy
 176: ```
 177:
 178: **Step 3: Verify go.mod has all deps**
 179:
 180: Run: `cat go.mod`
 181: Expected: Shows all five dependencies in require block
 182:
 183: **Step 4: Commit**
 184:
 185: ```bash
 186: git add go.mod go.sum
 187: git commit -m "chore: add core dependencies"
 188: ```
 189:
 190: ---
 191:
 192: ## Phase 2: Models
 193:
 194: ### Task 2.1: Create Feed Model
 195:
 196: **Files:**
 197: - Create: `internal/models/feed.go`
 198: - Create: `internal/models/feed_test.go`
 199:
 200: **Step 1: Write the failing test**
 201:
 202: ```go
 203: // ABOUTME: Tests for Feed model
 204: // ABOUTME: Validates Feed creation and field handling
 205:
 206: package models
 207:
 208: import (
 209: 	"testing"
 210: 	"time"
 211: )
 212:
 213: func TestNewFeed(t *testing.T) {
 214: 	url := "https://example.com/feed.xml"
 215: 	feed := NewFeed(url)
 216:
 217: 	if feed.URL != url {
 218: 		t.Errorf("expected URL %s, got %s", url, feed.URL)
 219: 	}
 220: 	if feed.ID == "" {
 221: 		t.Error("expected ID to be set")
 222: 	}
 223: 	if feed.CreatedAt.IsZero() {
 224: 		t.Error("expected CreatedAt to be set")
 225: 	}
 226: }
 227:
 228: func TestFeed_SetCacheHeaders(t *testing.T) {
 229: 	feed := NewFeed("https://example.com/feed.xml")
 230: 	etag := `"abc123"`
 231: 	lastMod := "Tue, 10 Dec 2024 12:00:00 GMT"
 232:
 233: 	feed.SetCacheHeaders(etag, lastMod)
 234:
 235: 	if feed.ETag == nil || *feed.ETag != etag {
 236: 		t.Errorf("expected ETag %s, got %v", etag, feed.ETag)
 237: 	}
 238: 	if feed.LastModified == nil || *feed.LastModified != lastMod {
 239: 		t.Errorf("expected LastModified %s, got %v", lastMod, feed.LastModified)
 240: 	}
 241: }
 242: ```
 243:
 244: **Step 2: Run test to verify it fails**
 245:
 246: Run: `go test ./internal/models/... -v`
 247: Expected: FAIL - package doesn't exist
 248:
 249: **Step 3: Write minimal implementation**
 250:
 251: ```go
 252: // ABOUTME: Feed model representing an RSS/Atom subscription
 253: // ABOUTME: Tracks URL, cache state, and sync metadata
 254:
 255: package models
 256:
 257: import (
 258: 	"time"
 259:
 260: 	"github.com/google/uuid"
 261: )
 262:
 263: type Feed struct {
 264: 	ID            string
 265: 	URL           string
 266: 	Title         *string
 267: 	ETag          *string
 268: 	LastModified  *string
 269: 	LastFetchedAt *time.Time
 270: 	LastError     *string
 271: 	ErrorCount    int
 272: 	CreatedAt     time.Time
 273: }
 274:
 275: func NewFeed(url string) *Feed {
 276: 	return &Feed{
 277: 		ID:        uuid.New().String(),
 278: 		URL:       url,
 279: 		CreatedAt: time.Now(),
 280: 	}
 281: }
 282:
 283: func (f *Feed) SetCacheHeaders(etag, lastModified string) {
 284: 	if etag != "" {
 285: 		f.ETag = &etag
 286: 	}
 287: 	if lastModified != "" {
 288: 		f.LastModified = &lastModified
 289: 	}
 290: }
 291: ```
 292:
 293: **Step 4: Run test to verify it passes**
 294:
 295: Run: `go test ./internal/models/... -v`
 296: Expected: PASS
 297:
 298: **Step 5: Commit**
 299:
 300: ```bash
 301: git add internal/
 302: git commit -m "feat: add Feed model with cache header support"
 303: ```
 304:
 305: ---
 306:
 307: ### Task 2.2: Create Entry Model
 308:
 309: **Files:**
 310: - Create: `internal/models/entry.go`
 311: - Modify: `internal/models/feed_test.go` (add entry tests)
 312:
 313: **Step 1: Write the failing test**
 314:
 315: Add to `internal/models/feed_test.go`:
 316:
 317: ```go
 318: func TestNewEntry(t *testing.T) {
 319: 	feedID := uuid.New().String()
 320: 	guid := "https://example.com/post/123"
 321: 	title := "Test Post"
 322:
 323: 	entry := NewEntry(feedID, guid, title)
 324:
 325: 	if entry.FeedID != feedID {
 326: 		t.Errorf("expected FeedID %s, got %s", feedID, entry.FeedID)
 327: 	}
 328: 	if entry.GUID != guid {
 329: 		t.Errorf("expected GUID %s, got %s", guid, entry.GUID)
 330: 	}
 331: 	if entry.Title == nil || *entry.Title != title {
 332: 		t.Errorf("expected Title %s, got %v", title, entry.Title)
 333: 	}
 334: 	if entry.Read {
 335: 		t.Error("expected Read to be false")
 336: 	}
 337: }
 338:
 339: func TestEntry_MarkRead(t *testing.T) {
 340: 	entry := NewEntry("feed-id", "guid", "title")
 341:
 342: 	if entry.Read {
 343: 		t.Error("expected entry to be unread initially")
 344: 	}
 345:
 346: 	entry.MarkRead()
 347:
 348: 	if !entry.Read {
 349: 		t.Error("expected entry to be read after MarkRead")
 350: 	}
 351: 	if entry.ReadAt == nil {
 352: 		t.Error("expected ReadAt to be set")
 353: 	}
 354: }
 355:
 356: func TestEntry_MarkUnread(t *testing.T) {
 357: 	entry := NewEntry("feed-id", "guid", "title")
 358: 	entry.MarkRead()
 359:
 360: 	entry.MarkUnread()
 361:
 362: 	if entry.Read {
 363: 		t.Error("expected entry to be unread after MarkUnread")
 364: 	}
 365: 	if entry.ReadAt != nil {
 366: 		t.Error("expected ReadAt to be nil")
 367: 	}
 368: }
 369: ```
 370:
 371: Add import for uuid at top of test file:
 372: ```go
 373: import (
 374: 	"testing"
 375:
 376: 	"github.com/google/uuid"
 377: )
 378: ```
 379:
 380: **Step 2: Run test to verify it fails**
 381:
 382: Run: `go test ./internal/models/... -v`
 383: Expected: FAIL - NewEntry undefined
 384:
 385: **Step 3: Write minimal implementation**
 386:
 387: Create `internal/models/entry.go`:
 388:
 389: ```go
 390: // ABOUTME: Entry model representing a single article/post from a feed
 391: // ABOUTME: Tracks content, read state, and metadata
 392:
 393: package models
 394:
 395: import (
 396: 	"time"
 397:
 398: 	"github.com/google/uuid"
 399: )
 400:
 401: type Entry struct {
 402: 	ID          string
 403: 	FeedID      string
 404: 	GUID        string
 405: 	Title       *string
 406: 	Link        *string
 407: 	Author      *string
 408: 	PublishedAt *time.Time
 409: 	Content     *string
 410: 	Read        bool
 411: 	ReadAt      *time.Time
 412: 	CreatedAt   time.Time
 413: }
 414:
 415: func NewEntry(feedID, guid, title string) *Entry {
 416: 	return &Entry{
 417: 		ID:        uuid.New().String(),
 418: 		FeedID:    feedID,
 419: 		GUID:      guid,
 420: 		Title:     &title,
 421: 		Read:      false,
 422: 		CreatedAt: time.Now(),
 423: 	}
 424: }
 425:
 426: func (e *Entry) MarkRead() {
 427: 	e.Read = true
 428: 	now := time.Now()
 429: 	e.ReadAt = &now
 430: }
 431:
 432: func (e *Entry) MarkUnread() {
 433: 	e.Read = false
 434: 	e.ReadAt = nil
 435: }
 436: ```
 437:
 438: **Step 4: Run test to verify it passes**
 439:
 440: Run: `go test ./internal/models/... -v`
 441: Expected: PASS
 442:
 443: **Step 5: Commit**
 444:
 445: ```bash
 446: git add internal/models/
 447: git commit -m "feat: add Entry model with read/unread state"
 448: ```
 449:
 450: ---
 451:
 452: ## Phase 3: Database Layer
 453:
 454: ### Task 3.1: Database Connection and Path Helpers
 455:
 456: **Files:**
 457: - Create: `internal/db/db.go`
 458: - Create: `internal/db/db_test.go`
 459:
 460: **Step 1: Write the failing test**
 461:
 462: ```go
 463: // ABOUTME: Tests for database connection and path helpers
 464: // ABOUTME: Validates XDG path resolution and connection lifecycle
 465:
 466: package db
 467:
 468: import (
 469: 	"os"
 470: 	"path/filepath"
 471: 	"testing"
 472: )
 473:
 474: func TestGetDefaultDBPath(t *testing.T) {
 475: 	path := GetDefaultDBPath()
 476:
 477: 	if !filepath.IsAbs(path) {
 478: 		t.Errorf("expected absolute path, got %s", path)
 479: 	}
 480: 	if filepath.Base(path) != "digest.db" {
 481: 		t.Errorf("expected digest.db, got %s", filepath.Base(path))
 482: 	}
 483: }
 484:
 485: func TestGetDefaultOPMLPath(t *testing.T) {
 486: 	path := GetDefaultOPMLPath()
 487:
 488: 	if !filepath.IsAbs(path) {
 489: 		t.Errorf("expected absolute path, got %s", path)
 490: 	}
 491: 	if filepath.Base(path) != "feeds.opml" {
 492: 		t.Errorf("expected feeds.opml, got %s", filepath.Base(path))
 493: 	}
 494: }
 495:
 496: func TestInitDB(t *testing.T) {
 497: 	tmpDir := t.TempDir()
 498: 	dbPath := filepath.Join(tmpDir, "test.db")
 499:
 500: 	conn, err := InitDB(dbPath)
 501: 	if err != nil {
 502: 		t.Fatalf("InitDB failed: %v", err)
 503: 	}
 504: 	defer conn.Close()
 505:
 506: 	// Verify tables exist
 507: 	var count int
 508: 	err = conn.QueryRow("SELECT COUNT(*) FROM sqlite_master WHERE type='table' AND name='feeds'").Scan(&count)
 509: 	if err != nil {
 510: 		t.Fatalf("query failed: %v", err)
 511: 	}
 512: 	if count != 1 {
 513: 		t.Error("feeds table not created")
 514: 	}
 515: }
 516: ```
 517:
 518: **Step 2: Run test to verify it fails**
 519:
 520: Run: `go test ./internal/db/... -v`
 521: Expected: FAIL - package doesn't exist
 522:
 523: **Step 3: Write minimal implementation**
 524:
 525: ```go
 526: // ABOUTME: Database connection management and initialization
 527: // ABOUTME: Handles SQLite connection, XDG paths, and migrations
 528:
 529: package db
 530:
 531: import (
 532: 	"database/sql"
 533: 	"fmt"
 534: 	"os"
 535: 	"path/filepath"
 536:
 537: 	_ "modernc.org/sqlite"
 538: )
 539:
 540: func InitDB(dbPath string) (*sql.DB, error) {
 541: 	dir := filepath.Dir(dbPath)
 542: 	if err := os.MkdirAll(dir, 0750); err != nil {
 543: 		return nil, fmt.Errorf("failed to create database directory: %w", err)
 544: 	}
 545:
 546: 	db, err := sql.Open("sqlite", dbPath)
 547: 	if err != nil {
 548: 		return nil, fmt.Errorf("failed to open database: %w", err)
 549: 	}
 550:
 551: 	if _, err := db.Exec("PRAGMA foreign_keys = ON"); err != nil {
 552: 		_ = db.Close()
 553: 		return nil, fmt.Errorf("failed to enable foreign keys: %w", err)
 554: 	}
 555:
 556: 	if err := runMigrations(db); err != nil {
 557: 		_ = db.Close()
 558: 		return nil, fmt.Errorf("failed to run migrations: %w", err)
 559: 	}
 560:
 561: 	return db, nil
 562: }
 563:
 564: func GetDefaultDBPath() string {
 565: 	return filepath.Join(getDataDir(), "digest", "digest.db")
 566: }
 567:
 568: func GetDefaultOPMLPath() string {
 569: 	return filepath.Join(getDataDir(), "digest", "feeds.opml")
 570: }
 571:
 572: func getDataDir() string {
 573: 	if dataDir := os.Getenv("XDG_DATA_HOME"); dataDir != "" {
 574: 		return dataDir
 575: 	}
 576: 	homeDir, err := os.UserHomeDir()
 577: 	if err != nil {
 578: 		return "."
 579: 	}
 580: 	return filepath.Join(homeDir, ".local", "share")
 581: }
 582:
 583: func runMigrations(db *sql.DB) error {
 584: 	schema := `
 585: 	CREATE TABLE IF NOT EXISTS feeds (
 586: 		id TEXT PRIMARY KEY,
 587: 		url TEXT UNIQUE NOT NULL,
 588: 		title TEXT,
 589: 		etag TEXT,
 590: 		last_modified TEXT,
 591: 		last_fetched_at DATETIME,
 592: 		last_error TEXT,
 593: 		error_count INTEGER DEFAULT 0,
 594: 		created_at DATETIME NOT NULL
 595: 	);
 596:
 597: 	CREATE TABLE IF NOT EXISTS entries (
 598: 		id TEXT PRIMARY KEY,
 599: 		feed_id TEXT NOT NULL,
 600: 		guid TEXT NOT NULL,
 601: 		title TEXT,
 602: 		link TEXT,
 603: 		author TEXT,
 604: 		published_at DATETIME,
 605: 		content TEXT,
 606: 		read BOOLEAN DEFAULT FALSE,
 607: 		read_at DATETIME,
 608: 		created_at DATETIME NOT NULL,
 609: 		FOREIGN KEY (feed_id) REFERENCES feeds(id) ON DELETE CASCADE,
 610: 		UNIQUE(feed_id, guid)
 611: 	);
 612:
 613: 	CREATE INDEX IF NOT EXISTS idx_entries_feed_id ON entries(feed_id);
 614: 	CREATE INDEX IF NOT EXISTS idx_entries_read ON entries(read);
 615: 	CREATE INDEX IF NOT EXISTS idx_entries_published_at ON entries(published_at);
 616:
 617: 	CREATE TABLE IF NOT EXISTS entry_tags (
 618: 		entry_id TEXT NOT NULL,
 619: 		tag TEXT NOT NULL,
 620: 		PRIMARY KEY (entry_id, tag),
 621: 		FOREIGN KEY (entry_id) REFERENCES entries(id) ON DELETE CASCADE
 622: 	);
 623: 	`
 624: 	_, err := db.Exec(schema)
 625: 	return err
 626: }
 627: ```
 628:
 629: **Step 4: Run test to verify it passes**
 630:
 631: Run: `go test ./internal/db/... -v`
 632: Expected: PASS
 633:
 634: **Step 5: Commit**
 635:
 636: ```bash
 637: git add internal/db/
 638: git commit -m "feat: add database initialization with migrations"
 639: ```
 640:
 641: ---
 642:
 643: ### Task 3.2: Feed CRUD Operations
 644:
 645: **Files:**
 646: - Create: `internal/db/feeds.go`
 647: - Create: `internal/db/feeds_test.go`
 648:
 649: **Step 1: Write the failing test**
 650:
 651: ```go
 652: // ABOUTME: Tests for feed database operations
 653: // ABOUTME: Validates CRUD operations for feeds table
 654:
 655: package db
 656:
 657: import (
 658: 	"path/filepath"
 659: 	"testing"
 660:
 661: 	"github.com/harper/digest/internal/models"
 662: )
 663:
 664: func TestCreateFeed(t *testing.T) {
 665: 	conn := setupTestDB(t)
 666: 	defer conn.Close()
 667:
 668: 	feed := models.NewFeed("https://example.com/feed.xml")
 669: 	feed.Title = strPtr("Test Feed")
 670:
 671: 	err := CreateFeed(conn, feed)
 672: 	if err != nil {
 673: 		t.Fatalf("CreateFeed failed: %v", err)
 674: 	}
 675:
 676: 	// Verify it exists
 677: 	got, err := GetFeedByURL(conn, feed.URL)
 678: 	if err != nil {
 679: 		t.Fatalf("GetFeedByURL failed: %v", err)
 680: 	}
 681: 	if got.ID != feed.ID {
 682: 		t.Errorf("expected ID %s, got %s", feed.ID, got.ID)
 683: 	}
 684: }
 685:
 686: func TestGetFeedByPrefix(t *testing.T) {
 687: 	conn := setupTestDB(t)
 688: 	defer conn.Close()
 689:
 690: 	feed := models.NewFeed("https://example.com/feed.xml")
 691: 	_ = CreateFeed(conn, feed)
 692:
 693: 	// Use first 8 chars of UUID
 694: 	prefix := feed.ID[:8]
 695: 	got, err := GetFeedByPrefix(conn, prefix)
 696: 	if err != nil {
 697: 		t.Fatalf("GetFeedByPrefix failed: %v", err)
 698: 	}
 699: 	if got.ID != feed.ID {
 700: 		t.Errorf("expected ID %s, got %s", feed.ID, got.ID)
 701: 	}
 702: }
 703:
 704: func TestListFeeds(t *testing.T) {
 705: 	conn := setupTestDB(t)
 706: 	defer conn.Close()
 707:
 708: 	feed1 := models.NewFeed("https://example.com/feed1.xml")
 709: 	feed2 := models.NewFeed("https://example.com/feed2.xml")
 710: 	_ = CreateFeed(conn, feed1)
 711: 	_ = CreateFeed(conn, feed2)
 712:
 713: 	feeds, err := ListFeeds(conn)
 714: 	if err != nil {
 715: 		t.Fatalf("ListFeeds failed: %v", err)
 716: 	}
 717: 	if len(feeds) != 2 {
 718: 		t.Errorf("expected 2 feeds, got %d", len(feeds))
 719: 	}
 720: }
 721:
 722: func TestDeleteFeed(t *testing.T) {
 723: 	conn := setupTestDB(t)
 724: 	defer conn.Close()
 725:
 726: 	feed := models.NewFeed("https://example.com/feed.xml")
 727: 	_ = CreateFeed(conn, feed)
 728:
 729: 	err := DeleteFeed(conn, feed.ID)
 730: 	if err != nil {
 731: 		t.Fatalf("DeleteFeed failed: %v", err)
 732: 	}
 733:
 734: 	_, err = GetFeedByURL(conn, feed.URL)
 735: 	if err == nil {
 736: 		t.Error("expected feed to be deleted")
 737: 	}
 738: }
 739:
 740: func setupTestDB(t *testing.T) *sql.DB {
 741: 	t.Helper()
 742: 	tmpDir := t.TempDir()
 743: 	dbPath := filepath.Join(tmpDir, "test.db")
 744: 	conn, err := InitDB(dbPath)
 745: 	if err != nil {
 746: 		t.Fatalf("failed to init test db: %v", err)
 747: 	}
 748: 	return conn
 749: }
 750:
 751: func strPtr(s string) *string {
 752: 	return &s
 753: }
 754: ```
 755:
 756: Add import at top:
 757: ```go
 758: import (
 759: 	"database/sql"
 760: 	"path/filepath"
 761: 	"testing"
 762:
 763: 	"github.com/harper/digest/internal/models"
 764: )
 765: ```
 766:
 767: **Step 2: Run test to verify it fails**
 768:
 769: Run: `go test ./internal/db/... -v`
 770: Expected: FAIL - CreateFeed undefined
 771:
 772: **Step 3: Write minimal implementation**
 773:
 774: ```go
 775: // ABOUTME: Feed database operations
 776: // ABOUTME: CRUD operations for the feeds table
 777:
 778: package db
 779:
 780: import (
 781: 	"database/sql"
 782: 	"fmt"
 783: 	"time"
 784:
 785: 	"github.com/harper/digest/internal/models"
 786: )
 787:
 788: func CreateFeed(db *sql.DB, feed *models.Feed) error {
 789: 	_, err := db.Exec(`
 790: 		INSERT INTO feeds (id, url, title, etag, last_modified, last_fetched_at, last_error, error_count, created_at)
 791: 		VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)`,
 792: 		feed.ID, feed.URL, feed.Title, feed.ETag, feed.LastModified,
 793: 		feed.LastFetchedAt, feed.LastError, feed.ErrorCount, feed.CreatedAt,
 794: 	)
 795: 	return err
 796: }
 797:
 798: func GetFeedByURL(db *sql.DB, url string) (*models.Feed, error) {
 799: 	feed := &models.Feed{}
 800: 	err := db.QueryRow(`
 801: 		SELECT id, url, title, etag, last_modified, last_fetched_at, last_error, error_count, created_at
 802: 		FROM feeds WHERE url = ?`, url,
 803: 	).Scan(&feed.ID, &feed.URL, &feed.Title, &feed.ETag, &feed.LastModified,
 804: 		&feed.LastFetchedAt, &feed.LastError, &feed.ErrorCount, &feed.CreatedAt)
 805: 	if err != nil {
 806: 		return nil, err
 807: 	}
 808: 	return feed, nil
 809: }
 810:
 811: func GetFeedByPrefix(db *sql.DB, prefix string) (*models.Feed, error) {
 812: 	if len(prefix) < 6 {
 813: 		return nil, fmt.Errorf("prefix must be at least 6 characters")
 814: 	}
 815:
 816: 	rows, err := db.Query(`
 817: 		SELECT id, url, title, etag, last_modified, last_fetched_at, last_error, error_count, created_at
 818: 		FROM feeds WHERE id LIKE ?`, prefix+"%",
 819: 	)
 820: 	if err != nil {
 821: 		return nil, err
 822: 	}
 823: 	defer rows.Close()
 824:
 825: 	var feeds []*models.Feed
 826: 	for rows.Next() {
 827: 		feed := &models.Feed{}
 828: 		if err := rows.Scan(&feed.ID, &feed.URL, &feed.Title, &feed.ETag, &feed.LastModified,
 829: 			&feed.LastFetchedAt, &feed.LastError, &feed.ErrorCount, &feed.CreatedAt); err != nil {
 830: 			return nil, err
 831: 		}
 832: 		feeds = append(feeds, feed)
 833: 	}
 834:
 835: 	if len(feeds) == 0 {
 836: 		return nil, fmt.Errorf("no feed found with prefix %s", prefix)
 837: 	}
 838: 	if len(feeds) > 1 {
 839: 		return nil, fmt.Errorf("ambiguous prefix %s matches %d feeds", prefix, len(feeds))
 840: 	}
 841: 	return feeds[0], nil
 842: }
 843:
 844: func GetFeedByID(db *sql.DB, id string) (*models.Feed, error) {
 845: 	feed := &models.Feed{}
 846: 	err := db.QueryRow(`
 847: 		SELECT id, url, title, etag, last_modified, last_fetched_at, last_error, error_count, created_at
 848: 		FROM feeds WHERE id = ?`, id,
 849: 	).Scan(&feed.ID, &feed.URL, &feed.Title, &feed.ETag, &feed.LastModified,
 850: 		&feed.LastFetchedAt, &feed.LastError, &feed.ErrorCount, &feed.CreatedAt)
 851: 	if err != nil {
 852: 		return nil, err
 853: 	}
 854: 	return feed, nil
 855: }
 856:
 857: func ListFeeds(db *sql.DB) ([]*models.Feed, error) {
 858: 	rows, err := db.Query(`
 859: 		SELECT id, url, title, etag, last_modified, last_fetched_at, last_error, error_count, created_at
 860: 		FROM feeds ORDER BY created_at DESC`)
 861: 	if err != nil {
 862: 		return nil, err
 863: 	}
 864: 	defer rows.Close()
 865:
 866: 	var feeds []*models.Feed
 867: 	for rows.Next() {
 868: 		feed := &models.Feed{}
 869: 		if err := rows.Scan(&feed.ID, &feed.URL, &feed.Title, &feed.ETag, &feed.LastModified,
 870: 			&feed.LastFetchedAt, &feed.LastError, &feed.ErrorCount, &feed.CreatedAt); err != nil {
 871: 			return nil, err
 872: 		}
 873: 		feeds = append(feeds, feed)
 874: 	}
 875: 	return feeds, nil
 876: }
 877:
 878: func UpdateFeed(db *sql.DB, feed *models.Feed) error {
 879: 	_, err := db.Exec(`
 880: 		UPDATE feeds SET
 881: 			title = ?, etag = ?, last_modified = ?, last_fetched_at = ?,
 882: 			last_error = ?, error_count = ?
 883: 		WHERE id = ?`,
 884: 		feed.Title, feed.ETag, feed.LastModified, feed.LastFetchedAt,
 885: 		feed.LastError, feed.ErrorCount, feed.ID,
 886: 	)
 887: 	return err
 888: }
 889:
 890: func DeleteFeed(db *sql.DB, id string) error {
 891: 	_, err := db.Exec("DELETE FROM feeds WHERE id = ?", id)
 892: 	return err
 893: }
 894:
 895: func UpdateFeedFetchState(db *sql.DB, feedID string, etag, lastModified *string, fetchedAt time.Time) error {
 896: 	_, err := db.Exec(`
 897: 		UPDATE feeds SET etag = ?, last_modified = ?, last_fetched_at = ?, last_error = NULL, error_count = 0
 898: 		WHERE id = ?`,
 899: 		etag, lastModified, fetchedAt, feedID,
 900: 	)
 901: 	return err
 902: }
 903:
 904: func UpdateFeedError(db *sql.DB, feedID string, errMsg string) error {
 905: 	_, err := db.Exec(`
 906: 		UPDATE feeds SET last_error = ?, error_count = error_count + 1
 907: 		WHERE id = ?`,
 908: 		errMsg, feedID,
 909: 	)
 910: 	return err
 911: }
 912: ```
 913:
 914: **Step 4: Run test to verify it passes**
 915:
 916: Run: `go test ./internal/db/... -v`
 917: Expected: PASS
 918:
 919: **Step 5: Commit**
 920:
 921: ```bash
 922: git add internal/db/
 923: git commit -m "feat: add feed CRUD database operations"
 924: ```
 925:
 926: ---
 927:
 928: ### Task 3.3: Entry CRUD Operations
 929:
 930: **Files:**
 931: - Create: `internal/db/entries.go`
 932: - Create: `internal/db/entries_test.go`
 933:
 934: **Step 1: Write the failing test**
 935:
 936: ```go
 937: // ABOUTME: Tests for entry database operations
 938: // ABOUTME: Validates CRUD operations for entries table
 939:
 940: package db
 941:
 942: import (
 943: 	"database/sql"
 944: 	"path/filepath"
 945: 	"testing"
 946: 	"time"
 947:
 948: 	"github.com/harper/digest/internal/models"
 949: )
 950:
 951: func TestCreateEntry(t *testing.T) {
 952: 	conn := setupTestDB(t)
 953: 	defer conn.Close()
 954:
 955: 	feed := models.NewFeed("https://example.com/feed.xml")
 956: 	_ = CreateFeed(conn, feed)
 957:
 958: 	entry := models.NewEntry(feed.ID, "guid-123", "Test Entry")
 959: 	err := CreateEntry(conn, entry)
 960: 	if err != nil {
 961: 		t.Fatalf("CreateEntry failed: %v", err)
 962: 	}
 963:
 964: 	got, err := GetEntryByID(conn, entry.ID)
 965: 	if err != nil {
 966: 		t.Fatalf("GetEntryByID failed: %v", err)
 967: 	}
 968: 	if *got.Title != "Test Entry" {
 969: 		t.Errorf("expected title 'Test Entry', got %s", *got.Title)
 970: 	}
 971: }
 972:
 973: func TestCreateEntry_Duplicate(t *testing.T) {
 974: 	conn := setupTestDB(t)
 975: 	defer conn.Close()
 976:
 977: 	feed := models.NewFeed("https://example.com/feed.xml")
 978: 	_ = CreateFeed(conn, feed)
 979:
 980: 	entry1 := models.NewEntry(feed.ID, "same-guid", "Entry 1")
 981: 	entry2 := models.NewEntry(feed.ID, "same-guid", "Entry 2")
 982:
 983: 	_ = CreateEntry(conn, entry1)
 984: 	err := CreateEntry(conn, entry2)
 985: 	if err == nil {
 986: 		t.Error("expected duplicate entry to fail")
 987: 	}
 988: }
 989:
 990: func TestListEntries_Unread(t *testing.T) {
 991: 	conn := setupTestDB(t)
 992: 	defer conn.Close()
 993:
 994: 	feed := models.NewFeed("https://example.com/feed.xml")
 995: 	_ = CreateFeed(conn, feed)
 996:
 997: 	entry1 := models.NewEntry(feed.ID, "guid-1", "Entry 1")
 998: 	entry2 := models.NewEntry(feed.ID, "guid-2", "Entry 2")
 999: 	entry2.MarkRead()
1000:
1001: 	_ = CreateEntry(conn, entry1)
1002: 	_ = CreateEntry(conn, entry2)
1003:
1004: 	unreadOnly := true
1005: 	entries, err := ListEntries(conn, nil, &unreadOnly, nil, 100)
1006: 	if err != nil {
1007: 		t.Fatalf("ListEntries failed: %v", err)
1008: 	}
1009: 	if len(entries) != 1 {
1010: 		t.Errorf("expected 1 unread entry, got %d", len(entries))
1011: 	}
1012: }
1013:
1014: func TestMarkEntryRead(t *testing.T) {
1015: 	conn := setupTestDB(t)
1016: 	defer conn.Close()
1017:
1018: 	feed := models.NewFeed("https://example.com/feed.xml")
1019: 	_ = CreateFeed(conn, feed)
1020:
1021: 	entry := models.NewEntry(feed.ID, "guid-1", "Entry 1")
1022: 	_ = CreateEntry(conn, entry)
1023:
1024: 	err := MarkEntryRead(conn, entry.ID)
1025: 	if err != nil {
1026: 		t.Fatalf("MarkEntryRead failed: %v", err)
1027: 	}
1028:
1029: 	got, _ := GetEntryByID(conn, entry.ID)
1030: 	if !got.Read {
1031: 		t.Error("expected entry to be marked read")
1032: 	}
1033: }
1034: ```
1035:
1036: **Step 2: Run test to verify it fails**
1037:
1038: Run: `go test ./internal/db/... -v -run Entry`
1039: Expected: FAIL - CreateEntry undefined
1040:
1041: **Step 3: Write minimal implementation**
1042:
1043: ```go
1044: // ABOUTME: Entry database operations
1045: // ABOUTME: CRUD operations for the entries table
1046:
1047: package db
1048:
1049: import (
1050: 	"database/sql"
1051: 	"fmt"
1052: 	"time"
1053:
1054: 	"github.com/harper/digest/internal/models"
1055: )
1056:
1057: func CreateEntry(db *sql.DB, entry *models.Entry) error {
1058: 	_, err := db.Exec(`
1059: 		INSERT INTO entries (id, feed_id, guid, title, link, author, published_at, content, read, read_at, created_at)
1060: 		VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)`,
1061: 		entry.ID, entry.FeedID, entry.GUID, entry.Title, entry.Link, entry.Author,
1062: 		entry.PublishedAt, entry.Content, entry.Read, entry.ReadAt, entry.CreatedAt,
1063: 	)
1064: 	return err
1065: }
1066:
1067: func GetEntryByID(db *sql.DB, id string) (*models.Entry, error) {
1068: 	entry := &models.Entry{}
1069: 	err := db.QueryRow(`
1070: 		SELECT id, feed_id, guid, title, link, author, published_at, content, read, read_at, created_at
1071: 		FROM entries WHERE id = ?`, id,
1072: 	).Scan(&entry.ID, &entry.FeedID, &entry.GUID, &entry.Title, &entry.Link, &entry.Author,
1073: 		&entry.PublishedAt, &entry.Content, &entry.Read, &entry.ReadAt, &entry.CreatedAt)
1074: 	if err != nil {
1075: 		return nil, err
1076: 	}
1077: 	return entry, nil
1078: }
1079:
1080: func GetEntryByPrefix(db *sql.DB, prefix string) (*models.Entry, error) {
1081: 	if len(prefix) < 6 {
1082: 		return nil, fmt.Errorf("prefix must be at least 6 characters")
1083: 	}
1084:
1085: 	rows, err := db.Query(`
1086: 		SELECT id, feed_id, guid, title, link, author, published_at, content, read, read_at, created_at
1087: 		FROM entries WHERE id LIKE ?`, prefix+"%",
1088: 	)
1089: 	if err != nil {
1090: 		return nil, err
1091: 	}
1092: 	defer rows.Close()
1093:
1094: 	var entries []*models.Entry
1095: 	for rows.Next() {
1096: 		entry := &models.Entry{}
1097: 		if err := rows.Scan(&entry.ID, &entry.FeedID, &entry.GUID, &entry.Title, &entry.Link, &entry.Author,
1098: 			&entry.PublishedAt, &entry.Content, &entry.Read, &entry.ReadAt, &entry.CreatedAt); err != nil {
1099: 			return nil, err
1100: 		}
1101: 		entries = append(entries, entry)
1102: 	}
1103:
1104: 	if len(entries) == 0 {
1105: 		return nil, fmt.Errorf("no entry found with prefix %s", prefix)
1106: 	}
1107: 	if len(entries) > 1 {
1108: 		return nil, fmt.Errorf("ambiguous prefix %s matches %d entries", prefix, len(entries))
1109: 	}
1110: 	return entries[0], nil
1111: }
1112:
1113: func ListEntries(db *sql.DB, feedID *string, unreadOnly *bool, since *time.Time, limit int) ([]*models.Entry, error) {
1114: 	query := `
1115: 		SELECT id, feed_id, guid, title, link, author, published_at, content, read, read_at, created_at
1116: 		FROM entries WHERE 1=1`
1117: 	args := []interface{}{}
1118:
1119: 	if feedID != nil {
1120: 		query += " AND feed_id = ?"
1121: 		args = append(args, *feedID)
1122: 	}
1123: 	if unreadOnly != nil && *unreadOnly {
1124: 		query += " AND read = FALSE"
1125: 	}
1126: 	if since != nil {
1127: 		query += " AND published_at >= ?"
1128: 		args = append(args, *since)
1129: 	}
1130:
1131: 	query += " ORDER BY published_at DESC LIMIT ?"
1132: 	args = append(args, limit)
1133:
1134: 	rows, err := db.Query(query, args...)
1135: 	if err != nil {
1136: 		return nil, err
1137: 	}
1138: 	defer rows.Close()
1139:
1140: 	var entries []*models.Entry
1141: 	for rows.Next() {
1142: 		entry := &models.Entry{}
1143: 		if err := rows.Scan(&entry.ID, &entry.FeedID, &entry.GUID, &entry.Title, &entry.Link, &entry.Author,
1144: 			&entry.PublishedAt, &entry.Content, &entry.Read, &entry.ReadAt, &entry.CreatedAt); err != nil {
1145: 			return nil, err
1146: 		}
1147: 		entries = append(entries, entry)
1148: 	}
1149: 	return entries, nil
1150: }
1151:
1152: func MarkEntryRead(db *sql.DB, id string) error {
1153: 	_, err := db.Exec("UPDATE entries SET read = TRUE, read_at = ? WHERE id = ?", time.Now(), id)
1154: 	return err
1155: }
1156:
1157: func MarkEntryUnread(db *sql.DB, id string) error {
1158: 	_, err := db.Exec("UPDATE entries SET read = FALSE, read_at = NULL WHERE id = ?", id)
1159: 	return err
1160: }
1161:
1162: func EntryExists(db *sql.DB, feedID, guid string) (bool, error) {
1163: 	var count int
1164: 	err := db.QueryRow("SELECT COUNT(*) FROM entries WHERE feed_id = ? AND guid = ?", feedID, guid).Scan(&count)
1165: 	if err != nil {
1166: 		return false, err
1167: 	}
1168: 	return count > 0, nil
1169: }
1170:
1171: func CountUnreadEntries(db *sql.DB, feedID *string) (int, error) {
1172: 	query := "SELECT COUNT(*) FROM entries WHERE read = FALSE"
1173: 	args := []interface{}{}
1174: 	if feedID != nil {
1175: 		query += " AND feed_id = ?"
1176: 		args = append(args, *feedID)
1177: 	}
1178: 	var count int
1179: 	err := db.QueryRow(query, args...).Scan(&count)
1180: 	return count, err
1181: }
1182: ```
1183:
1184: **Step 4: Run test to verify it passes**
1185:
1186: Run: `go test ./internal/db/... -v`
1187: Expected: PASS
1188:
1189: **Step 5: Commit**
1190:
1191: ```bash
1192: git add internal/db/
1193: git commit -m "feat: add entry CRUD database operations"
1194: ```
1195:
1196: ---
1197:
1198: ## Phase 4: OPML Handling
1199:
1200: ### Task 4.1: OPML Parser and Writer
1201:
1202: **Files:**
1203: - Create: `internal/opml/opml.go`
1204: - Create: `internal/opml/opml_test.go`
1205:
1206: **Step 1: Write the failing test**
1207:
1208: ```go
1209: // ABOUTME: Tests for OPML parsing and writing
1210: // ABOUTME: Validates round-trip read/write of OPML files
1211:
1212: package opml
1213:
1214: import (
1215: 	"os"
1216: 	"path/filepath"
1217: 	"strings"
1218: 	"testing"
1219: )
1220:
1221: func TestParseOPML(t *testing.T) {
1222: 	xml := `<?xml version="1.0" encoding="UTF-8"?>
1223: <opml version="2.0">
1224:   <head><title>Test Feeds</title></head>
1225:   <body>
1226:     <outline text="Tech" title="Tech">
1227:       <outline type="rss" text="HN" xmlUrl="https://news.ycombinator.com/rss"/>
1228:     </outline>
1229:     <outline type="rss" text="XKCD" xmlUrl="https://xkcd.com/rss.xml"/>
1230:   </body>
1231: </opml>`
1232:
1233: 	doc, err := Parse(strings.NewReader(xml))
1234: 	if err != nil {
1235: 		t.Fatalf("Parse failed: %v", err)
1236: 	}
1237:
1238: 	if doc.Title != "Test Feeds" {
1239: 		t.Errorf("expected title 'Test Feeds', got %s", doc.Title)
1240: 	}
1241:
1242: 	feeds := doc.AllFeeds()
1243: 	if len(feeds) != 2 {
1244: 		t.Errorf("expected 2 feeds, got %d", len(feeds))
1245: 	}
1246:
1247: 	folders := doc.Folders()
1248: 	if len(folders) != 1 {
1249: 		t.Errorf("expected 1 folder, got %d", len(folders))
1250: 	}
1251: 	if folders[0] != "Tech" {
1252: 		t.Errorf("expected folder 'Tech', got %s", folders[0])
1253: 	}
1254: }
1255:
1256: func TestOPML_AddFeed(t *testing.T) {
1257: 	doc := NewDocument("My Feeds")
1258:
1259: 	doc.AddFeed("https://example.com/feed.xml", "Example", "")
1260:
1261: 	feeds := doc.AllFeeds()
1262: 	if len(feeds) != 1 {
1263: 		t.Fatalf("expected 1 feed, got %d", len(feeds))
1264: 	}
1265: 	if feeds[0].URL != "https://example.com/feed.xml" {
1266: 		t.Errorf("expected URL, got %s", feeds[0].URL)
1267: 	}
1268: }
1269:
1270: func TestOPML_AddFeedToFolder(t *testing.T) {
1271: 	doc := NewDocument("My Feeds")
1272:
1273: 	doc.AddFolder("Tech")
1274: 	doc.AddFeed("https://example.com/feed.xml", "Example", "Tech")
1275:
1276: 	feeds := doc.FeedsInFolder("Tech")
1277: 	if len(feeds) != 1 {
1278: 		t.Fatalf("expected 1 feed in Tech folder, got %d", len(feeds))
1279: 	}
1280: }
1281:
1282: func TestOPML_RoundTrip(t *testing.T) {
1283: 	tmpDir := t.TempDir()
1284: 	path := filepath.Join(tmpDir, "test.opml")
1285:
1286: 	doc := NewDocument("Test Feeds")
1287: 	doc.AddFolder("News")
1288: 	doc.AddFeed("https://example.com/feed.xml", "Example", "News")
1289: 	doc.AddFeed("https://other.com/rss", "Other", "")
1290:
1291: 	err := doc.WriteFile(path)
1292: 	if err != nil {
1293: 		t.Fatalf("WriteFile failed: %v", err)
1294: 	}
1295:
1296: 	loaded, err := ParseFile(path)
1297: 	if err != nil {
1298: 		t.Fatalf("ParseFile failed: %v", err)
1299: 	}
1300:
1301: 	if len(loaded.AllFeeds()) != 2 {
1302: 		t.Errorf("expected 2 feeds after round-trip, got %d", len(loaded.AllFeeds()))
1303: 	}
1304: }
1305: ```
1306:
1307: **Step 2: Run test to verify it fails**
1308:
1309: Run: `go test ./internal/opml/... -v`
1310: Expected: FAIL - package doesn't exist
1311:
1312: **Step 3: Write minimal implementation**
1313:
1314: ```go
1315: // ABOUTME: OPML file parsing and writing
1316: // ABOUTME: Handles RSS subscription lists with folder organization
1317:
1318: package opml
1319:
1320: import (
1321: 	"encoding/xml"
1322: 	"fmt"
1323: 	"io"
1324: 	"os"
1325: )
1326:
1327: type Document struct {
1328: 	Title    string
1329: 	Outlines []*Outline
1330: }
1331:
1332: type Outline struct {
1333: 	Text     string
1334: 	Title    string
1335: 	Type     string
1336: 	XMLURL   string
1337: 	Children []*Outline
1338: }
1339:
1340: type Feed struct {
1341: 	URL    string
1342: 	Title  string
1343: 	Folder string
1344: }
1345:
1346: type opmlXML struct {
1347: 	XMLName xml.Name  `xml:"opml"`
1348: 	Version string    `xml:"version,attr"`
1349: 	Head    headXML   `xml:"head"`
1350: 	Body    bodyXML   `xml:"body"`
1351: }
1352:
1353: type headXML struct {
1354: 	Title string `xml:"title"`
1355: }
1356:
1357: type bodyXML struct {
1358: 	Outlines []outlineXML `xml:"outline"`
1359: }
1360:
1361: type outlineXML struct {
1362: 	Text     string       `xml:"text,attr"`
1363: 	Title    string       `xml:"title,attr,omitempty"`
1364: 	Type     string       `xml:"type,attr,omitempty"`
1365: 	XMLURL   string       `xml:"xmlUrl,attr,omitempty"`
1366: 	Children []outlineXML `xml:"outline,omitempty"`
1367: }
1368:
1369: func NewDocument(title string) *Document {
1370: 	return &Document{Title: title}
1371: }
1372:
1373: func Parse(r io.Reader) (*Document, error) {
1374: 	var raw opmlXML
1375: 	if err := xml.NewDecoder(r).Decode(&raw); err != nil {
1376: 		return nil, fmt.Errorf("failed to parse OPML: %w", err)
1377: 	}
1378:
1379: 	doc := &Document{Title: raw.Head.Title}
1380: 	for _, o := range raw.Body.Outlines {
1381: 		doc.Outlines = append(doc.Outlines, convertOutline(o))
1382: 	}
1383: 	return doc, nil
1384: }
1385:
1386: func ParseFile(path string) (*Document, error) {
1387: 	f, err := os.Open(path)
1388: 	if err != nil {
1389: 		return nil, err
1390: 	}
1391: 	defer f.Close()
1392: 	return Parse(f)
1393: }
1394:
1395: func convertOutline(o outlineXML) *Outline {
1396: 	outline := &Outline{
1397: 		Text:   o.Text,
1398: 		Title:  o.Title,
1399: 		Type:   o.Type,
1400: 		XMLURL: o.XMLURL,
1401: 	}
1402: 	for _, child := range o.Children {
1403: 		outline.Children = append(outline.Children, convertOutline(child))
1404: 	}
1405: 	return outline
1406: }
1407:
1408: func (d *Document) AllFeeds() []Feed {
1409: 	var feeds []Feed
1410: 	for _, o := range d.Outlines {
1411: 		feeds = append(feeds, collectFeeds(o, "")...)
1412: 	}
1413: 	return feeds
1414: }
1415:
1416: func collectFeeds(o *Outline, folder string) []Feed {
1417: 	var feeds []Feed
1418: 	if o.XMLURL != "" {
1419: 		title := o.Text
1420: 		if title == "" {
1421: 			title = o.Title
1422: 		}
1423: 		feeds = append(feeds, Feed{URL: o.XMLURL, Title: title, Folder: folder})
1424: 	}
1425: 	folderName := folder
1426: 	if o.XMLURL == "" && len(o.Children) > 0 {
1427: 		folderName = o.Text
1428: 		if folderName == "" {
1429: 			folderName = o.Title
1430: 		}
1431: 	}
1432: 	for _, child := range o.Children {
1433: 		feeds = append(feeds, collectFeeds(child, folderName)...)
1434: 	}
1435: 	return feeds
1436: }
1437:
1438: func (d *Document) Folders() []string {
1439: 	var folders []string
1440: 	for _, o := range d.Outlines {
1441: 		if o.XMLURL == "" && len(o.Children) > 0 {
1442: 			name := o.Text
1443: 			if name == "" {
1444: 				name = o.Title
1445: 			}
1446: 			folders = append(folders, name)
1447: 		}
1448: 	}
1449: 	return folders
1450: }
1451:
1452: func (d *Document) FeedsInFolder(folder string) []Feed {
1453: 	var feeds []Feed
1454: 	for _, f := range d.AllFeeds() {
1455: 		if f.Folder == folder {
1456: 			feeds = append(feeds, f)
1457: 		}
1458: 	}
1459: 	return feeds
1460: }
1461:
1462: func (d *Document) AddFolder(name string) {
1463: 	for _, o := range d.Outlines {
1464: 		if (o.Text == name || o.Title == name) && o.XMLURL == "" {
1465: 			return
1466: 		}
1467: 	}
1468: 	d.Outlines = append(d.Outlines, &Outline{Text: name, Title: name})
1469: }
1470:
1471: func (d *Document) AddFeed(url, title, folder string) {
1472: 	feed := &Outline{Text: title, Title: title, Type: "rss", XMLURL: url}
1473:
1474: 	if folder == "" {
1475: 		d.Outlines = append(d.Outlines, feed)
1476: 		return
1477: 	}
1478:
1479: 	for _, o := range d.Outlines {
1480: 		if (o.Text == folder || o.Title == folder) && o.XMLURL == "" {
1481: 			o.Children = append(o.Children, feed)
1482: 			return
1483: 		}
1484: 	}
1485:
1486: 	folderOutline := &Outline{Text: folder, Title: folder, Children: []*Outline{feed}}
1487: 	d.Outlines = append(d.Outlines, folderOutline)
1488: }
1489:
1490: func (d *Document) RemoveFeed(url string) bool {
1491: 	return removeFeedFromOutlines(&d.Outlines, url)
1492: }
1493:
1494: func removeFeedFromOutlines(outlines *[]*Outline, url string) bool {
1495: 	for i, o := range *outlines {
1496: 		if o.XMLURL == url {
1497: 			*outlines = append((*outlines)[:i], (*outlines)[i+1:]...)
1498: 			return true
1499: 		}
1500: 		if removeFeedFromOutlines(&o.Children, url) {
1501: 			return true
1502: 		}
1503: 	}
1504: 	return false
1505: }
1506:
1507: func (d *Document) Write(w io.Writer) error {
1508: 	raw := opmlXML{
1509: 		Version: "2.0",
1510: 		Head:    headXML{Title: d.Title},
1511: 	}
1512: 	for _, o := range d.Outlines {
1513: 		raw.Body.Outlines = append(raw.Body.Outlines, convertToXML(o))
1514: 	}
1515:
1516: 	if _, err := w.Write([]byte(xml.Header)); err != nil {
1517: 		return err
1518: 	}
1519: 	enc := xml.NewEncoder(w)
1520: 	enc.Indent("", "  ")
1521: 	return enc.Encode(raw)
1522: }
1523:
1524: func (d *Document) WriteFile(path string) error {
1525: 	f, err := os.Create(path)
1526: 	if err != nil {
1527: 		return err
1528: 	}
1529: 	defer f.Close()
1530: 	return d.Write(f)
1531: }
1532:
1533: func convertToXML(o *Outline) outlineXML {
1534: 	out := outlineXML{
1535: 		Text:   o.Text,
1536: 		Title:  o.Title,
1537: 		Type:   o.Type,
1538: 		XMLURL: o.XMLURL,
1539: 	}
1540: 	for _, child := range o.Children {
1541: 		out.Children = append(out.Children, convertToXML(child))
1542: 	}
1543: 	return out
1544: }
1545: ```
1546:
1547: **Step 4: Run test to verify it passes**
1548:
1549: Run: `go test ./internal/opml/... -v`
1550: Expected: PASS
1551:
1552: **Step 5: Commit**
1553:
1554: ```bash
1555: git add internal/opml/
1556: git commit -m "feat: add OPML parsing and writing with folder support"
1557: ```
1558:
1559: ---
1560:
1561: ## Phase 5: Feed Fetching
1562:
1563: ### Task 5.1: HTTP Fetcher with Caching
1564:
1565: **Files:**
1566: - Create: `internal/fetch/fetch.go`
1567: - Create: `internal/fetch/fetch_test.go`
1568:
1569: **Step 1: Write the failing test**
1570:
1571: ```go
1572: // ABOUTME: Tests for HTTP feed fetching with caching
1573: // ABOUTME: Validates ETag/Last-Modified conditional requests
1574:
1575: package fetch
1576:
1577: import (
1578: 	"net/http"
1579: 	"net/http/httptest"
1580: 	"testing"
1581: )
1582:
1583: func TestFetch_Fresh(t *testing.T) {
1584: 	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
1585: 		w.Header().Set("ETag", `"abc123"`)
1586: 		w.Header().Set("Last-Modified", "Tue, 10 Dec 2024 12:00:00 GMT")
1587: 		w.Write([]byte("<rss><channel><title>Test</title></channel></rss>"))
1588: 	}))
1589: 	defer server.Close()
1590:
1591: 	result, err := Fetch(server.URL, nil, nil)
1592: 	if err != nil {
1593: 		t.Fatalf("Fetch failed: %v", err)
1594: 	}
1595:
1596: 	if result.NotModified {
1597: 		t.Error("expected fresh content, got NotModified")
1598: 	}
1599: 	if len(result.Body) == 0 {
1600: 		t.Error("expected body content")
1601: 	}
1602: 	if result.ETag != `"abc123"` {
1603: 		t.Errorf("expected ETag, got %s", result.ETag)
1604: 	}
1605: }
1606:
1607: func TestFetch_Cached(t *testing.T) {
1608: 	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
1609: 		if r.Header.Get("If-None-Match") == `"abc123"` {
1610: 			w.WriteHeader(http.StatusNotModified)
1611: 			return
1612: 		}
1613: 		w.Write([]byte("content"))
1614: 	}))
1615: 	defer server.Close()
1616:
1617: 	etag := `"abc123"`
1618: 	result, err := Fetch(server.URL, &etag, nil)
1619: 	if err != nil {
1620: 		t.Fatalf("Fetch failed: %v", err)
1621: 	}
1622:
1623: 	if !result.NotModified {
1624: 		t.Error("expected NotModified for cached content")
1625: 	}
1626: }
1627:
1628: func TestFetch_Error(t *testing.T) {
1629: 	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
1630: 		w.WriteHeader(http.StatusNotFound)
1631: 	}))
1632: 	defer server.Close()
1633:
1634: 	_, err := Fetch(server.URL, nil, nil)
1635: 	if err == nil {
1636: 		t.Error("expected error for 404 response")
1637: 	}
1638: }
1639: ```
1640:
1641: **Step 2: Run test to verify it fails**
1642:
1643: Run: `go test ./internal/fetch/... -v`
1644: Expected: FAIL - package doesn't exist
1645:
1646: **Step 3: Write minimal implementation**
1647:
1648: ```go
1649: // ABOUTME: HTTP feed fetching with conditional request support
1650: // ABOUTME: Handles ETag and Last-Modified headers for efficient caching
1651:
1652: package fetch
1653:
1654: import (
1655: 	"fmt"
1656: 	"io"
1657: 	"net/http"
1658: 	"time"
1659: )
1660:
1661: type Result struct {
1662: 	Body         []byte
1663: 	ETag         string
1664: 	LastModified string
1665: 	NotModified  bool
1666: }
1667:
1668: var client = &http.Client{
1669: 	Timeout: 30 * time.Second,
1670: }
1671:
1672: func Fetch(url string, etag, lastModified *string) (*Result, error) {
1673: 	req, err := http.NewRequest("GET", url, nil)
1674: 	if err != nil {
1675: 		return nil, fmt.Errorf("failed to create request: %w", err)
1676: 	}
1677:
1678: 	req.Header.Set("User-Agent", "digest/1.0 (RSS reader)")
1679:
1680: 	if etag != nil && *etag != "" {
1681: 		req.Header.Set("If-None-Match", *etag)
1682: 	}
1683: 	if lastModified != nil && *lastModified != "" {
1684: 		req.Header.Set("If-Modified-Since", *lastModified)
1685: 	}
1686:
1687: 	resp, err := client.Do(req)
1688: 	if err != nil {
1689: 		return nil, fmt.Errorf("request failed: %w", err)
1690: 	}
1691: 	defer resp.Body.Close()
1692:
1693: 	if resp.StatusCode == http.StatusNotModified {
1694: 		return &Result{NotModified: true}, nil
1695: 	}
1696:
1697: 	if resp.StatusCode != http.StatusOK {
1698: 		return nil, fmt.Errorf("unexpected status: %d %s", resp.StatusCode, resp.Status)
1699: 	}
1700:
1701: 	body, err := io.ReadAll(resp.Body)
1702: 	if err != nil {
1703: 		return nil, fmt.Errorf("failed to read body: %w", err)
1704: 	}
1705:
1706: 	return &Result{
1707: 		Body:         body,
1708: 		ETag:         resp.Header.Get("ETag"),
1709: 		LastModified: resp.Header.Get("Last-Modified"),
1710: 		NotModified:  false,
1711: 	}, nil
1712: }
1713: ```
1714:
1715: **Step 4: Run test to verify it passes**
1716:
1717: Run: `go test ./internal/fetch/... -v`
1718: Expected: PASS
1719:
1720: **Step 5: Commit**
1721:
1722: ```bash
1723: git add internal/fetch/
1724: git commit -m "feat: add HTTP fetcher with ETag/Last-Modified caching"
1725: ```
1726:
1727: ---
1728:
1729: ## Phase 6: Feed Parsing
1730:
1731: ### Task 6.1: RSS/Atom Parser Integration
1732:
1733: **Files:**
1734: - Create: `internal/parse/parse.go`
1735: - Create: `internal/parse/parse_test.go`
1736:
1737: **Step 1: Write the failing test**
1738:
1739: ```go
1740: // ABOUTME: Tests for RSS/Atom feed parsing
1741: // ABOUTME: Validates conversion from gofeed to internal models
1742:
1743: package parse
1744:
1745: import (
1746: 	"testing"
1747: )
1748:
1749: func TestParse_RSS(t *testing.T) {
1750: 	rss := `<?xml version="1.0"?>
1751: <rss version="2.0">
1752:   <channel>
1753:     <title>Test Feed</title>
1754:     <item>
1755:       <title>Test Entry</title>
1756:       <link>https://example.com/post</link>
1757:       <guid>https://example.com/post</guid>
1758:       <pubDate>Tue, 10 Dec 2024 12:00:00 GMT</pubDate>
1759:       <description>Test content</description>
1760:     </item>
1761:   </channel>
1762: </rss>`
1763:
1764: 	result, err := Parse([]byte(rss))
1765: 	if err != nil {
1766: 		t.Fatalf("Parse failed: %v", err)
1767: 	}
1768:
1769: 	if result.Title != "Test Feed" {
1770: 		t.Errorf("expected title 'Test Feed', got %s", result.Title)
1771: 	}
1772: 	if len(result.Entries) != 1 {
1773: 		t.Fatalf("expected 1 entry, got %d", len(result.Entries))
1774: 	}
1775: 	if result.Entries[0].Title != "Test Entry" {
1776: 		t.Errorf("expected entry title 'Test Entry', got %s", result.Entries[0].Title)
1777: 	}
1778: }
1779:
1780: func TestParse_Atom(t *testing.T) {
1781: 	atom := `<?xml version="1.0"?>
1782: <feed xmlns="http://www.w3.org/2005/Atom">
1783:   <title>Test Atom Feed</title>
1784:   <entry>
1785:     <title>Atom Entry</title>
1786:     <id>urn:uuid:123</id>
1787:     <link href="https://example.com/atom"/>
1788:     <updated>2024-12-10T12:00:00Z</updated>
1789:     <content>Atom content</content>
1790:   </entry>
1791: </feed>`
1792:
1793: 	result, err := Parse([]byte(atom))
1794: 	if err != nil {
1795: 		t.Fatalf("Parse failed: %v", err)
1796: 	}
1797:
1798: 	if result.Title != "Test Atom Feed" {
1799: 		t.Errorf("expected title 'Test Atom Feed', got %s", result.Title)
1800: 	}
1801: 	if len(result.Entries) != 1 {
1802: 		t.Fatalf("expected 1 entry, got %d", len(result.Entries))
1803: 	}
1804: }
1805: ```
1806:
1807: **Step 2: Run test to verify it fails**
1808:
1809: Run: `go test ./internal/parse/... -v`
1810: Expected: FAIL - package doesn't exist
1811:
1812: **Step 3: Write minimal implementation**
1813:
1814: ```go
1815: // ABOUTME: RSS/Atom feed parsing using gofeed library
1816: // ABOUTME: Converts external feed formats to internal ParsedFeed structure
1817:
1818: package parse
1819:
1820: import (
1821: 	"bytes"
1822: 	"fmt"
1823: 	"time"
1824:
1825: 	"github.com/mmcdole/gofeed"
1826: )
1827:
1828: type ParsedFeed struct {
1829: 	Title   string
1830: 	Entries []ParsedEntry
1831: }
1832:
1833: type ParsedEntry struct {
1834: 	GUID        string
1835: 	Title       string
1836: 	Link        string
1837: 	Author      string
1838: 	PublishedAt *time.Time
1839: 	Content     string
1840: 	Categories  []string
1841: }
1842:
1843: func Parse(data []byte) (*ParsedFeed, error) {
1844: 	fp := gofeed.NewParser()
1845: 	feed, err := fp.Parse(bytes.NewReader(data))
1846: 	if err != nil {
1847: 		return nil, fmt.Errorf("failed to parse feed: %w", err)
1848: 	}
1849:
1850: 	result := &ParsedFeed{
1851: 		Title: feed.Title,
1852: 	}
1853:
1854: 	for _, item := range feed.Items {
1855: 		entry := ParsedEntry{
1856: 			GUID:       item.GUID,
1857: 			Title:      item.Title,
1858: 			Link:       item.Link,
1859: 			Categories: item.Categories,
1860: 		}
1861:
1862: 		if entry.GUID == "" {
1863: 			entry.GUID = item.Link
1864: 		}
1865:
1866: 		if item.Author != nil {
1867: 			entry.Author = item.Author.Name
1868: 		}
1869:
1870: 		if item.PublishedParsed != nil {
1871: 			entry.PublishedAt = item.PublishedParsed
1872: 		} else if item.UpdatedParsed != nil {
1873: 			entry.PublishedAt = item.UpdatedParsed
1874: 		}
1875:
1876: 		if item.Content != "" {
1877: 			entry.Content = item.Content
1878: 		} else {
1879: 			entry.Content = item.Description
1880: 		}
1881:
1882: 		result.Entries = append(result.Entries, entry)
1883: 	}
1884:
1885: 	return result, nil
1886: }
1887: ```
1888:
1889: **Step 4: Run test to verify it passes**
1890:
1891: Run: `go test ./internal/parse/... -v`
1892: Expected: PASS
1893:
1894: **Step 5: Commit**
1895:
1896: ```bash
1897: git add internal/parse/
1898: git commit -m "feat: add RSS/Atom parsing with gofeed"
1899: ```
1900:
1901: ---
1902:
1903: ## Phase 7: CLI Commands
1904:
1905: ### Task 7.1: Database/OPML Initialization in Root
1906:
1907: **Files:**
1908: - Modify: `cmd/digest/root.go`
1909:
1910: **Step 1: Update root.go with initialization**
1911:
1912: ```go
1913: // ABOUTME: Root Cobra command and global flags
1914: // ABOUTME: Sets up CLI structure and initializes database/OPML
1915:
1916: package main
1917:
1918: import (
1919: 	"database/sql"
1920: 	"fmt"
1921: 	"os"
1922:
1923: 	"github.com/harper/digest/internal/db"
1924: 	"github.com/harper/digest/internal/opml"
1925: 	"github.com/spf13/cobra"
1926: )
1927:
1928: var (
1929: 	dbPath   string
1930: 	opmlPath string
1931: 	dbConn   *sql.DB
1932: 	opmlDoc  *opml.Document
1933: )
1934:
1935: var rootCmd = &cobra.Command{
1936: 	Use:   "digest",
1937: 	Short: "RSS/Atom feed tracker with MCP integration",
1938: 	Long: `
1939: ██████╗ ██╗ ██████╗ ███████╗███████╗████████╗
1940: ██╔══██╗██║██╔════╝ ██╔════╝██╔════╝╚══██╔══╝
1941: ██║  ██║██║██║  ███╗█████╗  ███████╗   ██║
1942: ██║  ██║██║██║   ██║██╔══╝  ╚════██║   ██║
1943: ██████╔╝██║╚██████╔╝███████╗███████║   ██║
1944: ╚═════╝ ╚═╝ ╚═════╝ ╚══════╝╚══════╝   ╚═╝
1945:
1946: RSS/Atom feed tracker for humans and AI agents.
1947:
1948: Track feeds, sync content, and expose via MCP for Claude.`,
1949: 	PersistentPreRunE: func(cmd *cobra.Command, args []string) error {
1950: 		if dbPath == "" {
1951: 			dbPath = db.GetDefaultDBPath()
1952: 		}
1953: 		if opmlPath == "" {
1954: 			opmlPath = db.GetDefaultOPMLPath()
1955: 		}
1956:
1957: 		var err error
1958: 		dbConn, err = db.InitDB(dbPath)
1959: 		if err != nil {
1960: 			return fmt.Errorf("failed to initialize database: %w", err)
1961: 		}
1962:
1963: 		if _, err := os.Stat(opmlPath); os.IsNotExist(err) {
1964: 			opmlDoc = opml.NewDocument("digest feeds")
1965: 		} else {
1966: 			opmlDoc, err = opml.ParseFile(opmlPath)
1967: 			if err != nil {
1968: 				return fmt.Errorf("failed to load OPML: %w", err)
1969: 			}
1970: 		}
1971:
1972: 		return nil
1973: 	},
1974: 	PersistentPostRunE: func(cmd *cobra.Command, args []string) error {
1975: 		if dbConn != nil {
1976: 			return dbConn.Close()
1977: 		}
1978: 		return nil
1979: 	},
1980: }
1981:
1982: func Execute() error {
1983: 	return rootCmd.Execute()
1984: }
1985:
1986: func init() {
1987: 	rootCmd.PersistentFlags().StringVar(&dbPath, "db", "", "database file path")
1988: 	rootCmd.PersistentFlags().StringVar(&opmlPath, "opml", "", "OPML file path")
1989: }
1990:
1991: func saveOPML() error {
1992: 	return opmlDoc.WriteFile(opmlPath)
1993: }
1994: ```
1995:
1996: **Step 2: Verify it compiles and runs**
1997:
1998: Run: `go build ./cmd/digest && ./digest --help`
1999: Expected: Shows help, no errors
2000:
2001: **Step 3: Commit**
2002:
2003: ```bash
2004: git add cmd/digest/
2005: git commit -m "feat: add database and OPML initialization to root command"
2006: ```
2007:
2008: ---
2009:
2010: ### Task 7.2: Feed Add Command
2011:
2012: **Files:**
2013: - Create: `cmd/digest/feed.go`
2014:
2015: **Step 1: Create feed command with add subcommand**
2016:
2017: ```go
2018: // ABOUTME: Feed management commands
2019: // ABOUTME: Add, remove, list, and import RSS/Atom feeds
2020:
2021: package main
2022:
2023: import (
2024: 	"fmt"
2025:
2026: 	"github.com/harper/digest/internal/db"
2027: 	"github.com/harper/digest/internal/models"
2028: 	"github.com/spf13/cobra"
2029: )
2030:
2031: var feedCmd = &cobra.Command{
2032: 	Use:     "feed",
2033: 	Aliases: []string{"f"},
2034: 	Short:   "Manage RSS/Atom feeds",
2035: }
2036:
2037: var feedAddCmd = &cobra.Command{
2038: 	Use:   "add <url>",
2039: 	Short: "Add a new feed",
2040: 	Args:  cobra.ExactArgs(1),
2041: 	RunE: func(cmd *cobra.Command, args []string) error {
2042: 		url := args[0]
2043: 		folder, _ := cmd.Flags().GetString("folder")
2044: 		title, _ := cmd.Flags().GetString("title")
2045:
2046: 		existing, err := db.GetFeedByURL(dbConn, url)
2047: 		if err == nil && existing != nil {
2048: 			return fmt.Errorf("feed already exists: %s", url)
2049: 		}
2050:
2051: 		feed := models.NewFeed(url)
2052: 		if title != "" {
2053: 			feed.Title = &title
2054: 		}
2055:
2056: 		if err := db.CreateFeed(dbConn, feed); err != nil {
2057: 			return fmt.Errorf("failed to create feed: %w", err)
2058: 		}
2059:
2060: 		displayTitle := url
2061: 		if title != "" {
2062: 			displayTitle = title
2063: 		}
2064: 		opmlDoc.AddFeed(url, displayTitle, folder)
2065: 		if err := saveOPML(); err != nil {
2066: 			return fmt.Errorf("failed to save OPML: %w", err)
2067: 		}
2068:
2069: 		fmt.Printf("Added feed: %s\n", url)
2070: 		if folder != "" {
2071: 			fmt.Printf("  Folder: %s\n", folder)
2072: 		}
2073: 		return nil
2074: 	},
2075: }
2076:
2077: var feedListCmd = &cobra.Command{
2078: 	Use:     "list",
2079: 	Aliases: []string{"ls"},
2080: 	Short:   "List all feeds",
2081: 	RunE: func(cmd *cobra.Command, args []string) error {
2082: 		feeds := opmlDoc.AllFeeds()
2083: 		if len(feeds) == 0 {
2084: 			fmt.Println("No feeds configured. Use 'digest feed add <url>' to add one.")
2085: 			return nil
2086: 		}
2087:
2088: 		for _, f := range feeds {
2089: 			if f.Folder != "" {
2090: 				fmt.Printf("[%s] %s - %s\n", f.Folder, f.Title, f.URL)
2091: 			} else {
2092: 				fmt.Printf("%s - %s\n", f.Title, f.URL)
2093: 			}
2094: 		}
2095: 		return nil
2096: 	},
2097: }
2098:
2099: var feedRemoveCmd = &cobra.Command{
2100: 	Use:   "remove <url>",
2101: 	Short: "Remove a feed",
2102: 	Args:  cobra.ExactArgs(1),
2103: 	RunE: func(cmd *cobra.Command, args []string) error {
2104: 		url := args[0]
2105:
2106: 		feed, err := db.GetFeedByURL(dbConn, url)
2107: 		if err != nil {
2108: 			return fmt.Errorf("feed not found: %s", url)
2109: 		}
2110:
2111: 		if err := db.DeleteFeed(dbConn, feed.ID); err != nil {
2112: 			return fmt.Errorf("failed to delete feed: %w", err)
2113: 		}
2114:
2115: 		opmlDoc.RemoveFeed(url)
2116: 		if err := saveOPML(); err != nil {
2117: 			return fmt.Errorf("failed to save OPML: %w", err)
2118: 		}
2119:
2120: 		fmt.Printf("Removed feed: %s\n", url)
2121: 		return nil
2122: 	},
2123: }
2124:
2125: func init() {
2126: 	rootCmd.AddCommand(feedCmd)
2127: 	feedCmd.AddCommand(feedAddCmd)
2128: 	feedCmd.AddCommand(feedListCmd)
2129: 	feedCmd.AddCommand(feedRemoveCmd)
2130:
2131: 	feedAddCmd.Flags().StringP("folder", "f", "", "folder to add feed to")
2132: 	feedAddCmd.Flags().StringP("title", "t", "", "title for the feed")
2133: }
2134: ```
2135:
2136: **Step 2: Verify commands work**
2137:
2138: Run: `go build ./cmd/digest && ./digest feed --help`
2139: Expected: Shows feed subcommands
2140:
2141: **Step 3: Commit**
2142:
2143: ```bash
2144: git add cmd/digest/
2145: git commit -m "feat: add feed management commands (add, list, remove)"
2146: ```
2147:
2148: ---
2149:
2150: ### Task 7.3: Sync Command
2151:
2152: **Files:**
2153: - Create: `cmd/digest/sync.go`
2154:
2155: **Step 1: Create sync command**
2156:
2157: ```go
2158: // ABOUTME: Sync command for fetching feed content
2159: // ABOUTME: Handles HTTP caching and entry deduplication
2160:
2161: package main
2162:
2163: import (
2164: 	"fmt"
2165: 	"time"
2166:
2167: 	"github.com/fatih/color"
2168: 	"github.com/harper/digest/internal/db"
2169: 	"github.com/harper/digest/internal/fetch"
2170: 	"github.com/harper/digest/internal/models"
2171: 	"github.com/harper/digest/internal/parse"
2172: 	"github.com/spf13/cobra"
2173: )
2174:
2175: var syncCmd = &cobra.Command{
2176: 	Use:   "sync [url]",
2177: 	Short: "Fetch new entries from feeds",
2178: 	RunE: func(cmd *cobra.Command, args []string) error {
2179: 		force, _ := cmd.Flags().GetBool("force")
2180:
2181: 		feeds, err := db.ListFeeds(dbConn)
2182: 		if err != nil {
2183: 			return fmt.Errorf("failed to list feeds: %w", err)
2184: 		}
2185:
2186: 		if len(feeds) == 0 {
2187: 			fmt.Println("No feeds to sync. Use 'digest feed add <url>' to add one.")
2188: 			return nil
2189: 		}
2190:
2191: 		if len(args) > 0 {
2192: 			feed, err := db.GetFeedByURL(dbConn, args[0])
2193: 			if err != nil {
2194: 				feed, err = db.GetFeedByPrefix(dbConn, args[0])
2195: 				if err != nil {
2196: 					return fmt.Errorf("feed not found: %s", args[0])
2197: 				}
2198: 			}
2199: 			feeds = []*models.Feed{feed}
2200: 		}
2201:
2202: 		fmt.Printf("Syncing %d feeds...\n", len(feeds))
2203:
2204: 		var totalNew, cached, errors int
2205: 		for _, feed := range feeds {
2206: 			newCount, wasCached, err := syncFeed(feed, force)
2207: 			if err != nil {
2208: 				color.Red("  ✗ %s (%v)\n", feedDisplayName(feed), err)
2209: 				errors++
2210: 				_ = db.UpdateFeedError(dbConn, feed.ID, err.Error())
2211: 				continue
2212: 			}
2213:
2214: 			if wasCached {
2215: 				color.HiBlack("  - %s (not modified)\n", feedDisplayName(feed))
2216: 				cached++
2217: 			} else if newCount > 0 {
2218: 				color.Green("  ✓ %s +%d new\n", feedDisplayName(feed), newCount)
2219: 				totalNew += newCount
2220: 			} else {
2221: 				color.Green("  ✓ %s (no new entries)\n", feedDisplayName(feed))
2222: 			}
2223: 		}
2224:
2225: 		fmt.Printf("\nSynced %d new entries from %d feeds (%d cached, %d errors)\n",
2226: 			totalNew, len(feeds)-errors, cached, errors)
2227: 		return nil
2228: 	},
2229: }
2230:
2231: func syncFeed(feed *models.Feed, force bool) (int, bool, error) {
2232: 	var etag, lastMod *string
2233: 	if !force {
2234: 		etag = feed.ETag
2235: 		lastMod = feed.LastModified
2236: 	}
2237:
2238: 	result, err := fetch.Fetch(feed.URL, etag, lastMod)
2239: 	if err != nil {
2240: 		return 0, false, err
2241: 	}
2242:
2243: 	if result.NotModified {
2244: 		return 0, true, nil
2245: 	}
2246:
2247: 	parsed, err := parse.Parse(result.Body)
2248: 	if err != nil {
2249: 		return 0, false, err
2250: 	}
2251:
2252: 	if feed.Title == nil || *feed.Title == "" {
2253: 		feed.Title = &parsed.Title
2254: 	}
2255:
2256: 	var newCount int
2257: 	for _, pe := range parsed.Entries {
2258: 		exists, _ := db.EntryExists(dbConn, feed.ID, pe.GUID)
2259: 		if exists {
2260: 			continue
2261: 		}
2262:
2263: 		entry := models.NewEntry(feed.ID, pe.GUID, pe.Title)
2264: 		entry.Link = &pe.Link
2265: 		entry.Author = &pe.Author
2266: 		entry.PublishedAt = pe.PublishedAt
2267: 		entry.Content = &pe.Content
2268:
2269: 		if err := db.CreateEntry(dbConn, entry); err != nil {
2270: 			continue
2271: 		}
2272: 		newCount++
2273: 	}
2274:
2275: 	now := time.Now()
2276: 	feed.LastFetchedAt = &now
2277: 	feed.SetCacheHeaders(result.ETag, result.LastModified)
2278: 	_ = db.UpdateFeed(dbConn, feed)
2279:
2280: 	return newCount, false, nil
2281: }
2282:
2283: func feedDisplayName(feed *models.Feed) string {
2284: 	if feed.Title != nil && *feed.Title != "" {
2285: 		return *feed.Title
2286: 	}
2287: 	return feed.URL
2288: }
2289:
2290: func init() {
2291: 	rootCmd.AddCommand(syncCmd)
2292: 	syncCmd.Flags().Bool("force", false, "ignore cache headers")
2293: }
2294: ```
2295:
2296: **Step 2: Verify command compiles**
2297:
2298: Run: `go build ./cmd/digest && ./digest sync --help`
2299: Expected: Shows sync help
2300:
2301: **Step 3: Commit**
2302:
2303: ```bash
2304: git add cmd/digest/
2305: git commit -m "feat: add sync command with HTTP caching support"
2306: ```
2307:
2308: ---
2309:
2310: ### Task 7.4: List Entries Command
2311:
2312: **Files:**
2313: - Create: `cmd/digest/list.go`
2314:
2315: **Step 1: Create list command**
2316:
2317: ```go
2318: // ABOUTME: List entries command
2319: // ABOUTME: Shows feed entries with filtering options
2320:
2321: package main
2322:
2323: import (
2324: 	"fmt"
2325: 	"time"
2326:
2327: 	"github.com/fatih/color"
2328: 	"github.com/harper/digest/internal/db"
2329: 	"github.com/spf13/cobra"
2330: )
2331:
2332: var listCmd = &cobra.Command{
2333: 	Use:     "list",
2334: 	Aliases: []string{"ls", "l"},
2335: 	Short:   "List feed entries",
2336: 	RunE: func(cmd *cobra.Command, args []string) error {
2337: 		all, _ := cmd.Flags().GetBool("all")
2338: 		feedFilter, _ := cmd.Flags().GetString("feed")
2339: 		limit, _ := cmd.Flags().GetInt("limit")
2340:
2341: 		var feedID *string
2342: 		if feedFilter != "" {
2343: 			feed, err := db.GetFeedByURL(dbConn, feedFilter)
2344: 			if err != nil {
2345: 				feed, err = db.GetFeedByPrefix(dbConn, feedFilter)
2346: 				if err != nil {
2347: 					return fmt.Errorf("feed not found: %s", feedFilter)
2348: 				}
2349: 			}
2350: 			feedID = &feed.ID
2351: 		}
2352:
2353: 		unreadOnly := !all
2354: 		entries, err := db.ListEntries(dbConn, feedID, &unreadOnly, nil, limit)
2355: 		if err != nil {
2356: 			return fmt.Errorf("failed to list entries: %w", err)
2357: 		}
2358:
2359: 		if len(entries) == 0 {
2360: 			if all {
2361: 				fmt.Println("No entries found.")
2362: 			} else {
2363: 				fmt.Println("No unread entries. Use --all to see read entries.")
2364: 			}
2365: 			return nil
2366: 		}
2367:
2368: 		faint := color.New(color.Faint)
2369: 		for _, entry := range entries {
2370: 			prefix := entry.ID[:8]
2371: 			title := "Untitled"
2372: 			if entry.Title != nil {
2373: 				title = *entry.Title
2374: 			}
2375:
2376: 			readMark := " "
2377: 			if entry.Read {
2378: 				readMark = "✓"
2379: 			}
2380:
2381: 			pubDate := ""
2382: 			if entry.PublishedAt != nil {
2383: 				pubDate = entry.PublishedAt.Format(time.RFC822)
2384: 			}
2385:
2386: 			faint.Printf("%s ", prefix)
2387: 			fmt.Printf("%s %s", readMark, title)
2388: 			if pubDate != "" {
2389: 				faint.Printf(" (%s)", pubDate)
2390: 			}
2391: 			fmt.Println()
2392: 		}
2393:
2394: 		return nil
2395: 	},
2396: }
2397:
2398: func init() {
2399: 	rootCmd.AddCommand(listCmd)
2400: 	listCmd.Flags().BoolP("all", "a", false, "show all entries including read")
2401: 	listCmd.Flags().StringP("feed", "f", "", "filter by feed URL or ID prefix")
2402: 	listCmd.Flags().IntP("limit", "n", 20, "maximum entries to show")
2403: }
2404: ```
2405:
2406: **Step 2: Verify command compiles**
2407:
2408: Run: `go build ./cmd/digest && ./digest list --help`
2409: Expected: Shows list help with flags
2410:
2411: **Step 3: Commit**
2412:
2413: ```bash
2414: git add cmd/digest/
2415: git commit -m "feat: add list command for viewing entries"
2416: ```
2417:
2418: ---
2419:
2420: ### Task 7.5: Read/Unread Commands
2421:
2422: **Files:**
2423: - Create: `cmd/digest/read.go`
2424:
2425: **Step 1: Create read and unread commands**
2426:
2427: ```go
2428: // ABOUTME: Read state management commands
2429: // ABOUTME: Mark entries as read or unread
2430:
2431: package main
2432:
2433: import (
2434: 	"fmt"
2435:
2436: 	"github.com/harper/digest/internal/db"
2437: 	"github.com/spf13/cobra"
2438: )
2439:
2440: var readCmd = &cobra.Command{
2441: 	Use:   "read <entry-prefix>",
2442: 	Short: "Mark an entry as read",
2443: 	Args:  cobra.ExactArgs(1),
2444: 	RunE: func(cmd *cobra.Command, args []string) error {
2445: 		prefix := args[0]
2446:
2447: 		entry, err := db.GetEntryByPrefix(dbConn, prefix)
2448: 		if err != nil {
2449: 			return fmt.Errorf("entry not found: %s", prefix)
2450: 		}
2451:
2452: 		if err := db.MarkEntryRead(dbConn, entry.ID); err != nil {
2453: 			return fmt.Errorf("failed to mark read: %w", err)
2454: 		}
2455:
2456: 		title := "Untitled"
2457: 		if entry.Title != nil {
2458: 			title = *entry.Title
2459: 		}
2460: 		fmt.Printf("Marked as read: %s\n", title)
2461: 		return nil
2462: 	},
2463: }
2464:
2465: var unreadCmd = &cobra.Command{
2466: 	Use:   "unread <entry-prefix>",
2467: 	Short: "Mark an entry as unread",
2468: 	Args:  cobra.ExactArgs(1),
2469: 	RunE: func(cmd *cobra.Command, args []string) error {
2470: 		prefix := args[0]
2471:
2472: 		entry, err := db.GetEntryByPrefix(dbConn, prefix)
2473: 		if err != nil {
2474: 			return fmt.Errorf("entry not found: %s", prefix)
2475: 		}
2476:
2477: 		if err := db.MarkEntryUnread(dbConn, entry.ID); err != nil {
2478: 			return fmt.Errorf("failed to mark unread: %w", err)
2479: 		}
2480:
2481: 		title := "Untitled"
2482: 		if entry.Title != nil {
2483: 			title = *entry.Title
2484: 		}
2485: 		fmt.Printf("Marked as unread: %s\n", title)
2486: 		return nil
2487: 	},
2488: }
2489:
2490: func init() {
2491: 	rootCmd.AddCommand(readCmd)
2492: 	rootCmd.AddCommand(unreadCmd)
2493: }
2494: ```
2495:
2496: **Step 2: Verify commands compile**
2497:
2498: Run: `go build ./cmd/digest && ./digest read --help`
2499: Expected: Shows read help
2500:
2501: **Step 3: Commit**
2502:
2503: ```bash
2504: git add cmd/digest/
2505: git commit -m "feat: add read/unread commands for entry state"
2506: ```
2507:
2508: ---
2509:
2510: ### Task 7.6: Open Command
2511:
2512: **Files:**
2513: - Create: `cmd/digest/open.go`
2514:
2515: **Step 1: Create open command**
2516:
2517: ```go
2518: // ABOUTME: Open entry link in browser
2519: // ABOUTME: Opens URL and marks entry as read
2520:
2521: package main
2522:
2523: import (
2524: 	"fmt"
2525: 	"os/exec"
2526: 	"runtime"
2527:
2528: 	"github.com/harper/digest/internal/db"
2529: 	"github.com/spf13/cobra"
2530: )
2531:
2532: var openCmd = &cobra.Command{
2533: 	Use:   "open <entry-prefix>",
2534: 	Short: "Open entry link in browser and mark as read",
2535: 	Args:  cobra.ExactArgs(1),
2536: 	RunE: func(cmd *cobra.Command, args []string) error {
2537: 		prefix := args[0]
2538:
2539: 		entry, err := db.GetEntryByPrefix(dbConn, prefix)
2540: 		if err != nil {
2541: 			return fmt.Errorf("entry not found: %s", prefix)
2542: 		}
2543:
2544: 		if entry.Link == nil || *entry.Link == "" {
2545: 			return fmt.Errorf("entry has no link")
2546: 		}
2547:
2548: 		if err := openBrowser(*entry.Link); err != nil {
2549: 			return fmt.Errorf("failed to open browser: %w", err)
2550: 		}
2551:
2552: 		_ = db.MarkEntryRead(dbConn, entry.ID)
2553:
2554: 		title := "Untitled"
2555: 		if entry.Title != nil {
2556: 			title = *entry.Title
2557: 		}
2558: 		fmt.Printf("Opened: %s\n", title)
2559: 		return nil
2560: 	},
2561: }
2562:
2563: func openBrowser(url string) error {
2564: 	var cmd *exec.Cmd
2565: 	switch runtime.GOOS {
2566: 	case "darwin":
2567: 		cmd = exec.Command("open", url)
2568: 	case "linux":
2569: 		cmd = exec.Command("xdg-open", url)
2570: 	case "windows":
2571: 		cmd = exec.Command("rundll32", "url.dll,FileProtocolHandler", url)
2572: 	default:
2573: 		return fmt.Errorf("unsupported platform: %s", runtime.GOOS)
2574: 	}
2575: 	return cmd.Start()
2576: }
2577:
2578: func init() {
2579: 	rootCmd.AddCommand(openCmd)
2580: }
2581: ```
2582:
2583: **Step 2: Verify command compiles**
2584:
2585: Run: `go build ./cmd/digest && ./digest open --help`
2586: Expected: Shows open help
2587:
2588: **Step 3: Commit**
2589:
2590: ```bash
2591: git add cmd/digest/
2592: git commit -m "feat: add open command to launch browser"
2593: ```
2594:
2595: ---
2596:
2597: ### Task 7.7: Folder Commands
2598:
2599: **Files:**
2600: - Create: `cmd/digest/folder.go`
2601:
2602: **Step 1: Create folder commands**
2603:
2604: ```go
2605: // ABOUTME: Folder management commands
2606: // ABOUTME: Create and list OPML folders
2607:
2608: package main
2609:
2610: import (
2611: 	"fmt"
2612:
2613: 	"github.com/spf13/cobra"
2614: )
2615:
2616: var folderCmd = &cobra.Command{
2617: 	Use:   "folder",
2618: 	Short: "Manage feed folders",
2619: }
2620:
2621: var folderAddCmd = &cobra.Command{
2622: 	Use:   "add <name>",
2623: 	Short: "Create a new folder",
2624: 	Args:  cobra.ExactArgs(1),
2625: 	RunE: func(cmd *cobra.Command, args []string) error {
2626: 		name := args[0]
2627:
2628: 		opmlDoc.AddFolder(name)
2629: 		if err := saveOPML(); err != nil {
2630: 			return fmt.Errorf("failed to save OPML: %w", err)
2631: 		}
2632:
2633: 		fmt.Printf("Created folder: %s\n", name)
2634: 		return nil
2635: 	},
2636: }
2637:
2638: var folderListCmd = &cobra.Command{
2639: 	Use:     "list",
2640: 	Aliases: []string{"ls"},
2641: 	Short:   "List all folders",
2642: 	RunE: func(cmd *cobra.Command, args []string) error {
2643: 		folders := opmlDoc.Folders()
2644: 		if len(folders) == 0 {
2645: 			fmt.Println("No folders. Use 'digest folder add <name>' to create one.")
2646: 			return nil
2647: 		}
2648:
2649: 		for _, f := range folders {
2650: 			feeds := opmlDoc.FeedsInFolder(f)
2651: 			fmt.Printf("%s (%d feeds)\n", f, len(feeds))
2652: 		}
2653: 		return nil
2654: 	},
2655: }
2656:
2657: func init() {
2658: 	rootCmd.AddCommand(folderCmd)
2659: 	folderCmd.AddCommand(folderAddCmd)
2660: 	folderCmd.AddCommand(folderListCmd)
2661: }
2662: ```
2663:
2664: **Step 2: Verify commands compile**
2665:
2666: Run: `go build ./cmd/digest && ./digest folder --help`
2667: Expected: Shows folder subcommands
2668:
2669: **Step 3: Commit**
2670:
2671: ```bash
2672: git add cmd/digest/
2673: git commit -m "feat: add folder management commands"
2674: ```
2675:
2676: ---
2677:
2678: ### Task 7.8: Export Command
2679:
2680: **Files:**
2681: - Create: `cmd/digest/export.go`
2682:
2683: **Step 1: Create export command**
2684:
2685: ```go
2686: // ABOUTME: Export OPML to stdout
2687: // ABOUTME: Allows backing up or transferring subscriptions
2688:
2689: package main
2690:
2691: import (
2692: 	"os"
2693:
2694: 	"github.com/spf13/cobra"
2695: )
2696:
2697: var exportCmd = &cobra.Command{
2698: 	Use:   "export",
2699: 	Short: "Export OPML to stdout",
2700: 	RunE: func(cmd *cobra.Command, args []string) error {
2701: 		return opmlDoc.Write(os.Stdout)
2702: 	},
2703: }
2704:
2705: func init() {
2706: 	rootCmd.AddCommand(exportCmd)
2707: }
2708: ```
2709:
2710: **Step 2: Verify command compiles**
2711:
2712: Run: `go build ./cmd/digest && ./digest export --help`
2713: Expected: Shows export help
2714:
2715: **Step 3: Commit**
2716:
2717: ```bash
2718: git add cmd/digest/
2719: git commit -m "feat: add export command for OPML output"
2720: ```
2721:
2722: ---
2723:
2724: ## Phase 8: Integration Testing
2725:
2726: ### Task 8.1: End-to-End Workflow Test
2727:
2728: **Files:**
2729: - Create: `test/integration_test.go`
2730:
2731: **Step 1: Write integration test**
2732:
2733: ```go
2734: // ABOUTME: Integration tests for digest CLI
2735: // ABOUTME: Tests end-to-end workflows with real feeds
2736:
2737: package test
2738:
2739: import (
2740: 	"database/sql"
2741: 	"os"
2742: 	"path/filepath"
2743: 	"testing"
2744:
2745: 	"github.com/harper/digest/internal/db"
2746: 	"github.com/harper/digest/internal/fetch"
2747: 	"github.com/harper/digest/internal/models"
2748: 	"github.com/harper/digest/internal/opml"
2749: 	"github.com/harper/digest/internal/parse"
2750: )
2751:
2752: func TestFullWorkflow(t *testing.T) {
2753: 	tmpDir := t.TempDir()
2754: 	dbPath := filepath.Join(tmpDir, "test.db")
2755: 	opmlPath := filepath.Join(tmpDir, "feeds.opml")
2756:
2757: 	conn, err := db.InitDB(dbPath)
2758: 	if err != nil {
2759: 		t.Fatalf("InitDB failed: %v", err)
2760: 	}
2761: 	defer conn.Close()
2762:
2763: 	doc := opml.NewDocument("Test Feeds")
2764:
2765: 	url := "https://xkcd.com/rss.xml"
2766: 	feed := models.NewFeed(url)
2767: 	if err := db.CreateFeed(conn, feed); err != nil {
2768: 		t.Fatalf("CreateFeed failed: %v", err)
2769: 	}
2770: 	doc.AddFeed(url, "XKCD", "")
2771: 	doc.WriteFile(opmlPath)
2772:
2773: 	result, err := fetch.Fetch(url, nil, nil)
2774: 	if err != nil {
2775: 		t.Fatalf("Fetch failed: %v", err)
2776: 	}
2777:
2778: 	parsed, err := parse.Parse(result.Body)
2779: 	if err != nil {
2780: 		t.Fatalf("Parse failed: %v", err)
2781: 	}
2782:
2783: 	if len(parsed.Entries) == 0 {
2784: 		t.Fatal("expected entries from XKCD feed")
2785: 	}
2786:
2787: 	for _, pe := range parsed.Entries {
2788: 		entry := models.NewEntry(feed.ID, pe.GUID, pe.Title)
2789: 		entry.Link = &pe.Link
2790: 		entry.Content = &pe.Content
2791: 		entry.PublishedAt = pe.PublishedAt
2792: 		_ = db.CreateEntry(conn, entry)
2793: 	}
2794:
2795: 	unreadOnly := true
2796: 	entries, err := db.ListEntries(conn, nil, &unreadOnly, nil, 100)
2797: 	if err != nil {
2798: 		t.Fatalf("ListEntries failed: %v", err)
2799: 	}
2800: 	if len(entries) == 0 {
2801: 		t.Fatal("expected unread entries")
2802: 	}
2803:
2804: 	if err := db.MarkEntryRead(conn, entries[0].ID); err != nil {
2805: 		t.Fatalf("MarkEntryRead failed: %v", err)
2806: 	}
2807:
2808: 	unreadAfter, _ := db.ListEntries(conn, nil, &unreadOnly, nil, 100)
2809: 	if len(unreadAfter) >= len(entries) {
2810: 		t.Error("expected fewer unread entries after marking read")
2811: 	}
2812: }
2813:
2814: func TestOPMLRoundTrip(t *testing.T) {
2815: 	tmpDir := t.TempDir()
2816: 	path := filepath.Join(tmpDir, "test.opml")
2817:
2818: 	doc := opml.NewDocument("Test")
2819: 	doc.AddFolder("Tech")
2820: 	doc.AddFeed("https://example.com/feed1.xml", "Feed 1", "Tech")
2821: 	doc.AddFeed("https://example.com/feed2.xml", "Feed 2", "")
2822:
2823: 	if err := doc.WriteFile(path); err != nil {
2824: 		t.Fatalf("WriteFile failed: %v", err)
2825: 	}
2826:
2827: 	loaded, err := opml.ParseFile(path)
2828: 	if err != nil {
2829: 		t.Fatalf("ParseFile failed: %v", err)
2830: 	}
2831:
2832: 	feeds := loaded.AllFeeds()
2833: 	if len(feeds) != 2 {
2834: 		t.Errorf("expected 2 feeds, got %d", len(feeds))
2835: 	}
2836:
2837: 	folders := loaded.Folders()
2838: 	if len(folders) != 1 || folders[0] != "Tech" {
2839: 		t.Errorf("expected Tech folder, got %v", folders)
2840: 	}
2841: }
2842:
2843: func TestCachedSync(t *testing.T) {
2844: 	url := "https://xkcd.com/rss.xml"
2845:
2846: 	result1, err := fetch.Fetch(url, nil, nil)
2847: 	if err != nil {
2848: 		t.Fatalf("First fetch failed: %v", err)
2849: 	}
2850:
2851: 	if result1.ETag == "" && result1.LastModified == "" {
2852: 		t.Skip("Feed doesn't support caching headers")
2853: 	}
2854:
2855: 	etag := result1.ETag
2856: 	lastMod := result1.LastModified
2857:
2858: 	result2, err := fetch.Fetch(url, &etag, &lastMod)
2859: 	if err != nil {
2860: 		t.Fatalf("Second fetch failed: %v", err)
2861: 	}
2862:
2863: 	if !result2.NotModified {
2864: 		t.Log("Feed was modified between requests (or doesn't honor cache headers)")
2865: 	}
2866: }
2867: ```
2868:
2869: **Step 2: Run integration tests**
2870:
2871: Run: `go test ./test/... -v`
2872: Expected: PASS (requires network)
2873:
2874: **Step 3: Commit**
2875:
2876: ```bash
2877: git add test/
2878: git commit -m "test: add integration tests for full workflow"
2879: ```
2880:
2881: ---
2882:
2883: ## Phase 9: MCP Integration
2884:
2885: ### Task 9.1: MCP Server Setup
2886:
2887: **Files:**
2888: - Create: `internal/mcp/server.go`
2889: - Create: `cmd/digest/mcp.go`
2890:
2891: **Step 1: Create MCP server**
2892:
2893: ```go
2894: // ABOUTME: MCP server initialization
2895: // ABOUTME: Exposes digest functionality to AI agents
2896:
2897: package mcp
2898:
2899: import (
2900: 	"database/sql"
2901:
2902: 	"github.com/harper/digest/internal/opml"
2903: 	"github.com/mark3labs/mcp-go/server"
2904: )
2905:
2906: type Server struct {
2907: 	mcpServer *server.MCPServer
2908: 	db        *sql.DB
2909: 	opmlDoc   *opml.Document
2910: 	opmlPath  string
2911: }
2912:
2913: func NewServer(db *sql.DB, opmlDoc *opml.Document, opmlPath string) *Server {
2914: 	s := &Server{
2915: 		db:       db,
2916: 		opmlDoc:  opmlDoc,
2917: 		opmlPath: opmlPath,
2918: 	}
2919:
2920: 	s.mcpServer = server.NewMCPServer(
2921: 		"digest",
2922: 		"1.0.0",
2923: 		server.WithResourceCapabilities(true, false),
2924: 		server.WithPromptCapabilities(true),
2925: 	)
2926:
2927: 	s.registerTools()
2928: 	s.registerResources()
2929: 	s.registerPrompts()
2930:
2931: 	return s
2932: }
2933:
2934: func (s *Server) ServeStdio() error {
2935: 	return server.ServeStdio(s.mcpServer)
2936: }
2937: ```
2938:
2939: **Step 2: Create MCP command**
2940:
2941: ```go
2942: // ABOUTME: MCP server command
2943: // ABOUTME: Starts MCP server for AI agent integration
2944:
2945: package main
2946:
2947: import (
2948: 	"github.com/harper/digest/internal/mcp"
2949: 	"github.com/spf13/cobra"
2950: )
2951:
2952: var mcpCmd = &cobra.Command{
2953: 	Use:   "mcp",
2954: 	Short: "Start MCP server for AI agents",
2955: 	RunE: func(cmd *cobra.Command, args []string) error {
2956: 		server := mcp.NewServer(dbConn, opmlDoc, opmlPath)
2957: 		return server.ServeStdio()
2958: 	},
2959: }
2960:
2961: func init() {
2962: 	rootCmd.AddCommand(mcpCmd)
2963: }
2964: ```
2965:
2966: **Step 3: Commit stub**
2967:
2968: ```bash
2969: git add internal/mcp/ cmd/digest/mcp.go
2970: git commit -m "feat: add MCP server stub"
2971: ```
2972:
2973: ---
2974:
2975: ### Task 9.2: MCP Tools Implementation
2976:
2977: **Files:**
2978: - Create: `internal/mcp/tools.go`
2979:
2980: **Step 1: Implement MCP tools**
2981:
2982: ```go
2983: // ABOUTME: MCP tool implementations
2984: // ABOUTME: CRUD operations for feeds and entries via MCP
2985:
2986: package mcp
2987:
2988: import (
2989: 	"context"
2990: 	"encoding/json"
2991: 	"fmt"
2992: 	"time"
2993:
2994: 	"github.com/harper/digest/internal/db"
2995: 	"github.com/harper/digest/internal/fetch"
2996: 	"github.com/harper/digest/internal/models"
2997: 	"github.com/harper/digest/internal/parse"
2998: 	"github.com/mark3labs/mcp-go/mcp"
2999: 	"github.com/mark3labs/mcp-go/server"
3000: )
3001:
3002: func (s *Server) registerTools() {
3003: 	s.mcpServer.AddTool(mcp.NewTool("list_feeds",
3004: 		mcp.WithDescription("List all subscribed feeds"),
3005: 	), s.listFeedsHandler)
3006:
3007: 	s.mcpServer.AddTool(mcp.NewTool("add_feed",
3008: 		mcp.WithDescription("Add a new feed subscription"),
3009: 		mcp.WithString("url", mcp.Required(), mcp.Description("Feed URL")),
3010: 		mcp.WithString("title", mcp.Description("Optional title")),
3011: 		mcp.WithString("folder", mcp.Description("Optional folder name")),
3012: 	), s.addFeedHandler)
3013:
3014: 	s.mcpServer.AddTool(mcp.NewTool("remove_feed",
3015: 		mcp.WithDescription("Remove a feed subscription"),
3016: 		mcp.WithString("url", mcp.Required(), mcp.Description("Feed URL to remove")),
3017: 	), s.removeFeedHandler)
3018:
3019: 	s.mcpServer.AddTool(mcp.NewTool("sync_feeds",
3020: 		mcp.WithDescription("Fetch new entries from all or specific feed"),
3021: 		mcp.WithString("url", mcp.Description("Optional: specific feed URL to sync")),
3022: 	), s.syncFeedsHandler)
3023:
3024: 	s.mcpServer.AddTool(mcp.NewTool("list_entries",
3025: 		mcp.WithDescription("List feed entries with optional filters"),
3026: 		mcp.WithString("feed_id", mcp.Description("Filter by feed ID")),
3027: 		mcp.WithBoolean("unread_only", mcp.Description("Only show unread entries")),
3028: 		mcp.WithNumber("limit", mcp.Description("Max entries to return (default 20)")),
3029: 	), s.listEntriesHandler)
3030:
3031: 	s.mcpServer.AddTool(mcp.NewTool("mark_read",
3032: 		mcp.WithDescription("Mark an entry as read"),
3033: 		mcp.WithString("entry_id", mcp.Required(), mcp.Description("Entry ID or prefix")),
3034: 	), s.markReadHandler)
3035:
3036: 	s.mcpServer.AddTool(mcp.NewTool("mark_unread",
3037: 		mcp.WithDescription("Mark an entry as unread"),
3038: 		mcp.WithString("entry_id", mcp.Required(), mcp.Description("Entry ID or prefix")),
3039: 	), s.markUnreadHandler)
3040: }
3041:
3042: func (s *Server) listFeedsHandler(ctx context.Context, req mcp.CallToolRequest) (*mcp.CallToolResult, error) {
3043: 	feeds := s.opmlDoc.AllFeeds()
3044: 	data, _ := json.MarshalIndent(feeds, "", "  ")
3045: 	return mcp.NewToolResultText(string(data)), nil
3046: }
3047:
3048: func (s *Server) addFeedHandler(ctx context.Context, req mcp.CallToolRequest) (*mcp.CallToolResult, error) {
3049: 	url := req.Params.Arguments["url"].(string)
3050: 	title, _ := req.Params.Arguments["title"].(string)
3051: 	folder, _ := req.Params.Arguments["folder"].(string)
3052:
3053: 	feed := models.NewFeed(url)
3054: 	if title != "" {
3055: 		feed.Title = &title
3056: 	}
3057:
3058: 	if err := db.CreateFeed(s.db, feed); err != nil {
3059: 		return mcp.NewToolResultError(fmt.Sprintf("failed to create feed: %v", err)), nil
3060: 	}
3061:
3062: 	displayTitle := url
3063: 	if title != "" {
3064: 		displayTitle = title
3065: 	}
3066: 	s.opmlDoc.AddFeed(url, displayTitle, folder)
3067: 	s.opmlDoc.WriteFile(s.opmlPath)
3068:
3069: 	return mcp.NewToolResultText(fmt.Sprintf("Added feed: %s", url)), nil
3070: }
3071:
3072: func (s *Server) removeFeedHandler(ctx context.Context, req mcp.CallToolRequest) (*mcp.CallToolResult, error) {
3073: 	url := req.Params.Arguments["url"].(string)
3074:
3075: 	feed, err := db.GetFeedByURL(s.db, url)
3076: 	if err != nil {
3077: 		return mcp.NewToolResultError(fmt.Sprintf("feed not found: %s", url)), nil
3078: 	}
3079:
3080: 	if err := db.DeleteFeed(s.db, feed.ID); err != nil {
3081: 		return mcp.NewToolResultError(fmt.Sprintf("failed to delete: %v", err)), nil
3082: 	}
3083:
3084: 	s.opmlDoc.RemoveFeed(url)
3085: 	s.opmlDoc.WriteFile(s.opmlPath)
3086:
3087: 	return mcp.NewToolResultText(fmt.Sprintf("Removed feed: %s", url)), nil
3088: }
3089:
3090: func (s *Server) syncFeedsHandler(ctx context.Context, req mcp.CallToolRequest) (*mcp.CallToolResult, error) {
3091: 	feeds, _ := db.ListFeeds(s.db)
3092:
3093: 	if urlArg, ok := req.Params.Arguments["url"].(string); ok && urlArg != "" {
3094: 		feed, err := db.GetFeedByURL(s.db, urlArg)
3095: 		if err != nil {
3096: 			return mcp.NewToolResultError(fmt.Sprintf("feed not found: %s", urlArg)), nil
3097: 		}
3098: 		feeds = []*models.Feed{feed}
3099: 	}
3100:
3101: 	var totalNew int
3102: 	for _, feed := range feeds {
3103: 		result, err := fetch.Fetch(feed.URL, feed.ETag, feed.LastModified)
3104: 		if err != nil {
3105: 			continue
3106: 		}
3107: 		if result.NotModified {
3108: 			continue
3109: 		}
3110:
3111: 		parsed, err := parse.Parse(result.Body)
3112: 		if err != nil {
3113: 			continue
3114: 		}
3115:
3116: 		for _, pe := range parsed.Entries {
3117: 			exists, _ := db.EntryExists(s.db, feed.ID, pe.GUID)
3118: 			if exists {
3119: 				continue
3120: 			}
3121:
3122: 			entry := models.NewEntry(feed.ID, pe.GUID, pe.Title)
3123: 			entry.Link = &pe.Link
3124: 			entry.Content = &pe.Content
3125: 			entry.PublishedAt = pe.PublishedAt
3126: 			if err := db.CreateEntry(s.db, entry); err == nil {
3127: 				totalNew++
3128: 			}
3129: 		}
3130:
3131: 		now := time.Now()
3132: 		feed.LastFetchedAt = &now
3133: 		feed.SetCacheHeaders(result.ETag, result.LastModified)
3134: 		db.UpdateFeed(s.db, feed)
3135: 	}
3136:
3137: 	return mcp.NewToolResultText(fmt.Sprintf("Synced %d new entries from %d feeds", totalNew, len(feeds))), nil
3138: }
3139:
3140: func (s *Server) listEntriesHandler(ctx context.Context, req mcp.CallToolRequest) (*mcp.CallToolResult, error) {
3141: 	var feedID *string
3142: 	if fid, ok := req.Params.Arguments["feed_id"].(string); ok && fid != "" {
3143: 		feedID = &fid
3144: 	}
3145:
3146: 	unreadOnly := true
3147: 	if uo, ok := req.Params.Arguments["unread_only"].(bool); ok {
3148: 		unreadOnly = uo
3149: 	}
3150:
3151: 	limit := 20
3152: 	if l, ok := req.Params.Arguments["limit"].(float64); ok {
3153: 		limit = int(l)
3154: 	}
3155:
3156: 	entries, err := db.ListEntries(s.db, feedID, &unreadOnly, nil, limit)
3157: 	if err != nil {
3158: 		return mcp.NewToolResultError(fmt.Sprintf("failed to list entries: %v", err)), nil
3159: 	}
3160:
3161: 	data, _ := json.MarshalIndent(entries, "", "  ")
3162: 	return mcp.NewToolResultText(string(data)), nil
3163: }
3164:
3165: func (s *Server) markReadHandler(ctx context.Context, req mcp.CallToolRequest) (*mcp.CallToolResult, error) {
3166: 	entryID := req.Params.Arguments["entry_id"].(string)
3167:
3168: 	entry, err := db.GetEntryByPrefix(s.db, entryID)
3169: 	if err != nil {
3170: 		entry, err = db.GetEntryByID(s.db, entryID)
3171: 		if err != nil {
3172: 			return mcp.NewToolResultError(fmt.Sprintf("entry not found: %s", entryID)), nil
3173: 		}
3174: 	}
3175:
3176: 	if err := db.MarkEntryRead(s.db, entry.ID); err != nil {
3177: 		return mcp.NewToolResultError(fmt.Sprintf("failed to mark read: %v", err)), nil
3178: 	}
3179:
3180: 	return mcp.NewToolResultText(fmt.Sprintf("Marked as read: %s", entry.ID)), nil
3181: }
3182:
3183: func (s *Server) markUnreadHandler(ctx context.Context, req mcp.CallToolRequest) (*mcp.CallToolResult, error) {
3184: 	entryID := req.Params.Arguments["entry_id"].(string)
3185:
3186: 	entry, err := db.GetEntryByPrefix(s.db, entryID)
3187: 	if err != nil {
3188: 		entry, err = db.GetEntryByID(s.db, entryID)
3189: 		if err != nil {
3190: 			return mcp.NewToolResultError(fmt.Sprintf("entry not found: %s", entryID)), nil
3191: 		}
3192: 	}
3193:
3194: 	if err := db.MarkEntryUnread(s.db, entry.ID); err != nil {
3195: 		return mcp.NewToolResultError(fmt.Sprintf("failed to mark unread: %v", err)), nil
3196: 	}
3197:
3198: 	return mcp.NewToolResultText(fmt.Sprintf("Marked as unread: %s", entry.ID)), nil
3199: }
3200: ```
3201:
3202: **Step 2: Commit tools**
3203:
3204: ```bash
3205: git add internal/mcp/tools.go
3206: git commit -m "feat: add MCP tools for feed and entry operations"
3207: ```
3208:
3209: ---
3210:
3211: ### Task 9.3: MCP Resources Implementation
3212:
3213: **Files:**
3214: - Create: `internal/mcp/resources.go`
3215:
3216: **Step 1: Implement MCP resources**
3217:
3218: ```go
3219: // ABOUTME: MCP resource implementations
3220: // ABOUTME: Read-only views of feeds and entries for AI agents
3221:
3222: package mcp
3223:
3224: import (
3225: 	"context"
3226: 	"encoding/json"
3227: 	"fmt"
3228: 	"time"
3229:
3230: 	"github.com/harper/digest/internal/db"
3231: 	"github.com/mark3labs/mcp-go/mcp"
3232: )
3233:
3234: func (s *Server) registerResources() {
3235: 	s.mcpServer.AddResource(mcp.NewResource(
3236: 		"digest://feeds",
3237: 		"All subscribed feeds",
3238: 		mcp.WithResourceMIMEType("application/json"),
3239: 	), s.feedsResourceHandler)
3240:
3241: 	s.mcpServer.AddResource(mcp.NewResource(
3242: 		"digest://entries/unread",
3243: 		"Unread entries",
3244: 		mcp.WithResourceMIMEType("application/json"),
3245: 	), s.unreadEntriesHandler)
3246:
3247: 	s.mcpServer.AddResource(mcp.NewResource(
3248: 		"digest://entries/today",
3249: 		"Today's entries",
3250: 		mcp.WithResourceMIMEType("application/json"),
3251: 	), s.todayEntriesHandler)
3252:
3253: 	s.mcpServer.AddResource(mcp.NewResource(
3254: 		"digest://stats",
3255: 		"Feed statistics",
3256: 		mcp.WithResourceMIMEType("application/json"),
3257: 	), s.statsHandler)
3258: }
3259:
3260: func (s *Server) feedsResourceHandler(ctx context.Context, req mcp.ReadResourceRequest) ([]mcp.ResourceContents, error) {
3261: 	feeds := s.opmlDoc.AllFeeds()
3262: 	data, _ := json.MarshalIndent(feeds, "", "  ")
3263: 	return []mcp.ResourceContents{
3264: 		mcp.NewTextResourceContents(req.Params.URI, "application/json", string(data)),
3265: 	}, nil
3266: }
3267:
3268: func (s *Server) unreadEntriesHandler(ctx context.Context, req mcp.ReadResourceRequest) ([]mcp.ResourceContents, error) {
3269: 	unreadOnly := true
3270: 	entries, err := db.ListEntries(s.db, nil, &unreadOnly, nil, 50)
3271: 	if err != nil {
3272: 		return nil, err
3273: 	}
3274: 	data, _ := json.MarshalIndent(entries, "", "  ")
3275: 	return []mcp.ResourceContents{
3276: 		mcp.NewTextResourceContents(req.Params.URI, "application/json", string(data)),
3277: 	}, nil
3278: }
3279:
3280: func (s *Server) todayEntriesHandler(ctx context.Context, req mcp.ReadResourceRequest) ([]mcp.ResourceContents, error) {
3281: 	today := time.Now().Truncate(24 * time.Hour)
3282: 	entries, err := db.ListEntries(s.db, nil, nil, &today, 50)
3283: 	if err != nil {
3284: 		return nil, err
3285: 	}
3286: 	data, _ := json.MarshalIndent(entries, "", "  ")
3287: 	return []mcp.ResourceContents{
3288: 		mcp.NewTextResourceContents(req.Params.URI, "application/json", string(data)),
3289: 	}, nil
3290: }
3291:
3292: func (s *Server) statsHandler(ctx context.Context, req mcp.ReadResourceRequest) ([]mcp.ResourceContents, error) {
3293: 	feeds, _ := db.ListFeeds(s.db)
3294: 	unreadCount, _ := db.CountUnreadEntries(s.db, nil)
3295:
3296: 	stats := map[string]interface{}{
3297: 		"total_feeds":    len(feeds),
3298: 		"unread_entries": unreadCount,
3299: 		"timestamp":      time.Now().Format(time.RFC3339),
3300: 	}
3301:
3302: 	data, _ := json.MarshalIndent(stats, "", "  ")
3303: 	return []mcp.ResourceContents{
3304: 		mcp.NewTextResourceContents(req.Params.URI, "application/json", string(data)),
3305: 	}, nil
3306: }
3307: ```
3308:
3309: **Step 2: Commit resources**
3310:
3311: ```bash
3312: git add internal/mcp/resources.go
3313: git commit -m "feat: add MCP resources for feed data views"
3314: ```
3315:
3316: ---
3317:
3318: ### Task 9.4: MCP Prompts Implementation
3319:
3320: **Files:**
3321: - Create: `internal/mcp/prompts.go`
3322:
3323: **Step 1: Implement MCP prompts**
3324:
3325: ```go
3326: // ABOUTME: MCP prompt templates
3327: // ABOUTME: Workflow templates for AI agent interactions
3328:
3329: package mcp
3330:
3331: import (
3332: 	"context"
3333:
3334: 	"github.com/mark3labs/mcp-go/mcp"
3335: )
3336:
3337: func (s *Server) registerPrompts() {
3338: 	s.mcpServer.AddPrompt(mcp.NewPrompt("daily-digest",
3339: 		mcp.WithPromptDescription("Summarize today's feed entries"),
3340: 	), s.dailyDigestPrompt)
3341:
3342: 	s.mcpServer.AddPrompt(mcp.NewPrompt("catch-up",
3343: 		mcp.WithPromptDescription("Catch up on missed entries"),
3344: 		mcp.WithArgument("days", mcp.ArgumentDescription("Number of days to look back")),
3345: 	), s.catchUpPrompt)
3346:
3347: 	s.mcpServer.AddPrompt(mcp.NewPrompt("curate-feeds",
3348: 		mcp.WithPromptDescription("Review and suggest feed improvements"),
3349: 	), s.curateFeedsPrompt)
3350: }
3351:
3352: func (s *Server) dailyDigestPrompt(ctx context.Context, req mcp.GetPromptRequest) (*mcp.GetPromptResult, error) {
3353: 	return &mcp.GetPromptResult{
3354: 		Description: "Daily feed digest",
3355: 		Messages: []mcp.PromptMessage{
3356: 			{
3357: 				Role: mcp.RoleUser,
3358: 				Content: mcp.TextContent{
3359: 					Type: "text",
3360: 					Text: `Review today's feed entries and provide a summary:
3361:
3362: 1. First, use the list_entries tool with unread_only=true to get unread entries
3363: 2. Group entries by topic or feed
3364: 3. Highlight the most important or interesting items
3365: 4. Suggest which entries I should read in full
3366: 5. Mark entries as read after summarizing them
3367:
3368: Keep the summary concise but informative.`,
3369: 				},
3370: 			},
3371: 		},
3372: 	}, nil
3373: }
3374:
3375: func (s *Server) catchUpPrompt(ctx context.Context, req mcp.GetPromptRequest) (*mcp.GetPromptResult, error) {
3376: 	days := "3"
3377: 	if d, ok := req.Params.Arguments["days"]; ok {
3378: 		days = d
3379: 	}
3380:
3381: 	return &mcp.GetPromptResult{
3382: 		Description: "Catch up on missed entries",
3383: 		Messages: []mcp.PromptMessage{
3384: 			{
3385: 				Role: mcp.RoleUser,
3386: 				Content: mcp.TextContent{
3387: 					Type: "text",
3388: 					Text: `I've been away for ` + days + ` days. Help me catch up:
3389:
3390: 1. Sync feeds to get latest entries
3391: 2. List all unread entries
3392: 3. Identify the most significant news or updates
3393: 4. Summarize key themes across all feeds
3394: 5. Recommend top 5 entries I should read in full
3395:
3396: Prioritize important updates over routine posts.`,
3397: 				},
3398: 			},
3399: 		},
3400: 	}, nil
3401: }
3402:
3403: func (s *Server) curateFeedsPrompt(ctx context.Context, req mcp.GetPromptRequest) (*mcp.GetPromptResult, error) {
3404: 	return &mcp.GetPromptResult{
3405: 		Description: "Review and curate feeds",
3406: 		Messages: []mcp.PromptMessage{
3407: 			{
3408: 				Role: mcp.RoleUser,
3409: 				Content: mcp.TextContent{
3410: 					Type: "text",
3411: 					Text: `Review my feed subscriptions and suggest improvements:
3412:
3413: 1. List all current feeds
3414: 2. Check for feeds that haven't had new content recently
3415: 3. Identify any feeds with frequent errors
3416: 4. Suggest categories/folders for better organization
3417: 5. Recommend any feeds I might want to add based on my interests
3418:
3419: Provide actionable suggestions for improving my feed collection.`,
3420: 				},
3421: 			},
3422: 		},
3423: 	}, nil
3424: }
3425: ```
3426:
3427: **Step 2: Commit prompts**
3428:
3429: ```bash
3430: git add internal/mcp/prompts.go
3431: git commit -m "feat: add MCP prompts for agent workflows"
3432: ```
3433:
3434: ---
3435:
3436: ## Phase 10: Final Polish
3437:
3438: ### Task 10.1: Add Version Command
3439:
3440: **Files:**
3441: - Create: `cmd/digest/version.go`
3442:
3443: **Step 1: Create version command**
3444:
3445: ```go
3446: // ABOUTME: Version command
3447: // ABOUTME: Displays build version information
3448:
3449: package main
3450:
3451: import (
3452: 	"fmt"
3453:
3454: 	"github.com/spf13/cobra"
3455: )
3456:
3457: var (
3458: 	Version   = "dev"
3459: 	Commit    = "none"
3460: 	BuildDate = "unknown"
3461: )
3462:
3463: var versionCmd = &cobra.Command{
3464: 	Use:   "version",
3465: 	Short: "Print version information",
3466: 	Run: func(cmd *cobra.Command, args []string) {
3467: 		fmt.Printf("digest %s\n", Version)
3468: 		fmt.Printf("  commit: %s\n", Commit)
3469: 		fmt.Printf("  built:  %s\n", BuildDate)
3470: 	},
3471: }
3472:
3473: func init() {
3474: 	rootCmd.AddCommand(versionCmd)
3475: }
3476: ```
3477:
3478: **Step 2: Commit**
3479:
3480: ```bash
3481: git add cmd/digest/version.go
3482: git commit -m "feat: add version command"
3483: ```
3484:
3485: ---
3486:
3487: ### Task 10.2: Add Makefile
3488:
3489: **Files:**
3490: - Create: `Makefile`
3491:
3492: **Step 1: Create Makefile**
3493:
3494: ```makefile
3495: # ABOUTME: Build and development tasks
3496: # ABOUTME: Provides common operations for digest CLI
3497:
3498: .PHONY: build test clean install
3499:
3500: VERSION ?= $(shell git describe --tags --always --dirty 2>/dev/null || echo "dev")
3501: COMMIT ?= $(shell git rev-parse --short HEAD 2>/dev/null || echo "none")
3502: BUILD_DATE ?= $(shell date -u +"%Y-%m-%dT%H:%M:%SZ")
3503:
3504: LDFLAGS = -ldflags "-X main.Version=$(VERSION) -X main.Commit=$(COMMIT) -X main.BuildDate=$(BUILD_DATE)"
3505:
3506: build:
3507: 	go build $(LDFLAGS) -o digest ./cmd/digest
3508:
3509: test:
3510: 	go test ./... -v
3511:
3512: test-coverage:
3513: 	go test ./... -coverprofile=coverage.out
3514: 	go tool cover -html=coverage.out -o coverage.html
3515:
3516: clean:
3517: 	rm -f digest coverage.out coverage.html
3518:
3519: install:
3520: 	go install $(LDFLAGS) ./cmd/digest
3521: ```
3522:
3523: **Step 2: Verify build**
3524:
3525: Run: `make build && ./digest version`
3526: Expected: Shows version info
3527:
3528: **Step 3: Commit**
3529:
3530: ```bash
3531: git add Makefile
3532: git commit -m "chore: add Makefile for build tasks"
3533: ```
3534:
3535: ---
3536:
3537: ### Task 10.3: Run All Tests
3538:
3539: **Step 1: Run full test suite**
3540:
3541: Run: `make test`
3542: Expected: All tests pass
3543:
3544: **Step 2: Final commit if any fixes needed**
3545:
3546: ```bash
3547: git add .
3548: git commit -m "fix: address any test failures"
3549: ```
3550:
3551: ---
3552:
3553: ## Execution Checklist
3554:
3555: After completing all tasks, verify:
3556:
3557: - [ ] `make build` succeeds
3558: - [ ] `make test` passes all tests
3559: - [ ] `./digest --help` shows all commands
3560: - [ ] `./digest feed add https://xkcd.com/rss.xml` adds a feed
3561: - [ ] `./digest sync` fetches entries
3562: - [ ] `./digest list` shows unread entries
3563: - [ ] `./digest read <prefix>` marks entry read
3564: - [ ] `./digest export` outputs valid OPML
3565: - [ ] `./digest mcp` starts without error (Ctrl+C to exit)
3566:
3567: ---
3568:
3569: **Plan complete and saved to `docs/plans/2025-12-10-digest-implementation.md`.**
3570:
3571: **Two execution options:**
3572:
3573: 1. **Subagent-Driven (this session)** - I dispatch fresh subagent per task, review between tasks, fast iteration
3574:
3575: 2. **Parallel Session (separate)** - Open new session with executing-plans, batch execution with checkpoints
3576:
3577: **Which approach, Doctor Biz?**
</file>

<file path="internal/content/content_test.go">
  1: // ABOUTME: Tests for content processing utilities
  2: // ABOUTME: Validates HTML detection and Markdown conversion
  3: package content
  4: import (
  5: 	"strings"
  6: 	"testing"
  7: )
  8: func TestIsHTML(t *testing.T) {
  9: 	tests := []struct {
 10: 		name     string
 11: 		content  string
 12: 		expected bool
 13: 	}{
 14: 		{
 15: 			name:     "plain text",
 16: 			content:  "This is just plain text without any HTML.",
 17: 			expected: false,
 18: 		},
 19: 		{
 20: 			name:     "paragraph tag",
 21: 			content:  "<p>This is a paragraph.</p>",
 22: 			expected: true,
 23: 		},
 24: 		{
 25: 			name:     "div tag",
 26: 			content:  "<div class=\"content\">Some content</div>",
 27: 			expected: true,
 28: 		},
 29: 		{
 30: 			name:     "link tag",
 31: 			content:  "Check out <a href=\"https://example.com\">this link</a>.",
 32: 			expected: true,
 33: 		},
 34: 		{
 35: 			name:     "DOCTYPE",
 36: 			content:  "<!DOCTYPE html><html><body>Test</body></html>",
 37: 			expected: true,
 38: 		},
 39: 		{
 40: 			name:     "br tag",
 41: 			content:  "Line one<br>Line two",
 42: 			expected: true,
 43: 		},
 44: 		{
 45: 			name:     "empty string",
 46: 			content:  "",
 47: 			expected: false,
 48: 		},
 49: 		{
 50: 			name:     "angle brackets but not HTML",
 51: 			content:  "5 < 10 and 10 > 5",
 52: 			expected: false,
 53: 		},
 54: 	}
 55: 	for _, tt := range tests {
 56: 		t.Run(tt.name, func(t *testing.T) {
 57: 			result := IsHTML(tt.content)
 58: 			if result != tt.expected {
 59: 				t.Errorf("IsHTML(%q) = %v, want %v", tt.content, result, tt.expected)
 60: 			}
 61: 		})
 62: 	}
 63: }
 64: func TestToMarkdown(t *testing.T) {
 65: 	tests := []struct {
 66: 		name     string
 67: 		input    string
 68: 		contains []string // strings that should be in the output
 69: 		excludes []string // strings that should NOT be in the output
 70: 	}{
 71: 		{
 72: 			name:     "plain text unchanged",
 73: 			input:    "Just plain text here.",
 74: 			contains: []string{"Just plain text here."},
 75: 		},
 76: 		{
 77: 			name:     "paragraph to text",
 78: 			input:    "<p>A paragraph of text.</p>",
 79: 			contains: []string{"A paragraph of text."},
 80: 			excludes: []string{"<p>", "</p>"},
 81: 		},
 82: 		{
 83: 			name:     "link to markdown",
 84: 			input:    "<a href=\"https://example.com\">Example</a>",
 85: 			contains: []string{"[Example]", "(https://example.com)"},
 86: 			excludes: []string{"<a", "</a>"},
 87: 		},
 88: 		{
 89: 			name:     "bold to markdown",
 90: 			input:    "<strong>Bold text</strong>",
 91: 			contains: []string{"**Bold text**"},
 92: 			excludes: []string{"<strong>"},
 93: 		},
 94: 		{
 95: 			name:     "italic to markdown",
 96: 			input:    "<em>Italic text</em>",
 97: 			contains: []string{"*Italic text*"},
 98: 			excludes: []string{"<em>"},
 99: 		},
100: 		{
101: 			name:     "list to markdown",
102: 			input:    "<ul><li>Item 1</li><li>Item 2</li></ul>",
103: 			contains: []string{"Item 1", "Item 2"},
104: 			excludes: []string{"<ul>", "<li>"},
105: 		},
106: 		{
107: 			name:     "empty string",
108: 			input:    "",
109: 			contains: []string{},
110: 		},
111: 	}
112: 	for _, tt := range tests {
113: 		t.Run(tt.name, func(t *testing.T) {
114: 			result := ToMarkdown(tt.input)
115: 			for _, s := range tt.contains {
116: 				if !strings.Contains(result, s) {
117: 					t.Errorf("ToMarkdown() result should contain %q, got %q", s, result)
118: 				}
119: 			}
120: 			for _, s := range tt.excludes {
121: 				if strings.Contains(result, s) {
122: 					t.Errorf("ToMarkdown() result should NOT contain %q, got %q", s, result)
123: 				}
124: 			}
125: 		})
126: 	}
127: }
</file>

<file path="internal/content/content.go">
 1: // ABOUTME: Content processing utilities for feed entries
 2: // ABOUTME: Detects HTML and converts to Markdown for clean display
 3: package content
 4: import (
 5: 	"regexp"
 6: 	"strings"
 7: 	htmltomarkdown "github.com/JohannesKaufmann/html-to-markdown/v2"
 8: )
 9: // htmlTagPattern matches common HTML tags
10: var htmlTagPattern = regexp.MustCompile(`<\s*(p|div|span|a|br|img|h[1-6]|ul|ol|li|table|tr|td|th|strong|em|b|i|code|pre|blockquote)[^>]*>`)
11: // IsHTML checks if content appears to be HTML
12: func IsHTML(content string) bool {
13: 	// Quick checks for obvious HTML markers
14: 	if strings.Contains(content, "<!DOCTYPE") || strings.Contains(content, "<html") {
15: 		return true
16: 	}
17: 	// Check for common HTML tags
18: 	return htmlTagPattern.MatchString(content)
19: }
20: // ToMarkdown converts HTML content to Markdown
21: // If the content doesn't appear to be HTML, returns it unchanged
22: func ToMarkdown(content string) string {
23: 	if content == "" {
24: 		return content
25: 	}
26: 	if !IsHTML(content) {
27: 		return content
28: 	}
29: 	markdown, err := htmltomarkdown.ConvertString(content)
30: 	if err != nil {
31: 		// If conversion fails, return original content
32: 		return content
33: 	}
34: 	// Clean up excessive whitespace
35: 	markdown = strings.TrimSpace(markdown)
36: 	return markdown
37: }
</file>

<file path="internal/db/db_test.go">
 1: // ABOUTME: Tests for database connection and path helpers
 2: // ABOUTME: Validates XDG path resolution and connection lifecycle
 3: package db
 4: import (
 5: 	"path/filepath"
 6: 	"testing"
 7: )
 8: func TestGetDefaultDBPath(t *testing.T) {
 9: 	path := GetDefaultDBPath()
10: 	if !filepath.IsAbs(path) {
11: 		t.Errorf("expected absolute path, got %s", path)
12: 	}
13: 	if filepath.Base(path) != "digest.db" {
14: 		t.Errorf("expected digest.db, got %s", filepath.Base(path))
15: 	}
16: }
17: func TestGetDefaultOPMLPath(t *testing.T) {
18: 	path := GetDefaultOPMLPath()
19: 	if !filepath.IsAbs(path) {
20: 		t.Errorf("expected absolute path, got %s", path)
21: 	}
22: 	if filepath.Base(path) != "feeds.opml" {
23: 		t.Errorf("expected feeds.opml, got %s", filepath.Base(path))
24: 	}
25: }
26: func TestInitDB(t *testing.T) {
27: 	tmpDir := t.TempDir()
28: 	dbPath := filepath.Join(tmpDir, "test.db")
29: 	conn, err := InitDB(dbPath)
30: 	if err != nil {
31: 		t.Fatalf("InitDB failed: %v", err)
32: 	}
33: 	defer conn.Close()
34: 	// Verify tables exist
35: 	var count int
36: 	err = conn.QueryRow("SELECT COUNT(*) FROM sqlite_master WHERE type='table' AND name='feeds'").Scan(&count)
37: 	if err != nil {
38: 		t.Fatalf("query failed: %v", err)
39: 	}
40: 	if count != 1 {
41: 		t.Error("feeds table not created")
42: 	}
43: }
</file>

<file path="internal/db/feeds_test.go">
 1: // ABOUTME: Tests for feed database operations
 2: // ABOUTME: Validates CRUD operations for feeds table
 3: package db
 4: import (
 5: 	"database/sql"
 6: 	"path/filepath"
 7: 	"testing"
 8: 	"github.com/harper/digest/internal/models"
 9: )
10: func TestCreateFeed(t *testing.T) {
11: 	conn := setupTestDB(t)
12: 	defer conn.Close()
13: 	feed := models.NewFeed("https://example.com/feed.xml")
14: 	feed.Title = strPtr("Test Feed")
15: 	err := CreateFeed(conn, feed)
16: 	if err != nil {
17: 		t.Fatalf("CreateFeed failed: %v", err)
18: 	}
19: 	// Verify it exists
20: 	got, err := GetFeedByURL(conn, feed.URL)
21: 	if err != nil {
22: 		t.Fatalf("GetFeedByURL failed: %v", err)
23: 	}
24: 	if got.ID != feed.ID {
25: 		t.Errorf("expected ID %s, got %s", feed.ID, got.ID)
26: 	}
27: }
28: func TestGetFeedByPrefix(t *testing.T) {
29: 	conn := setupTestDB(t)
30: 	defer conn.Close()
31: 	feed := models.NewFeed("https://example.com/feed.xml")
32: 	_ = CreateFeed(conn, feed)
33: 	// Use first 8 chars of UUID
34: 	prefix := feed.ID[:8]
35: 	got, err := GetFeedByPrefix(conn, prefix)
36: 	if err != nil {
37: 		t.Fatalf("GetFeedByPrefix failed: %v", err)
38: 	}
39: 	if got.ID != feed.ID {
40: 		t.Errorf("expected ID %s, got %s", feed.ID, got.ID)
41: 	}
42: }
43: func TestListFeeds(t *testing.T) {
44: 	conn := setupTestDB(t)
45: 	defer conn.Close()
46: 	feed1 := models.NewFeed("https://example.com/feed1.xml")
47: 	feed2 := models.NewFeed("https://example.com/feed2.xml")
48: 	_ = CreateFeed(conn, feed1)
49: 	_ = CreateFeed(conn, feed2)
50: 	feeds, err := ListFeeds(conn)
51: 	if err != nil {
52: 		t.Fatalf("ListFeeds failed: %v", err)
53: 	}
54: 	if len(feeds) != 2 {
55: 		t.Errorf("expected 2 feeds, got %d", len(feeds))
56: 	}
57: }
58: func TestDeleteFeed(t *testing.T) {
59: 	conn := setupTestDB(t)
60: 	defer conn.Close()
61: 	feed := models.NewFeed("https://example.com/feed.xml")
62: 	_ = CreateFeed(conn, feed)
63: 	err := DeleteFeed(conn, feed.ID)
64: 	if err != nil {
65: 		t.Fatalf("DeleteFeed failed: %v", err)
66: 	}
67: 	_, err = GetFeedByURL(conn, feed.URL)
68: 	if err == nil {
69: 		t.Error("expected feed to be deleted")
70: 	}
71: }
72: func setupTestDB(t *testing.T) *sql.DB {
73: 	t.Helper()
74: 	tmpDir := t.TempDir()
75: 	dbPath := filepath.Join(tmpDir, "test.db")
76: 	conn, err := InitDB(dbPath)
77: 	if err != nil {
78: 		t.Fatalf("failed to init test db: %v", err)
79: 	}
80: 	return conn
81: }
82: func strPtr(s string) *string {
83: 	return &s
84: }
</file>

<file path="internal/discover/discover_test.go">
  1: // ABOUTME: Unit tests for feed discovery package
  2: // ABOUTME: Tests direct feed, HTML link extraction, and common path probing
  3: package discover
  4: import (
  5: 	"net/http"
  6: 	"net/http/httptest"
  7: 	"testing"
  8: )
  9: const testRSSFeed = `<?xml version="1.0" encoding="UTF-8"?>
 10: <rss version="2.0">
 11:   <channel>
 12:     <title>Test Feed</title>
 13:     <link>https://example.com</link>
 14:     <description>A test feed</description>
 15:     <item>
 16:       <title>Test Entry</title>
 17:       <link>https://example.com/entry1</link>
 18:       <guid>entry-1</guid>
 19:     </item>
 20:   </channel>
 21: </rss>`
 22: const testAtomFeed = `<?xml version="1.0" encoding="UTF-8"?>
 23: <feed xmlns="http://www.w3.org/2005/Atom">
 24:   <title>Test Atom Feed</title>
 25:   <link href="https://example.com"/>
 26:   <entry>
 27:     <title>Test Entry</title>
 28:     <link href="https://example.com/entry1"/>
 29:     <id>entry-1</id>
 30:   </entry>
 31: </feed>`
 32: const testHTMLWithFeedLink = `<!DOCTYPE html>
 33: <html>
 34: <head>
 35:   <title>Test Site</title>
 36:   <link rel="alternate" type="application/rss+xml" title="RSS Feed" href="/feed.xml">
 37:   <link rel="alternate" type="application/atom+xml" title="Atom Feed" href="/atom.xml">
 38: </head>
 39: <body>
 40:   <h1>Test Site</h1>
 41: </body>
 42: </html>`
 43: const testHTMLWithRelativeFeedLink = `<!DOCTYPE html>
 44: <html>
 45: <head>
 46:   <title>Test Site</title>
 47:   <link rel="alternate" type="application/rss+xml" href="feed.xml">
 48: </head>
 49: <body></body>
 50: </html>`
 51: const testHTMLNoFeedLinks = `<!DOCTYPE html>
 52: <html>
 53: <head>
 54:   <title>Test Site</title>
 55: </head>
 56: <body>
 57:   <h1>No feeds here</h1>
 58: </body>
 59: </html>`
 60: func TestDiscover_DirectFeed(t *testing.T) {
 61: 	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
 62: 		w.Header().Set("Content-Type", "application/rss+xml")
 63: 		w.Write([]byte(testRSSFeed))
 64: 	}))
 65: 	defer server.Close()
 66: 	feed, err := Discover(server.URL)
 67: 	if err != nil {
 68: 		t.Fatalf("expected no error, got: %v", err)
 69: 	}
 70: 	if feed == nil {
 71: 		t.Fatal("expected feed, got nil")
 72: 	}
 73: 	if feed.URL != server.URL {
 74: 		t.Errorf("expected URL %s, got %s", server.URL, feed.URL)
 75: 	}
 76: 	if feed.Title != "Test Feed" {
 77: 		t.Errorf("expected title 'Test Feed', got '%s'", feed.Title)
 78: 	}
 79: }
 80: func TestDiscover_DirectAtomFeed(t *testing.T) {
 81: 	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
 82: 		w.Header().Set("Content-Type", "application/atom+xml")
 83: 		w.Write([]byte(testAtomFeed))
 84: 	}))
 85: 	defer server.Close()
 86: 	feed, err := Discover(server.URL)
 87: 	if err != nil {
 88: 		t.Fatalf("expected no error, got: %v", err)
 89: 	}
 90: 	if feed == nil {
 91: 		t.Fatal("expected feed, got nil")
 92: 	}
 93: 	if feed.Title != "Test Atom Feed" {
 94: 		t.Errorf("expected title 'Test Atom Feed', got '%s'", feed.Title)
 95: 	}
 96: }
 97: func TestDiscover_HTMLWithFeedLink(t *testing.T) {
 98: 	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
 99: 		switch r.URL.Path {
100: 		case "/":
101: 			w.Header().Set("Content-Type", "text/html")
102: 			w.Write([]byte(testHTMLWithFeedLink))
103: 		case "/feed.xml":
104: 			w.Header().Set("Content-Type", "application/rss+xml")
105: 			w.Write([]byte(testRSSFeed))
106: 		case "/atom.xml":
107: 			w.Header().Set("Content-Type", "application/atom+xml")
108: 			w.Write([]byte(testAtomFeed))
109: 		default:
110: 			http.NotFound(w, r)
111: 		}
112: 	}))
113: 	defer server.Close()
114: 	feed, err := Discover(server.URL)
115: 	if err != nil {
116: 		t.Fatalf("expected no error, got: %v", err)
117: 	}
118: 	if feed == nil {
119: 		t.Fatal("expected feed, got nil")
120: 	}
121: 	expectedURL := server.URL + "/feed.xml"
122: 	if feed.URL != expectedURL {
123: 		t.Errorf("expected URL %s, got %s", expectedURL, feed.URL)
124: 	}
125: 	if feed.Title != "Test Feed" {
126: 		t.Errorf("expected title 'Test Feed', got '%s'", feed.Title)
127: 	}
128: }
129: func TestDiscover_HTMLWithRelativeFeedLink(t *testing.T) {
130: 	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
131: 		switch r.URL.Path {
132: 		case "/blog/":
133: 			w.Header().Set("Content-Type", "text/html")
134: 			w.Write([]byte(testHTMLWithRelativeFeedLink))
135: 		case "/blog/feed.xml":
136: 			w.Header().Set("Content-Type", "application/rss+xml")
137: 			w.Write([]byte(testRSSFeed))
138: 		default:
139: 			http.NotFound(w, r)
140: 		}
141: 	}))
142: 	defer server.Close()
143: 	feed, err := Discover(server.URL + "/blog/")
144: 	if err != nil {
145: 		t.Fatalf("expected no error, got: %v", err)
146: 	}
147: 	if feed == nil {
148: 		t.Fatal("expected feed, got nil")
149: 	}
150: 	expectedURL := server.URL + "/blog/feed.xml"
151: 	if feed.URL != expectedURL {
152: 		t.Errorf("expected URL %s, got %s", expectedURL, feed.URL)
153: 	}
154: }
155: func TestDiscover_ProbeCommonPaths(t *testing.T) {
156: 	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
157: 		switch r.URL.Path {
158: 		case "/":
159: 			w.Header().Set("Content-Type", "text/html")
160: 			w.Write([]byte(testHTMLNoFeedLinks))
161: 		case "/rss.xml":
162: 			w.Header().Set("Content-Type", "application/rss+xml")
163: 			w.Write([]byte(testRSSFeed))
164: 		default:
165: 			http.NotFound(w, r)
166: 		}
167: 	}))
168: 	defer server.Close()
169: 	feed, err := Discover(server.URL)
170: 	if err != nil {
171: 		t.Fatalf("expected no error, got: %v", err)
172: 	}
173: 	if feed == nil {
174: 		t.Fatal("expected feed, got nil")
175: 	}
176: 	expectedURL := server.URL + "/rss.xml"
177: 	if feed.URL != expectedURL {
178: 		t.Errorf("expected URL %s, got %s", expectedURL, feed.URL)
179: 	}
180: }
181: func TestDiscover_NoFeedFound(t *testing.T) {
182: 	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
183: 		w.Header().Set("Content-Type", "text/html")
184: 		w.Write([]byte(testHTMLNoFeedLinks))
185: 	}))
186: 	defer server.Close()
187: 	feed, err := Discover(server.URL)
188: 	if err == nil {
189: 		t.Fatal("expected error, got nil")
190: 	}
191: 	if feed != nil {
192: 		t.Errorf("expected nil feed, got: %+v", feed)
193: 	}
194: 	if err != ErrNoFeedFound {
195: 		t.Errorf("expected ErrNoFeedFound, got: %v", err)
196: 	}
197: }
198: func TestDiscover_InvalidURL(t *testing.T) {
199: 	_, err := Discover("not-a-valid-url")
200: 	if err == nil {
201: 		t.Fatal("expected error for invalid URL")
202: 	}
203: }
204: func TestDiscover_MissingScheme(t *testing.T) {
205: 	_, err := Discover("example.com/feed")
206: 	if err == nil {
207: 		t.Fatal("expected error for URL without scheme")
208: 	}
209: }
210: func TestExtractFeedLinks_MultipleFeeds(t *testing.T) {
211: 	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
212: 		w.Header().Set("Content-Type", "text/html")
213: 		w.Write([]byte(testHTMLWithFeedLink))
214: 	}))
215: 	defer server.Close()
216: 	// Fetch the HTML
217: 	result, _, err := tryDirectFeed(server.URL)
218: 	if result != nil {
219: 		t.Fatal("expected HTML page, not a feed")
220: 	}
221: 	if err != nil {
222: 		t.Fatalf("unexpected error: %v", err)
223: 	}
224: }
225: func TestIsFeedContentType(t *testing.T) {
226: 	tests := []struct {
227: 		contentType string
228: 		expected    bool
229: 	}{
230: 		{"application/rss+xml", true},
231: 		{"application/atom+xml", true},
232: 		{"application/xml", true},
233: 		{"text/xml", true},
234: 		{"text/html", false},
235: 		{"application/json", false},
236: 		{"", false},
237: 	}
238: 	for _, tc := range tests {
239: 		result := isFeedContentType(tc.contentType)
240: 		if result != tc.expected {
241: 			t.Errorf("isFeedContentType(%q) = %v, expected %v", tc.contentType, result, tc.expected)
242: 		}
243: 	}
244: }
245: func TestDiscover_AbsoluteFeedLink(t *testing.T) {
246: 	feedServer := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
247: 		w.Header().Set("Content-Type", "application/rss+xml")
248: 		w.Write([]byte(testRSSFeed))
249: 	}))
250: 	defer feedServer.Close()
251: 	htmlWithAbsoluteLink := `<!DOCTYPE html>
252: <html>
253: <head>
254:   <link rel="alternate" type="application/rss+xml" href="` + feedServer.URL + `/feed">
255: </head>
256: <body></body>
257: </html>`
258: 	htmlServer := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
259: 		w.Header().Set("Content-Type", "text/html")
260: 		w.Write([]byte(htmlWithAbsoluteLink))
261: 	}))
262: 	defer htmlServer.Close()
263: 	feed, err := Discover(htmlServer.URL)
264: 	if err != nil {
265: 		t.Fatalf("expected no error, got: %v", err)
266: 	}
267: 	if feed == nil {
268: 		t.Fatal("expected feed, got nil")
269: 	}
270: 	expectedURL := feedServer.URL + "/feed"
271: 	if feed.URL != expectedURL {
272: 		t.Errorf("expected URL %s, got %s", expectedURL, feed.URL)
273: 	}
274: }
</file>

<file path="internal/discover/discover.go">
  1: // ABOUTME: Feed discovery package for finding RSS/Atom feeds from URLs
  2: // ABOUTME: Supports direct feeds, HTML link headers, and common path probing
  3: package discover
  4: import (
  5: 	"errors"
  6: 	"fmt"
  7: 	"net/url"
  8: 	"strings"
  9: 	"github.com/harper/digest/internal/fetch"
 10: 	"github.com/harper/digest/internal/parse"
 11: 	"golang.org/x/net/html"
 12: )
 13: // Common feed paths to probe when other discovery methods fail
 14: var commonFeedPaths = []string{
 15: 	"/feed.xml",
 16: 	"/feed",
 17: 	"/rss.xml",
 18: 	"/rss",
 19: 	"/atom.xml",
 20: 	"/atom",
 21: 	"/index.xml",
 22: 	"/feed/rss",
 23: 	"/feed/atom",
 24: 	"/feeds/posts/default",
 25: }
 26: // Errors returned by discovery functions
 27: var (
 28: 	ErrNoFeedFound = errors.New("no RSS/Atom feed found at URL")
 29: 	ErrInvalidURL  = errors.New("invalid URL")
 30: )
 31: // DiscoveredFeed represents a feed found during discovery
 32: type DiscoveredFeed struct {
 33: 	URL   string // Absolute URL of the feed
 34: 	Title string // Feed title (from content or link element)
 35: }
 36: // Discover attempts to find an RSS/Atom feed from the given URL.
 37: // It tries the following strategies in order:
 38: //  1. Parse URL as a direct feed
 39: //  2. Parse URL as HTML and extract <link rel="alternate"> headers
 40: //  3. Probe common feed URL patterns
 41: //
 42: // Returns the discovered feed, or an error if none found.
 43: func Discover(inputURL string) (*DiscoveredFeed, error) {
 44: 	parsedURL, err := url.Parse(inputURL)
 45: 	if err != nil {
 46: 		return nil, fmt.Errorf("%w: %v", ErrInvalidURL, err)
 47: 	}
 48: 	if parsedURL.Scheme == "" || parsedURL.Host == "" {
 49: 		return nil, fmt.Errorf("%w: missing scheme or host", ErrInvalidURL)
 50: 	}
 51: 	// Strategy 1: Try direct feed
 52: 	feed, body, err := tryDirectFeed(inputURL)
 53: 	if err != nil {
 54: 		return nil, fmt.Errorf("failed to fetch URL: %w", err)
 55: 	}
 56: 	if feed != nil {
 57: 		return feed, nil
 58: 	}
 59: 	// Strategy 2: Extract feed links from HTML
 60: 	feeds, err := extractFeedLinks(body, parsedURL)
 61: 	if err == nil && len(feeds) > 0 {
 62: 		// Verify the first discovered link is a valid feed
 63: 		for _, candidate := range feeds {
 64: 			verifiedFeed, _, verifyErr := tryDirectFeed(candidate.URL)
 65: 			if verifyErr == nil && verifiedFeed != nil {
 66: 				// Use title from HTML link if feed doesn't have one
 67: 				if verifiedFeed.Title == "" && candidate.Title != "" {
 68: 					verifiedFeed.Title = candidate.Title
 69: 				}
 70: 				return verifiedFeed, nil
 71: 			}
 72: 		}
 73: 	}
 74: 	// Strategy 3: Probe common paths
 75: 	feed, err = probeCommonPaths(parsedURL)
 76: 	if err == nil && feed != nil {
 77: 		return feed, nil
 78: 	}
 79: 	return nil, ErrNoFeedFound
 80: }
 81: // tryDirectFeed attempts to fetch and parse the URL as an RSS/Atom feed.
 82: // Returns the feed if successful, or nil if the content is not a valid feed.
 83: // Also returns the raw body for use in HTML parsing if it's not a feed.
 84: func tryDirectFeed(feedURL string) (*DiscoveredFeed, []byte, error) {
 85: 	result, err := fetch.Fetch(feedURL, nil, nil)
 86: 	if err != nil {
 87: 		return nil, nil, err
 88: 	}
 89: 	// Try to parse as a feed
 90: 	parsed, parseErr := parse.Parse(result.Body)
 91: 	if parseErr != nil {
 92: 		// Not a valid feed, return body for HTML parsing (not an error condition)
 93: 		return nil, result.Body, nil //nolint:nilerr // parseErr means not a feed, which is expected
 94: 	}
 95: 	return &DiscoveredFeed{
 96: 		URL:   feedURL,
 97: 		Title: parsed.Title,
 98: 	}, result.Body, nil
 99: }
100: // extractFeedLinks parses HTML and returns feed URLs from <link rel="alternate"> elements
101: func extractFeedLinks(htmlBody []byte, baseURL *url.URL) ([]DiscoveredFeed, error) {
102: 	doc, err := html.Parse(strings.NewReader(string(htmlBody)))
103: 	if err != nil {
104: 		return nil, err
105: 	}
106: 	var feeds []DiscoveredFeed
107: 	var findLinks func(*html.Node)
108: 	findLinks = func(n *html.Node) {
109: 		if n.Type == html.ElementNode && n.Data == "link" {
110: 			var rel, linkType, href, title string
111: 			for _, attr := range n.Attr {
112: 				switch attr.Key {
113: 				case "rel":
114: 					rel = attr.Val
115: 				case "type":
116: 					linkType = attr.Val
117: 				case "href":
118: 					href = attr.Val
119: 				case "title":
120: 					title = attr.Val
121: 				}
122: 			}
123: 			// Check if this is an alternate feed link
124: 			if rel == "alternate" && isFeedContentType(linkType) && href != "" {
125: 				resolvedURL, err := resolveURL(href, baseURL)
126: 				if err == nil {
127: 					feeds = append(feeds, DiscoveredFeed{
128: 						URL:   resolvedURL,
129: 						Title: title,
130: 					})
131: 				}
132: 			}
133: 		}
134: 		for c := n.FirstChild; c != nil; c = c.NextSibling {
135: 			findLinks(c)
136: 		}
137: 	}
138: 	findLinks(doc)
139: 	return feeds, nil
140: }
141: // probeCommonPaths tries common feed URL patterns against the base URL
142: func probeCommonPaths(baseURL *url.URL) (*DiscoveredFeed, error) {
143: 	// Build base URL without path
144: 	probeBase := &url.URL{
145: 		Scheme: baseURL.Scheme,
146: 		Host:   baseURL.Host,
147: 	}
148: 	for _, path := range commonFeedPaths {
149: 		probeURL := probeBase.String() + path
150: 		feed, _, err := tryDirectFeed(probeURL)
151: 		if err == nil && feed != nil {
152: 			return feed, nil
153: 		}
154: 	}
155: 	return nil, ErrNoFeedFound
156: }
157: // resolveURL resolves a potentially relative URL against a base URL
158: func resolveURL(href string, baseURL *url.URL) (string, error) {
159: 	refURL, err := url.Parse(href)
160: 	if err != nil {
161: 		return "", err
162: 	}
163: 	return baseURL.ResolveReference(refURL).String(), nil
164: }
165: // isFeedContentType checks if the content type indicates a feed
166: func isFeedContentType(contentType string) bool {
167: 	contentType = strings.ToLower(contentType)
168: 	return strings.Contains(contentType, "rss") ||
169: 		strings.Contains(contentType, "atom") ||
170: 		strings.Contains(contentType, "xml")
171: }
</file>

<file path="internal/fetch/fetch_test.go">
 1: // ABOUTME: Tests for HTTP fetcher with ETag and Last-Modified caching support.
 2: // ABOUTME: Uses httptest to simulate server responses including 304 Not Modified.
 3: package fetch_test
 4: import (
 5: 	"net/http"
 6: 	"net/http/httptest"
 7: 	"testing"
 8: 	"github.com/harper/digest/internal/fetch"
 9: )
10: func TestFetch_Fresh(t *testing.T) {
11: 	// Server returns fresh content with cache headers
12: 	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
13: 		// Verify User-Agent header
14: 		if ua := r.Header.Get("User-Agent"); ua != "digest/1.0 (RSS reader)" {
15: 			t.Errorf("expected User-Agent 'digest/1.0 (RSS reader)', got %q", ua)
16: 		}
17: 		w.Header().Set("ETag", `"abc123"`)
18: 		w.Header().Set("Last-Modified", "Mon, 02 Jan 2006 15:04:05 GMT")
19: 		w.WriteHeader(http.StatusOK)
20: 		w.Write([]byte("<rss>test content</rss>"))
21: 	}))
22: 	defer server.Close()
23: 	result, err := fetch.Fetch(server.URL, nil, nil)
24: 	if err != nil {
25: 		t.Fatalf("unexpected error: %v", err)
26: 	}
27: 	if result.NotModified {
28: 		t.Error("expected NotModified=false for fresh fetch")
29: 	}
30: 	if string(result.Body) != "<rss>test content</rss>" {
31: 		t.Errorf("expected body '<rss>test content</rss>', got %q", string(result.Body))
32: 	}
33: 	if result.ETag != `"abc123"` {
34: 		t.Errorf("expected ETag '\"abc123\"', got %q", result.ETag)
35: 	}
36: 	if result.LastModified != "Mon, 02 Jan 2006 15:04:05 GMT" {
37: 		t.Errorf("expected LastModified 'Mon, 02 Jan 2006 15:04:05 GMT', got %q", result.LastModified)
38: 	}
39: }
40: func TestFetch_Cached(t *testing.T) {
41: 	// Server returns 304 Not Modified when If-None-Match matches
42: 	etag := `"abc123"`
43: 	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
44: 		// Verify If-None-Match header was sent
45: 		if inm := r.Header.Get("If-None-Match"); inm != etag {
46: 			t.Errorf("expected If-None-Match %q, got %q", etag, inm)
47: 		}
48: 		// Return 304 Not Modified
49: 		w.WriteHeader(http.StatusNotModified)
50: 	}))
51: 	defer server.Close()
52: 	result, err := fetch.Fetch(server.URL, &etag, nil)
53: 	if err != nil {
54: 		t.Fatalf("unexpected error: %v", err)
55: 	}
56: 	if !result.NotModified {
57: 		t.Error("expected NotModified=true for 304 response")
58: 	}
59: 	if len(result.Body) != 0 {
60: 		t.Errorf("expected empty body for 304 response, got %d bytes", len(result.Body))
61: 	}
62: }
63: func TestFetch_Error(t *testing.T) {
64: 	// Server returns 404, expect error
65: 	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
66: 		w.WriteHeader(http.StatusNotFound)
67: 		w.Write([]byte("Not Found"))
68: 	}))
69: 	defer server.Close()
70: 	result, err := fetch.Fetch(server.URL, nil, nil)
71: 	if err == nil {
72: 		t.Fatal("expected error for 404 response, got nil")
73: 	}
74: 	if result != nil {
75: 		t.Errorf("expected nil result for error case, got %+v", result)
76: 	}
77: }
</file>

<file path="internal/fetch/fetch.go">
 1: // ABOUTME: HTTP fetcher with support for conditional requests using ETag and Last-Modified headers.
 2: // ABOUTME: Returns 304 Not Modified status when content hasn't changed, enabling efficient polling.
 3: package fetch
 4: import (
 5: 	"fmt"
 6: 	"io"
 7: 	"net/http"
 8: 	"time"
 9: )
10: // Result contains the response from an HTTP fetch operation.
11: type Result struct {
12: 	Body         []byte
13: 	ETag         string
14: 	LastModified string
15: 	NotModified  bool
16: }
17: var httpClient = &http.Client{
18: 	Timeout: 30 * time.Second,
19: }
20: // Fetch retrieves a URL with optional conditional request headers.
21: // If etag is provided, sets If-None-Match header.
22: // If lastModified is provided, sets If-Modified-Since header.
23: // Returns NotModified=true for 304 responses.
24: // Returns error for non-200/304 status codes.
25: func Fetch(url string, etag, lastModified *string) (*Result, error) {
26: 	req, err := http.NewRequest("GET", url, nil)
27: 	if err != nil {
28: 		return nil, fmt.Errorf("failed to create request: %w", err)
29: 	}
30: 	req.Header.Set("User-Agent", "digest/1.0 (RSS reader)")
31: 	if etag != nil && *etag != "" {
32: 		req.Header.Set("If-None-Match", *etag)
33: 	}
34: 	if lastModified != nil && *lastModified != "" {
35: 		req.Header.Set("If-Modified-Since", *lastModified)
36: 	}
37: 	resp, err := httpClient.Do(req)
38: 	if err != nil {
39: 		return nil, fmt.Errorf("failed to fetch URL: %w", err)
40: 	}
41: 	defer resp.Body.Close()
42: 	// Handle 304 Not Modified
43: 	if resp.StatusCode == http.StatusNotModified {
44: 		return &Result{
45: 			NotModified: true,
46: 		}, nil
47: 	}
48: 	// Handle non-200 status
49: 	if resp.StatusCode != http.StatusOK {
50: 		return nil, fmt.Errorf("unexpected status code: %d", resp.StatusCode)
51: 	}
52: 	// Read response body
53: 	body, err := io.ReadAll(resp.Body)
54: 	if err != nil {
55: 		return nil, fmt.Errorf("failed to read response body: %w", err)
56: 	}
57: 	return &Result{
58: 		Body:         body,
59: 		ETag:         resp.Header.Get("ETag"),
60: 		LastModified: resp.Header.Get("Last-Modified"),
61: 		NotModified:  false,
62: 	}, nil
63: }
</file>

<file path="internal/mcp/prompts.go">
  1: // ABOUTME: MCP prompt definitions and handlers
  2: // ABOUTME: Provides workflow templates for RSS digest operations
  3: package mcp
  4: import (
  5: 	"context"
  6: 	"fmt"
  7: 	"github.com/mark3labs/mcp-go/mcp"
  8: )
  9: func (s *Server) registerPrompts() {
 10: 	s.registerDailyDigestPrompt()
 11: 	s.registerCatchUpPrompt()
 12: 	s.registerCurateFeedsPrompt()
 13: }
 14: func (s *Server) registerDailyDigestPrompt() {
 15: 	s.mcpServer.AddPrompt(
 16: 		mcp.Prompt{
 17: 			Name:        "daily-digest",
 18: 			Description: "Generate a summary of today's feed entries to catch up on the latest content from your subscriptions",
 19: 			Arguments:   []mcp.PromptArgument{},
 20: 		},
 21: 		s.handleDailyDigest,
 22: 	)
 23: }
 24: //nolint:funlen // Prompt handlers contain large template strings
 25: func (s *Server) handleDailyDigest(_ context.Context, _ mcp.GetPromptRequest) (*mcp.GetPromptResult, error) {
 26: 	template := `# Daily Digest
 27: ## Overview
 28: Review and summarize today's feed entries to stay up-to-date with your RSS/Atom subscriptions. This workflow helps you quickly digest the most important content published today across all your feeds.
 29: ## When to Use
 30: - Daily morning routine to catch up on overnight content
 31: - End of day review to see what you missed
 32: - After syncing feeds to see new content
 33: - When you want a quick overview without reading every entry
 34: ## Workflow Steps
 35: ### Step 1: Check Today's Statistics
 36: Get an overview of today's activity across all feeds.
 37: **Use digest://stats resource:**
 38: - Review total entries published today
 39: - Check per-feed breakdown
 40: - Identify which feeds are most active
 41: **Example:**
 42: - digest://stats shows 45 new entries today
 43: - Top feeds: Tech Blog (12), News Feed (15), Dev Community (8)
 44: ### Step 2: Scan Today's Entries
 45: Review the full list of entries published today.
 46: **Use digest://entries/today resource:**
 47: - See all entries from the past 24 hours
 48: - Sort by feed or publication time
 49: - Note titles, authors, and brief excerpts
 50: **What to look for:**
 51: - Breaking news or time-sensitive content
 52: - Topics matching your current interests
 53: - High-value content from trusted sources
 54: **Example:**
 55: - digest://entries/today → 45 entries
 56: - Scan titles for keywords: "release", "announcement", "breaking"
 57: - Identify 5-10 must-read items
 58: ### Step 3: Prioritize Content
 59: Group entries by importance and relevance.
 60: **Categorize by priority:**
 61: - **Must read now:** Breaking news, urgent updates, deadline-sensitive content
 62: - **Read today:** High-value content, trending topics, important analysis
 63: - **Read later:** Interesting but not time-sensitive, tutorials, long-form
 64: - **Skip:** Low-priority, duplicate coverage, off-topic
 65: **Example prioritization:**
 66: - Must read (3 entries): Product launch announcement, security advisory, industry news
 67: - Read today (7 entries): Analysis pieces, feature announcements, community discussions
 68: - Read later (20 entries): Tutorials, opinion pieces, in-depth guides
 69: - Skip (15 entries): Duplicate coverage, off-topic, low-signal
 70: ### Step 4: Read High-Priority Content
 71: Focus on must-read and high-priority items.
 72: **Reading strategy:**
 73: - Start with must-read items
 74: - Scan headlines and summaries for read-today items
 75: - Use mark_entry_read tool as you go
 76: - Take notes on important insights
 77: **Tips:**
 78: - Set a time limit (e.g., 30 minutes)
 79: - Focus on unique insights, skip duplicate coverage
 80: - Mark entries read even if you skim (keeps tracking accurate)
 81: ### Step 5: Generate Summary
 82: Create a brief digest of key takeaways.
 83: **Summary structure:**
 84: - **Top Stories:** 2-3 most important items with key points
 85: - **Notable Updates:** 3-5 significant but not urgent items
 86: - **Trending Topics:** Themes or patterns across multiple entries
 87: - **Action Items:** Follow-ups, things to investigate, or share
 88: **Example summary:**
 89:     Daily Digest - December 11, 2025
 90:     Top Stories:
 91:     1. Major Framework v5.0 Released - Breaking changes to API, migration guide available
 92:     2. Security Advisory: CVE-2025-1234 - Patch available, affects production systems
 93:     3. Industry Analysis: AI Coding Tools Adoption Study - 67%% of devs now use daily
 94:     Notable Updates:
 95:     - New database features announced at conference
 96:     - Tutorial on performance optimization published
 97:     - Community discussion on best practices heating up
 98:     Trending: AI/ML tools, performance optimization, security updates
 99:     Action Items:
100:     - Review Framework v5.0 migration guide
101:     - Apply security patch to production
102:     - Share AI adoption study with team
103: ### Step 6: Mark Entries and Clean Up
104: Update read status for processed entries.
105: **Use mark_entry_read tool:**
106: - Mark all read entries (even if skimmed)
107: - Leave unread items you want to revisit
108: - Accurate tracking helps with future catch-up
109: **Clean-up actions:**
110: - Mark must-read and read-today items as read
111: - Leave read-later items unread for future sessions
112: - Check digest://entries/unread to see remaining backlog
113: ## Tips and Best Practices
114: - **Consistent timing:** Run daily digest at the same time each day (morning or evening)
115: - **Time-box it:** Set a limit (15-30 minutes) to avoid rabbit holes
116: - **Trust your feeds:** If a feed consistently provides low-value content, unsubscribe
117: - **Themes over individual items:** Look for patterns across multiple entries
118: - **Mark as read liberally:** Better to mark read and maintain clean state
119: - **Focus on unique insights:** Skip duplicate coverage of same story
120: - **Use resources efficiently:** digest://entries/today is faster than filtering manually
121: ## Integration with Other Workflows
122: - **After sync:** Run daily-digest after using sync_feeds tool
123: - **Before catch-up:** Use daily-digest for recent content, catch-up for backlog
124: - **With curate-feeds:** Use digest to identify low-value feeds to remove
125: ## Example Daily Digest Session
126: **Step 1: Stats**
127: - digest://stats → 38 new entries today across 8 feeds
128: - Most active: HackerNews (15), Tech Blog (8), Dev.to (6)
129: **Step 2: Scan**
130: - digest://entries/today → Review all 38 entries
131: - Scan titles and authors
132: - Note 3-4 particularly interesting items
133: **Step 3: Prioritize**
134: - Must read (2): Security advisory, product launch
135: - Read today (5): Analysis pieces, feature announcements
136: - Read later (18): Tutorials, long-form
137: - Skip (13): Duplicate HackerNews discussions
138: **Step 4: Read**
139: - 15 minutes: Read 2 must-read items, skim 5 read-today items
140: - Mark 7 entries as read using mark_entry_read
141: - Take notes on security advisory (action required)
142: **Step 5: Summary**
143: Generated concise summary with top 3 stories, trending topics, 1 action item
144: **Step 6: Cleanup**
145: - Marked 20 entries as read (7 fully read, 13 skipped)
146: - Left 18 entries unread for later
147: - Backlog status: digest://entries/unread shows 156 total unread (manageable)
148: **Ready to create your daily digest?**
149: 1. Check digest://stats for today's overview
150: 2. Review digest://entries/today for all new content
151: 3. Prioritize by importance and relevance
152: 4. Read high-priority items
153: 5. Generate summary with key takeaways
154: 6. Mark entries read and clean up
155: `
156: 	return &mcp.GetPromptResult{
157: 		Description: "Daily digest workflow for today's feed entries",
158: 		Messages: []mcp.PromptMessage{
159: 			{
160: 				Role: mcp.RoleUser,
161: 				Content: mcp.TextContent{
162: 					Type: "text",
163: 					Text: template,
164: 				},
165: 			},
166: 		},
167: 	}, nil
168: }
169: func (s *Server) registerCatchUpPrompt() {
170: 	s.mcpServer.AddPrompt(
171: 		mcp.Prompt{
172: 			Name:        "catch-up",
173: 			Description: "Catch up on missed entries from recent days when you've fallen behind on your RSS feeds",
174: 			Arguments: []mcp.PromptArgument{
175: 				{
176: 					Name:        "days",
177: 					Description: "Number of days to catch up on (default: 7)",
178: 					Required:    false,
179: 				},
180: 			},
181: 		},
182: 		s.handleCatchUp,
183: 	)
184: }
185: //nolint:funlen // Prompt handlers contain large template strings
186: func (s *Server) handleCatchUp(_ context.Context, req mcp.GetPromptRequest) (*mcp.GetPromptResult, error) {
187: 	days := "7"
188: 	if req.Params.Arguments != nil {
189: 		if d, ok := req.Params.Arguments["days"]; ok && d != "" {
190: 			days = d
191: 		}
192: 	}
193: 	template := fmt.Sprintf(`# Catch Up on Missed Entries
194: ## Overview
195: Efficiently process a backlog of unread entries from the past %s days when you've fallen behind on your RSS subscriptions. This workflow helps you triage, prioritize, and quickly get back to inbox zero without reading every single item.
196: ## When to Use
197: - After vacation or time away from feeds
198: - When unread count has grown unmanageable
199: - Weekly cleanup to process accumulated entries
200: - After subscribing to several new feeds at once
201: - When digest://stats shows large unread backlog
202: ## Workflow Steps
203: ### Step 1: Assess the Backlog
204: Understand the scope of what you're catching up on.
205: **Use digest://stats resource:**
206: - Check total unread count
207: - Review per-feed breakdown
208: - Identify which feeds have the most unread
209: **Questions to answer:**
210: - How many total unread entries?
211: - Which feeds are the biggest contributors?
212: - Are any feeds consistently high-volume low-value?
213: **Example:**
214: - digest://stats → 347 unread entries (yikes!)
215: - Top contributors: HackerNews (156), Reddit Dev (89), Tech Blog (45)
216: - Observation: HackerNews and Reddit are very high-volume
217: ### Step 2: Set Realistic Goals
218: Be honest about what you can process in %s days of catch-up.
219: **Triage strategy:**
220: - **Can't read everything:** Accept that you'll skip most items
221: - **Focus on high-value:** Prioritize feeds with best signal-to-noise
222: - **Time-box it:** Allocate 30-60 minutes total, not per feed
223: - **Aim for progress:** Reducing backlog by 50%% is success
224: **Goal setting:**
225: - Total backlog: 347 entries
226: - Time available: 45 minutes
227: - Realistic goal: Process top 3 feeds (~200 entries), mark rest as read
228: - Acceptable outcome: Identify 10-15 must-read items, skip the rest
229: ### Step 3: Process High-Value Feeds First
230: Start with feeds that consistently provide the best content.
231: **For each high-value feed:**
232: 1. Use list_entries with feed_id filter
233: 2. Scan titles and dates quickly
234: 3. Mark interesting items for reading
235: 4. Bulk mark the rest as read
236: **Processing strategy per feed:**
237: - **30 seconds:** Scan all titles
238: - **2 minutes:** Read summaries of interesting items
239: - **5 minutes:** Deep read 1-2 must-read items
240: - **Bulk action:** Mark remaining entries as read
241: **Example - Tech Blog (45 entries):**
242: - Scan: 30 seconds to review all titles
243: - Identify: 5 interesting items
244: - Read: 5 minutes on 2 critical posts
245: - Mark read: Remaining 40 entries (use mark_entry_read)
246: ### Step 4: Handle High-Volume Low-Signal Feeds
247: Deal with feeds that produce lots of content but limited value.
248: **Strategies:**
249: - **Skim mode:** Read only titles, mark all as read
250: - **Spot check:** Read 2-3 recent items, mark rest as read
251: - **Declare bankruptcy:** Mark entire feed's backlog as read
252: - **Consider unsubscribing:** Use curate-feeds workflow
253: **High-volume feed processing (e.g., HackerNews - 156 entries):**
254: - Don't read everything - impossible and unnecessary
255: - Scan last 2 days of titles (maybe 30 entries)
256: - Pick 2-3 most interesting discussions
257: - Mark all 156 as read and move on
258: **Feed bankruptcy:**
259: When a feed has >100 unread and low value:
260: 1. Accept you won't read them
261: 2. Mark all as read in bulk
262: 3. Start fresh from today
263: 4. Consider if you really need this subscription
264: ### Step 5: Identify Must-Read Content
265: Extract the truly important items from the backlog.
266: **Criteria for must-read:**
267: - **Time-sensitive:** Security advisories, breaking news, deadlines
268: - **High relevance:** Directly applicable to current work/interests
269: - **Unique insights:** Content you can't get elsewhere
270: - **Trusted sources:** Known high-quality authors
271: **NOT must-read:**
272: - Duplicate coverage of same story
273: - Generic tutorials (can find anytime)
274: - Opinion pieces (unless exceptional)
275: - Old news (already happened, you missed it, move on)
276: **Example must-read list:**
277: From 347 unread, identified:
278: 1. Security patch announcement (Tech Blog)
279: 2. Framework release notes (Dev Community)
280: 3. Industry analysis piece (Newsletter)
281: 4. Interview with expert (Podcast Blog)
282: Total: 4 items to actually read deeply
283: ### Step 6: Execute Bulk Actions
284: Efficiently mark processed content as read.
285: **Use mark_entry_read tool:**
286: - Mark must-read items as read after reading
287: - Bulk mark skipped feeds as read
288: - Leave a few interesting items unread if you want to revisit
289: **Bulk processing tips:**
290: - Process by feed (mark entire feed at once)
291: - Use list_entries to get entry IDs
292: - Mark in batches to avoid rate limits
293: **Example bulk actions:**
294: - HackerNews: Mark all 156 as read (bankruptcy)
295: - Reddit Dev: Mark all 89 as read (low signal)
296: - Tech Blog: Mark 43 as read (kept 2 unread)
297: - Newsletters: Mark 30 as read (kept 4 must-read)
298: - Result: 347 → 6 unread (clean slate!)
299: ### Step 7: Create Catch-Up Summary
300: Document what you learned and what needs follow-up.
301: **Summary structure:**
302: - **Backlog processed:** Starting count → ending count
303: - **Must-read items:** List of truly important content
304: - **Key takeaways:** Main themes or insights
305: - **Action items:** Follow-ups from catch-up session
306: - **Feed health:** Notes on which feeds to keep/remove
307: **Example summary:**
308:     Catch-Up Summary - %s Days
309:     Processed: 347 → 6 unread (98%%%% reduction)
310:     Must-Read Items:
311:     1. [Tech Blog] Security Advisory CVE-2025-5678 - Action required
312:     2. [Dev Community] Framework 6.0 Release - Migration needed
313:     3. [Newsletter] State of Developer Tools 2025 - Informative
314:     4. [Podcast] Expert Interview on AI - Worth listening
315:     Key Takeaways:
316:     - Security patches needed urgently
317:     - Major framework upgrade coming, plan migration
318:     - AI tools becoming industry standard
319:     Action Items:
320:     - [ ] Apply security patch by Friday
321:     - [ ] Schedule framework upgrade planning meeting
322:     - [ ] Review AI tools for team evaluation
323:     Feed Health:
324:     - Keep: Tech Blog, Dev Community, Newsletter (high signal)
325:     - Consider removing: HackerNews, Reddit Dev (high volume, low personal relevance)
326:     - Total feeds: 8, manageable if processed regularly
327: ## Tips and Best Practices
328: - **Don't aim for perfection:** Catching up means strategic skipping
329: - **Trust your instincts:** If a title doesn't grab you, skip it
330: - **Declare bankruptcy when needed:** Better to mark all read and start fresh
331: - **Time-box ruthlessly:** Set timer, stop when it goes off
332: - **Focus on recency:** Recent content is more valuable than old
333: - **Use feed health insights:** Catch-up reveals which feeds are worth keeping
334: - **Prevent future backlog:** Unsubscribe from consistently low-value feeds
335: - **Regular cadence:** Weekly catch-up prevents massive backlogs
336: ## Anti-Patterns to Avoid
337: - ❌ Trying to read every entry (impossible and exhausting)
338: - ❌ Starting with low-value high-volume feeds (waste of time)
339: - ❌ Reading old news that's no longer relevant
340: - ❌ Keeping feeds "just in case" (unsubscribe!)
341: - ❌ Feeling guilty about marking as read (it's a tool, not a moral obligation)
342: - ❌ Processing feeds alphabetically (prioritize by value)
343: - ❌ Saving items "to read later" that you'll never read (be honest)
344: ## Example Catch-Up Session (%s Days)
345: **Step 1: Assess**
346: - digest://stats → 347 unread across 8 feeds
347: - Breakdown: HackerNews (156), Reddit (89), Tech Blog (45), Others (57)
348: **Step 2: Goals**
349: - Time budget: 45 minutes
350: - Goal: Reduce to <10 unread
351: - Strategy: Process top 3 feeds, bankruptcy on high-volume
352: **Step 3: High-Value Feeds (20 minutes)**
353: - Tech Blog: 5 min → Found 2 must-reads
354: - Dev Community: 5 min → Found 1 must-read
355: - Newsletter: 5 min → Found 1 must-read
356: - Others: 5 min → Scanned, found nothing critical
357: **Step 4: High-Volume Feeds (5 minutes)**
358: - HackerNews: Declared bankruptcy, marked all 156 as read
359: - Reddit Dev: Declared bankruptcy, marked all 89 as read
360: **Step 5: Must-Read**
361: Identified 4 critical items from 347 total
362: **Step 6: Bulk Actions (5 minutes)**
363: - Marked 341 entries as read
364: - Kept 6 items unread (4 must-read, 2 interesting)
365: **Step 7: Summary (5 minutes)**
366: Documented findings, action items, feed health insights
367: **Result: 347 → 6 unread in 40 minutes!**
368: **Ready to catch up?**
369: 1. Check digest://stats to assess backlog size
370: 2. Set realistic goals (time and coverage)
371: 3. Process high-value feeds first
372: 4. Declare bankruptcy on high-volume low-signal feeds
373: 5. Identify must-read content
374: 6. Bulk mark processed entries as read
375: 7. Create summary with action items and feed health notes
376: `, days, days, days, days)
377: 	return &mcp.GetPromptResult{
378: 		Description: fmt.Sprintf("Catch-up workflow for processing %s days of unread entries", days),
379: 		Messages: []mcp.PromptMessage{
380: 			{
381: 				Role: mcp.RoleUser,
382: 				Content: mcp.TextContent{
383: 					Type: "text",
384: 					Text: template,
385: 				},
386: 			},
387: 		},
388: 	}, nil
389: }
390: func (s *Server) registerCurateFeedsPrompt() {
391: 	s.mcpServer.AddPrompt(
392: 		mcp.Prompt{
393: 			Name:        "curate-feeds",
394: 			Description: "Review your feed subscriptions and optimize them by removing low-value feeds and finding better sources",
395: 			Arguments:   []mcp.PromptArgument{},
396: 		},
397: 		s.handleCurateFeeds,
398: 	)
399: }
400: //nolint:funlen // Prompt handlers contain large template strings
401: func (s *Server) handleCurateFeeds(_ context.Context, _ mcp.GetPromptRequest) (*mcp.GetPromptResult, error) {
402: 	template := `# Curate Feeds
403: ## Overview
404: Systematically review and optimize your RSS/Atom feed subscriptions to maintain a high signal-to-noise ratio. Remove feeds that no longer provide value, identify gaps in coverage, and discover better sources. A well-curated feed list makes daily digest and catch-up workflows much more effective.
405: ## When to Use
406: - Feed backlog consistently grows despite regular catch-up
407: - Many unread entries but few interesting items
408: - Quarterly or monthly feed hygiene routine
409: - After trying several new feeds, need to decide which to keep
410: - Feeling overwhelmed by total feed volume
411: - When digest://stats shows concerning patterns
412: ## Workflow Steps
413: ### Step 1: Analyze Current Feed Health
414: Get quantitative data on all subscriptions.
415: **Use digest://stats resource:**
416: - Review total feed count
417: - Check per-feed entry counts
418: - Identify unread distribution
419: - Note error counts and fetch failures
420: **Key metrics to track:**
421: - **Volume:** Entries per feed per week
422: - **Read rate:** % of entries you actually read
423: - **Value ratio:** Must-read items / total entries
424: - **Freshness:** Last successful fetch time
425: - **Errors:** Persistent fetch failures
426: **Example analysis:**
427:     Total feeds: 12
428:     Total unread: 423
429:     Per-feed breakdown:
430:     1. HackerNews: 187 unread, 0%% read (HIGH VOLUME, LOW VALUE)
431:     2. Tech Blog: 45 unread, 60%% read (GOOD VALUE)
432:     3. Dev.to: 89 unread, 10%% read (HIGH VOLUME, LOW VALUE)
433:     4. Newsletter: 12 unread, 90%% read (EXCELLENT VALUE)
434:     5. Podcast Blog: 8 unread, 75%% read (GOOD VALUE)
435:     6. Generic News: 34 unread, 5%% read (LOW VALUE)
436:     7. Dead Feed: 0 unread, 0 errors, last fetch 45 days ago (INACTIVE)
437:     8. [Continue for all feeds...]
438:     Patterns:
439:     - 3 feeds producing 70%% of volume
440:     - 2 feeds with <10%% read rate
441:     - 1 feed hasn't updated in 6 weeks
442: ### Step 2: Categorize Feeds
443: Group feeds by value and usage patterns.
444: **Categories:**
445: **✅ Keep (High Value):**
446: - High read rate (>50%)
447: - Consistently interesting content
448: - Unique perspective or information
449: - Manageable volume
450: **⚠️ Probation (Uncertain):**
451: - Recently added, need more data
452: - Inconsistent quality
453: - Moderate read rate (20-50%)
454: - Might improve with selective reading
455: **❌ Remove (Low Value):**
456: - Very low read rate (<10%)
457: - High volume, low signal
458: - Duplicate coverage with better feeds
459: - No longer relevant to interests
460: - Persistent fetch errors
461: - Inactive (no new entries in 30+ days)
462: **Example categorization:**
463: - Keep (5): Tech Blog, Newsletter, Podcast Blog, Expert Blog, Industry News
464: - Probation (3): Dev.to, New Feed, Experimental Source
465: - Remove (4): HackerNews, Generic News, Dead Feed, Duplicate Coverage
466: ### Step 3: Evaluate Each Feed
467: Review feeds systematically, starting with candidates for removal.
468: **For each feed:**
469: 1. Review recent entries (use list_entries with feed_id)
470: 2. Check how many you read vs skipped
471: 3. Identify if content is unique or duplicated
472: 4. Consider if it aligns with current interests
473: 5. Decide: Keep, Probation, or Remove
474: **Evaluation questions:**
475: - **Value:** Has this feed taught me something valuable in past month?
476: - **Uniqueness:** Do I get this information elsewhere?
477: - **Relevance:** Still aligned with my interests/work?
478: - **Volume:** Is the entry volume manageable?
479: - **Quality:** Is signal-to-noise ratio acceptable?
480: - **Timeliness:** Are updates still regular and fresh?
481: **Example evaluation - HackerNews:**
482: - Volume: ~25 entries/day = 175/week (VERY HIGH)
483: - Read rate: 0% (skipped all 187 unread)
484: - Uniqueness: Duplicate coverage with other tech feeds
485: - Value: Interesting discussions but overwhelming volume
486: - Decision: REMOVE - Getting news from better-curated sources
487: ### Step 4: Remove Low-Value Feeds
488: Unsubscribe from feeds that don't make the cut.
489: **Before removing:**
490: - Mark all entries as read (cleanup)
491: - Export feed URL for reference (in case you want to re-add later)
492: - Document reason for removal (learn from the pattern)
493: **Use remove_feed tool:**
494: - remove_feed(feed_id="...")
495: - Removes feed and all associated entries
496: - Frees up mental bandwidth
497: **Example removals:**
498: - HackerNews: Too high volume, duplicate coverage
499: - Generic News: Low relevance, better sources exist
500: - Dead Feed: No updates in 6 weeks, likely abandoned
501: - Duplicate Coverage: Already getting same info from better feed
502: ### Step 5: Optimize Probation Feeds
503: Decide strategy for uncertain feeds.
504: **Strategies:**
505: **Strategy 1: Time-limited trial**
506: - Keep for 2-4 weeks
507: - Track read rate actively
508: - Review again and decide
509: **Strategy 2: Selective reading**
510: - Only read specific topics/authors
511: - Mark rest as read immediately
512: - If too much effort, remove
513: **Strategy 3: Reduce frequency**
514: - If high volume, check less often
515: - Process weekly instead of daily
516: - If still overwhelming, remove
517: **Example probation decisions:**
518: - Dev.to: Keep for 2 more weeks, track if quality improves
519: - New Feed: Interesting but new, give it 1 month trial
520: - Experimental Source: Too much manual filtering needed, REMOVE
521: ### Step 6: Identify Coverage Gaps
522: Look for missing topics or perspectives.
523: **Gap analysis questions:**
524: - What topics interest me but aren't covered?
525: - Are there important sources I'm missing?
526: - Do I have diverse perspectives or echo chamber?
527: - Any tools/technologies I use without following updates?
528: **Finding new feeds:**
529: - Search for topic-specific blogs
530: - Follow thought leaders' RSS feeds
531: - Look for official project blogs/announcements
532: - Ask community for recommendations
533: - Check OPML directories
534: **Example gap identification:**
535: - Missing: Security news (need dedicated feed)
536: - Missing: Database updates (following but no feed)
537: - Echo chamber: All feeds have similar perspective
538: - Action: Add security feed, database blog, contrarian viewpoint
539: ### Step 7: Document Feed Hygiene
540: Create a record of your curation decisions.
541: **Documentation to keep:**
542: - Date of curation
543: - Feeds removed and why
544: - Feeds added and why
545: - Current feed count and target
546: - Next review date
547: **Example documentation:**
548:     Feed Curation - December 11, 2025
549:     Starting feeds: 12
550:     Ending feeds: 8 (33%% reduction)
551:     Removed (4):
552:     1. HackerNews - High volume (175/week), 0%% read rate, duplicate coverage
553:     2. Generic News - Low relevance, better sources exist
554:     3. Dead Feed - No updates since Oct 15, abandoned
555:     4. Duplicate Tech Feed - Same content as Tech Blog but worse UX
556:     Added (1):
557:     1. Security Weekly - Gap in security news coverage
558:     Kept (7):
559:     - Tech Blog, Newsletter, Podcast Blog, Expert Blog, Industry News
560:     - Dev.to (probation - review in 2 weeks)
561:     - New Feed (probation - review in 4 weeks)
562:     Target: <10 feeds with >50%% read rate
563:     Next review: March 1, 2026 (quarterly)
564: ## Tips and Best Practices
565: - **Quarterly reviews:** Curate every 3 months minimum
566: - **Ruthless curation:** Better 5 great feeds than 20 mediocre ones
567: - **Track metrics:** Use read rate as objective measure
568: - **Quality over quantity:** More feeds ≠ more value
569: - **Accept FOMO:** Can't follow everything, focus on best sources
570: - **Evolve with interests:** Remove feeds when interests change
571: - **Give new feeds time:** 2-4 weeks trial before deciding
572: - **Export before deleting:** Keep URLs for potential re-subscription
573: - **Document decisions:** Learn from patterns in removed feeds
574: ## Key Metrics for Feed Health
575: **Healthy feed list:**
576: - Total feeds: <15 (manageable daily volume)
577: - Read rate per feed: >40% (mostly valuable content)
578: - Unread backlog: <100 (caught up within week)
579: - Inactive feeds: 0 (all actively publishing)
580: - Error feeds: 0 (all fetching successfully)
581: **Warning signs:**
582: - Total feeds: >25 (too much to process)
583: - Read rate: <20% on multiple feeds (low value)
584: - Unread backlog: >500 (overwhelmed)
585: - Inactive feeds: >2 (deadweight)
586: - Persistent errors: >1 (feed issues)
587: ## Anti-Patterns to Avoid
588: - ❌ Keeping feeds "just in case" (be honest about value)
589: - ❌ Subscribing to everything (quality over quantity)
590: - ❌ Never removing feeds (interests change, sources change)
591: - ❌ Feeling obligated to read (feeds serve you, not vice versa)
592: - ❌ Optimizing for completeness (optimize for value)
593: - ❌ Avoiding curation due to effort (pays dividends quickly)
594: - ❌ Not trying new feeds (stagnation is also a problem)
595: ## Example Curation Session
596: **Step 1: Analyze (10 minutes)**
597: - digest://stats shows 12 feeds, 423 unread
598: - Calculate read rates per feed
599: - Identify 3 high-volume low-value feeds
600: **Step 2: Categorize (5 minutes)**
601: - Keep: 5 feeds (high value, good read rate)
602: - Probation: 3 feeds (uncertain, need more data)
603: - Remove: 4 feeds (low value, clear decision)
604: **Step 3: Evaluate (15 minutes)**
605: - Review recent entries from probation and remove categories
606: - Check uniqueness of content
607: - Make final keep/remove decisions
608: **Step 4: Remove (10 minutes)**
609: - Mark all entries in removed feeds as read
610: - Export feed URLs for records
611: - Remove 4 feeds using remove_feed tool
612: - Document removal reasons
613: **Step 5: Optimize Probation (5 minutes)**
614: - Set 2-week trial for Dev.to
615: - Decide to remove one probation feed immediately (too much work)
616: - Keep one probation feed with selective reading strategy
617: **Step 6: Gaps (10 minutes)**
618: - Identify security news gap
619: - Search for and subscribe to Security Weekly feed
620: - Add to high-value category
621: **Step 7: Document (5 minutes)**
622: - Record decisions and rationale
623: - Set next review date (3 months)
624: - Update feed management notes
625: **Result: 12 → 8 feeds, much higher average value!**
626: **Ready to curate your feeds?**
627: 1. Analyze current feed health with digest://stats
628: 2. Categorize feeds: Keep, Probation, Remove
629: 3. Evaluate each feed systematically
630: 4. Remove low-value feeds
631: 5. Set strategy for probation feeds
632: 6. Identify and fill coverage gaps
633: 7. Document your decisions and next review date
634: `
635: 	return &mcp.GetPromptResult{
636: 		Description: "Feed curation workflow for optimizing RSS subscriptions",
637: 		Messages: []mcp.PromptMessage{
638: 			{
639: 				Role: mcp.RoleUser,
640: 				Content: mcp.TextContent{
641: 					Type: "text",
642: 					Text: template,
643: 				},
644: 			},
645: 		},
646: 	}, nil
647: }
</file>

<file path="internal/models/entry.go">
 1: // ABOUTME: Entry model representing a single feed entry with read/unread state
 2: // ABOUTME: Provides methods to mark entries as read or unread with timestamps
 3: package models
 4: import (
 5: 	"time"
 6: 	"github.com/google/uuid"
 7: )
 8: // Entry represents a single entry (article/item) in an RSS/Atom feed
 9: type Entry struct {
10: 	ID          string
11: 	FeedID      string
12: 	GUID        string
13: 	Title       *string
14: 	Link        *string
15: 	Author      *string
16: 	PublishedAt *time.Time
17: 	Content     *string
18: 	Read        bool
19: 	ReadAt      *time.Time
20: 	CreatedAt   time.Time
21: }
22: // NewEntry creates a new Entry with the given feedID, guid, and title
23: // Sets ID to a new UUID, CreatedAt to current time, and Read to false
24: func NewEntry(feedID, guid, title string) *Entry {
25: 	now := time.Now()
26: 	return &Entry{
27: 		ID:        uuid.New().String(),
28: 		FeedID:    feedID,
29: 		GUID:      guid,
30: 		Title:     &title,
31: 		Read:      false,
32: 		ReadAt:    nil,
33: 		CreatedAt: now,
34: 	}
35: }
36: // MarkRead marks the entry as read and sets ReadAt to the current time
37: func (e *Entry) MarkRead() {
38: 	now := time.Now()
39: 	e.Read = true
40: 	e.ReadAt = &now
41: }
42: // MarkUnread marks the entry as unread and clears the ReadAt timestamp
43: func (e *Entry) MarkUnread() {
44: 	e.Read = false
45: 	e.ReadAt = nil
46: }
</file>

<file path="internal/models/feed.go">
 1: // ABOUTME: Feed model representing an RSS/Atom feed source with HTTP caching support
 2: // ABOUTME: Tracks feed metadata, fetch history, and conditional request headers (ETag, Last-Modified)
 3: package models
 4: import (
 5: 	"time"
 6: 	"github.com/google/uuid"
 7: )
 8: // Feed represents an RSS/Atom feed subscription
 9: type Feed struct {
10: 	ID            string     // Unique identifier for the feed
11: 	URL           string     // Feed URL
12: 	Title         *string    // Feed title (from RSS/Atom metadata)
13: 	ETag          *string    // HTTP ETag header for conditional requests
14: 	LastModified  *string    // HTTP Last-Modified header for conditional requests
15: 	LastFetchedAt *time.Time // Timestamp of last successful fetch
16: 	LastError     *string    // Last error message (if any)
17: 	ErrorCount    int        // Consecutive error count for backoff strategy
18: 	CreatedAt     time.Time  // Feed creation timestamp
19: }
20: // NewFeed creates a new Feed instance with a generated ID and timestamp
21: func NewFeed(url string) *Feed {
22: 	return &Feed{
23: 		ID:        uuid.New().String(),
24: 		URL:       url,
25: 		CreatedAt: time.Now(),
26: 	}
27: }
28: // SetCacheHeaders updates the feed's HTTP caching headers for conditional requests
29: func (f *Feed) SetCacheHeaders(etag, lastModified string) {
30: 	if etag != "" {
31: 		f.ETag = &etag
32: 	}
33: 	if lastModified != "" {
34: 		f.LastModified = &lastModified
35: 	}
36: }
</file>

<file path="internal/parse/parse_test.go">
  1: // ABOUTME: Test suite for RSS/Atom feed parsing functionality
  2: // ABOUTME: Validates parsing of RSS 2.0 and Atom feeds using inline XML test data
  3: package parse
  4: import (
  5: 	"testing"
  6: 	"time"
  7: )
  8: const rss20XML = `<?xml version="1.0" encoding="UTF-8"?>
  9: <rss version="2.0">
 10:   <channel>
 11:     <title>Test RSS Feed</title>
 12:     <link>https://example.com</link>
 13:     <description>A test RSS feed</description>
 14:     <item>
 15:       <guid>https://example.com/post/1</guid>
 16:       <title>First Post</title>
 17:       <link>https://example.com/post/1</link>
 18:       <author>john@example.com (John Doe)</author>
 19:       <pubDate>Mon, 02 Jan 2006 15:04:05 MST</pubDate>
 20:       <description>First post description</description>
 21:       <category>tech</category>
 22:       <category>golang</category>
 23:     </item>
 24:     <item>
 25:       <title>Second Post</title>
 26:       <link>https://example.com/post/2</link>
 27:       <pubDate>Tue, 03 Jan 2006 15:04:05 MST</pubDate>
 28:       <description>Second post description</description>
 29:     </item>
 30:   </channel>
 31: </rss>`
 32: const atomXML = `<?xml version="1.0" encoding="UTF-8"?>
 33: <feed xmlns="http://www.w3.org/2005/Atom">
 34:   <title>Test Atom Feed</title>
 35:   <link href="https://example.com"/>
 36:   <updated>2006-01-02T15:04:05Z</updated>
 37:   <entry>
 38:     <id>https://example.com/entry/1</id>
 39:     <title>First Entry</title>
 40:     <link href="https://example.com/entry/1"/>
 41:     <author>
 42:       <name>Jane Smith</name>
 43:     </author>
 44:     <published>2006-01-02T15:04:05Z</published>
 45:     <updated>2006-01-02T16:04:05Z</updated>
 46:     <content type="html">First entry content</content>
 47:     <summary>First entry summary</summary>
 48:     <category term="science"/>
 49:   </entry>
 50:   <entry>
 51:     <id>https://example.com/entry/2</id>
 52:     <title>Second Entry</title>
 53:     <link href="https://example.com/entry/2"/>
 54:     <updated>2006-01-03T15:04:05Z</updated>
 55:     <summary>Second entry summary</summary>
 56:   </entry>
 57: </feed>`
 58: func TestParse_RSS(t *testing.T) {
 59: 	feed, err := Parse([]byte(rss20XML))
 60: 	if err != nil {
 61: 		t.Fatalf("Parse() error = %v", err)
 62: 	}
 63: 	if feed.Title != "Test RSS Feed" {
 64: 		t.Errorf("feed.Title = %q, want %q", feed.Title, "Test RSS Feed")
 65: 	}
 66: 	if len(feed.Entries) != 2 {
 67: 		t.Fatalf("len(feed.Entries) = %d, want 2", len(feed.Entries))
 68: 	}
 69: 	// Check first entry
 70: 	entry1 := feed.Entries[0]
 71: 	if entry1.GUID != "https://example.com/post/1" {
 72: 		t.Errorf("entry1.GUID = %q, want %q", entry1.GUID, "https://example.com/post/1")
 73: 	}
 74: 	if entry1.Title != "First Post" {
 75: 		t.Errorf("entry1.Title = %q, want %q", entry1.Title, "First Post")
 76: 	}
 77: 	if entry1.Link != "https://example.com/post/1" {
 78: 		t.Errorf("entry1.Link = %q, want %q", entry1.Link, "https://example.com/post/1")
 79: 	}
 80: 	if entry1.Author != "John Doe" {
 81: 		t.Errorf("entry1.Author = %q, want %q", entry1.Author, "John Doe")
 82: 	}
 83: 	if entry1.PublishedAt == nil {
 84: 		t.Error("entry1.PublishedAt is nil, want non-nil")
 85: 	}
 86: 	if entry1.Content != "First post description" {
 87: 		t.Errorf("entry1.Content = %q, want %q", entry1.Content, "First post description")
 88: 	}
 89: 	if len(entry1.Categories) != 2 {
 90: 		t.Errorf("len(entry1.Categories) = %d, want 2", len(entry1.Categories))
 91: 	}
 92: 	if len(entry1.Categories) >= 2 && (entry1.Categories[0] != "tech" || entry1.Categories[1] != "golang") {
 93: 		t.Errorf("entry1.Categories = %v, want [tech golang]", entry1.Categories)
 94: 	}
 95: 	// Check second entry (no GUID, should fallback to Link)
 96: 	entry2 := feed.Entries[1]
 97: 	if entry2.GUID != "https://example.com/post/2" {
 98: 		t.Errorf("entry2.GUID = %q, want %q (fallback to Link)", entry2.GUID, "https://example.com/post/2")
 99: 	}
100: 	if entry2.Title != "Second Post" {
101: 		t.Errorf("entry2.Title = %q, want %q", entry2.Title, "Second Post")
102: 	}
103: 	if entry2.Author != "" {
104: 		t.Errorf("entry2.Author = %q, want empty string", entry2.Author)
105: 	}
106: }
107: func TestParse_Atom(t *testing.T) {
108: 	feed, err := Parse([]byte(atomXML))
109: 	if err != nil {
110: 		t.Fatalf("Parse() error = %v", err)
111: 	}
112: 	if feed.Title != "Test Atom Feed" {
113: 		t.Errorf("feed.Title = %q, want %q", feed.Title, "Test Atom Feed")
114: 	}
115: 	if len(feed.Entries) != 2 {
116: 		t.Fatalf("len(feed.Entries) = %d, want 2", len(feed.Entries))
117: 	}
118: 	// Check first entry
119: 	entry1 := feed.Entries[0]
120: 	if entry1.GUID != "https://example.com/entry/1" {
121: 		t.Errorf("entry1.GUID = %q, want %q", entry1.GUID, "https://example.com/entry/1")
122: 	}
123: 	if entry1.Title != "First Entry" {
124: 		t.Errorf("entry1.Title = %q, want %q", entry1.Title, "First Entry")
125: 	}
126: 	if entry1.Link != "https://example.com/entry/1" {
127: 		t.Errorf("entry1.Link = %q, want %q", entry1.Link, "https://example.com/entry/1")
128: 	}
129: 	if entry1.Author != "Jane Smith" {
130: 		t.Errorf("entry1.Author = %q, want %q", entry1.Author, "Jane Smith")
131: 	}
132: 	if entry1.PublishedAt == nil {
133: 		t.Error("entry1.PublishedAt is nil, want non-nil")
134: 	} else {
135: 		expected := time.Date(2006, 1, 2, 15, 4, 5, 0, time.UTC)
136: 		if !entry1.PublishedAt.Equal(expected) {
137: 			t.Errorf("entry1.PublishedAt = %v, want %v", entry1.PublishedAt, expected)
138: 		}
139: 	}
140: 	if entry1.Content != "First entry content" {
141: 		t.Errorf("entry1.Content = %q, want %q", entry1.Content, "First entry content")
142: 	}
143: 	if len(entry1.Categories) != 1 || entry1.Categories[0] != "science" {
144: 		t.Errorf("entry1.Categories = %v, want [science]", entry1.Categories)
145: 	}
146: 	// Check second entry (no published date, should use updated)
147: 	entry2 := feed.Entries[1]
148: 	if entry2.GUID != "https://example.com/entry/2" {
149: 		t.Errorf("entry2.GUID = %q, want %q", entry2.GUID, "https://example.com/entry/2")
150: 	}
151: 	if entry2.Title != "Second Entry" {
152: 		t.Errorf("entry2.Title = %q, want %q", entry2.Title, "Second Entry")
153: 	}
154: 	if entry2.PublishedAt == nil {
155: 		t.Error("entry2.PublishedAt is nil, want non-nil (should fallback to updated)")
156: 	} else {
157: 		expected := time.Date(2006, 1, 3, 15, 4, 5, 0, time.UTC)
158: 		if !entry2.PublishedAt.Equal(expected) {
159: 			t.Errorf("entry2.PublishedAt = %v, want %v", entry2.PublishedAt, expected)
160: 		}
161: 	}
162: 	if entry2.Content != "Second entry summary" {
163: 		t.Errorf("entry2.Content = %q, want %q (fallback to summary)", entry2.Content, "Second entry summary")
164: 	}
165: }
</file>

<file path="internal/parse/parse.go">
 1: // ABOUTME: RSS/Atom feed parsing using gofeed library
 2: // ABOUTME: Converts gofeed.Feed to simplified ParsedFeed structure with normalized fields
 3: package parse
 4: import (
 5: 	"strings"
 6: 	"time"
 7: 	"github.com/mmcdole/gofeed"
 8: )
 9: // ParsedFeed represents a normalized feed structure
10: type ParsedFeed struct {
11: 	Title   string
12: 	Entries []ParsedEntry
13: }
14: // ParsedEntry represents a normalized feed entry
15: type ParsedEntry struct {
16: 	GUID        string
17: 	Title       string
18: 	Link        string
19: 	Author      string
20: 	PublishedAt *time.Time
21: 	Content     string
22: 	Categories  []string
23: }
24: // Parse parses RSS or Atom feed data and returns a normalized ParsedFeed
25: func Parse(data []byte) (*ParsedFeed, error) {
26: 	parser := gofeed.NewParser()
27: 	feed, err := parser.ParseString(string(data))
28: 	if err != nil {
29: 		return nil, err
30: 	}
31: 	parsed := &ParsedFeed{
32: 		Title:   feed.Title,
33: 		Entries: make([]ParsedEntry, 0, len(feed.Items)),
34: 	}
35: 	for _, item := range feed.Items {
36: 		entry := ParsedEntry{
37: 			GUID:       item.GUID,
38: 			Title:      item.Title,
39: 			Link:       item.Link,
40: 			Categories: item.Categories,
41: 		}
42: 		// Fallback GUID to Link if empty
43: 		if entry.GUID == "" {
44: 			entry.GUID = item.Link
45: 		}
46: 		// Extract author name
47: 		if item.Author != nil {
48: 			entry.Author = item.Author.Name
49: 		}
50: 		// Use PublishedParsed or fallback to UpdatedParsed
51: 		if item.PublishedParsed != nil {
52: 			entry.PublishedAt = item.PublishedParsed
53: 		} else if item.UpdatedParsed != nil {
54: 			entry.PublishedAt = item.UpdatedParsed
55: 		}
56: 		// Prefer Content over Description
57: 		if item.Content != "" {
58: 			entry.Content = item.Content
59: 		} else {
60: 			entry.Content = item.Description
61: 		}
62: 		// Clean up content - remove HTML tags if needed
63: 		entry.Content = strings.TrimSpace(entry.Content)
64: 		parsed.Entries = append(parsed.Entries, entry)
65: 	}
66: 	return parsed, nil
67: }
</file>

<file path="internal/timeutil/timeutil_test.go">
 1: // ABOUTME: Tests for time utility functions
 2: // ABOUTME: Verifies date range calculations for smart views
 3: package timeutil
 4: import (
 5: 	"testing"
 6: 	"time"
 7: )
 8: func TestStartOfToday(t *testing.T) {
 9: 	result := StartOfToday()
10: 	now := time.Now()
11: 	if result.Year() != now.Year() || result.Month() != now.Month() || result.Day() != now.Day() {
12: 		t.Errorf("StartOfToday() date mismatch: got %v, expected date %v", result, now)
13: 	}
14: 	if result.Hour() != 0 || result.Minute() != 0 || result.Second() != 0 {
15: 		t.Errorf("StartOfToday() should be midnight, got %v", result)
16: 	}
17: }
18: func TestStartOfYesterday(t *testing.T) {
19: 	result := StartOfYesterday()
20: 	today := StartOfToday()
21: 	expected := today.AddDate(0, 0, -1)
22: 	if !result.Equal(expected) {
23: 		t.Errorf("StartOfYesterday() = %v, expected %v", result, expected)
24: 	}
25: }
26: func TestEndOfYesterday(t *testing.T) {
27: 	result := EndOfYesterday()
28: 	today := StartOfToday()
29: 	if !result.Equal(today) {
30: 		t.Errorf("EndOfYesterday() = %v, expected %v (start of today)", result, today)
31: 	}
32: }
33: func TestStartOfWeek(t *testing.T) {
34: 	result := StartOfWeek()
35: 	// Should be a Sunday
36: 	if result.Weekday() != time.Sunday {
37: 		t.Errorf("StartOfWeek() weekday = %v, expected Sunday", result.Weekday())
38: 	}
39: 	// Should be midnight
40: 	if result.Hour() != 0 || result.Minute() != 0 || result.Second() != 0 {
41: 		t.Errorf("StartOfWeek() should be midnight, got %v", result)
42: 	}
43: 	// Should be on or before today
44: 	if result.After(StartOfToday()) {
45: 		t.Errorf("StartOfWeek() = %v, should not be after today", result)
46: 	}
47: }
48: func TestStartOfMonth(t *testing.T) {
49: 	result := StartOfMonth()
50: 	now := time.Now()
51: 	if result.Year() != now.Year() || result.Month() != now.Month() {
52: 		t.Errorf("StartOfMonth() year/month mismatch: got %v, expected %d-%02d", result, now.Year(), now.Month())
53: 	}
54: 	if result.Day() != 1 {
55: 		t.Errorf("StartOfMonth() day = %d, expected 1", result.Day())
56: 	}
57: 	if result.Hour() != 0 || result.Minute() != 0 || result.Second() != 0 {
58: 		t.Errorf("StartOfMonth() should be midnight, got %v", result)
59: 	}
60: }
61: func TestParsePeriod(t *testing.T) {
62: 	tests := []struct {
63: 		period   string
64: 		expected func() time.Time
65: 		valid    bool
66: 	}{
67: 		{"today", StartOfToday, true},
68: 		{"yesterday", StartOfYesterday, true},
69: 		{"week", StartOfWeek, true},
70: 		{"month", StartOfMonth, true},
71: 		{"invalid", nil, false},
72: 		{"", nil, false},
73: 	}
74: 	for _, tc := range tests {
75: 		result, ok := ParsePeriod(tc.period)
76: 		if ok != tc.valid {
77: 			t.Errorf("ParsePeriod(%q) valid = %v, expected %v", tc.period, ok, tc.valid)
78: 			continue
79: 		}
80: 		if tc.valid {
81: 			expected := tc.expected()
82: 			if !result.Equal(expected) {
83: 				t.Errorf("ParsePeriod(%q) = %v, expected %v", tc.period, result, expected)
84: 			}
85: 		}
86: 	}
87: }
</file>

<file path="internal/timeutil/timeutil.go">
 1: // ABOUTME: Time utility functions for date range calculations
 2: // ABOUTME: Provides helpers for smart views like today, yesterday, this week
 3: package timeutil
 4: import "time"
 5: // StartOfToday returns midnight (00:00:00) of the current day in local time
 6: func StartOfToday() time.Time {
 7: 	now := time.Now()
 8: 	return time.Date(now.Year(), now.Month(), now.Day(), 0, 0, 0, 0, now.Location())
 9: }
10: // StartOfYesterday returns midnight (00:00:00) of yesterday in local time
11: func StartOfYesterday() time.Time {
12: 	return StartOfToday().AddDate(0, 0, -1)
13: }
14: // EndOfYesterday returns the last moment of yesterday (start of today) in local time
15: func EndOfYesterday() time.Time {
16: 	return StartOfToday()
17: }
18: // StartOfWeek returns midnight of the most recent Sunday in local time
19: // Note: Week starts on Sunday
20: func StartOfWeek() time.Time {
21: 	today := StartOfToday()
22: 	weekday := int(today.Weekday())
23: 	return today.AddDate(0, 0, -weekday)
24: }
25: // StartOfMonth returns midnight of the first day of the current month in local time
26: func StartOfMonth() time.Time {
27: 	now := time.Now()
28: 	return time.Date(now.Year(), now.Month(), 1, 0, 0, 0, 0, now.Location())
29: }
30: // ParsePeriod converts a period string to a time.Time representing the cutoff
31: // Supported values: "today", "yesterday", "week", "month"
32: // Returns the start of that period (articles before this time would be marked)
33: func ParsePeriod(period string) (time.Time, bool) {
34: 	switch period {
35: 	case "today":
36: 		return StartOfToday(), true
37: 	case "yesterday":
38: 		return StartOfYesterday(), true
39: 	case "week":
40: 		return StartOfWeek(), true
41: 	case "month":
42: 		return StartOfMonth(), true
43: 	default:
44: 		return time.Time{}, false
45: 	}
46: }
</file>

<file path=".gitignore">
1: /digest
</file>

<file path=".pre-commit-config.yaml">
 1: # ABOUTME: Pre-commit hooks configuration for digest project
 2: # ABOUTME: Runs formatting, linting, and tests before each commit
 3: repos:
 4:   - repo: https://github.com/pre-commit/pre-commit-hooks
 5:     rev: v4.6.0
 6:     hooks:
 7:       - id: trailing-whitespace
 8:         exclude: ^go\.(mod|sum)$
 9:       - id: end-of-file-fixer
10:         exclude: ^go\.(mod|sum)$
11:       - id: check-yaml
12:       - id: check-added-large-files
13:         args: ['--maxkb=1000']  # Block files > 1MB (except binary)
14:         exclude: ^digest$  # Allow the binary
15:       - id: check-merge-conflict
16:       - id: mixed-line-ending
17:         args: ['--fix=lf']
18:       - id: check-case-conflict
19:   - repo: https://github.com/dnephin/pre-commit-golang
20:     rev: v0.5.1
21:     hooks:
22:       - id: go-fmt
23:       - id: go-imports
24:       - id: go-mod-tidy
25:       - id: golangci-lint
26:         args: ['--timeout=10m', '--fix']
27:       - id: go-unit-tests
28:         args: ['-race', '-count=1', './...']  # -count=1 disables test caching
29:   - repo: local
30:     hooks:
31:       - id: go-mod-verify
32:         name: go mod verify
33:         entry: go mod verify
34:         language: system
35:         files: go\.(mod|sum)$
36:         pass_filenames: false
37:       - id: go-vet
38:         name: go vet
39:         entry: go vet
40:         language: system
41:         args: ['./...']
42:         files: \.go$
43:         pass_filenames: false
</file>

<file path="Makefile">
 1: # ABOUTME: Makefile for building digest CLI
 2: # ABOUTME: Provides build, test, install, and clean targets
 3:
 4: VERSION ?= $(shell git describe --tags --always --dirty 2>/dev/null || echo "dev")
 5: COMMIT ?= $(shell git rev-parse --short HEAD 2>/dev/null || echo "unknown")
 6: BUILD_DATE ?= $(shell date -u +"%Y-%m-%dT%H:%M:%SZ")
 7:
 8: LDFLAGS := -ldflags "-X main.Version=$(VERSION) -X main.Commit=$(COMMIT) -X main.BuildDate=$(BUILD_DATE)"
 9:
10: .PHONY: all build test install clean
11:
12: all: build
13:
14: build:
15: 	go build $(LDFLAGS) -o digest ./cmd/digest
16:
17: test:
18: 	go test -v ./...
19:
20: test-short:
21: 	go test -short -v ./...
22:
23: install:
24: 	go install $(LDFLAGS) ./cmd/digest
25:
26: clean:
27: 	rm -f digest
28: 	go clean ./...
29:
30: # Run integration tests (requires network)
31: test-integration:
32: 	go test -v -run Integration ./test/...
</file>

<file path="cmd/digest/main.go">
 1: // ABOUTME: Entry point for digest CLI
 2: // ABOUTME: Initializes and executes root command
 3: package main
 4: import (
 5: 	"fmt"
 6: 	"os"
 7: )
 8: func main() {
 9: 	if err := Execute(); err != nil {
10: 		fmt.Fprintln(os.Stderr, err)
11: 		os.Exit(1)
12: 	}
13: }
</file>

<file path="cmd/digest/markread.go">
 1: // ABOUTME: Mark-read command for marking entries as read
 2: // ABOUTME: Supports single entry by ID or bulk operations by date
 3: package main
 4: import (
 5: 	"database/sql"
 6: 	"errors"
 7: 	"fmt"
 8: 	"time"
 9: 	"github.com/spf13/cobra"
10: 	"github.com/harper/digest/internal/db"
11: 	"github.com/harper/digest/internal/timeutil"
12: )
13: var markReadCmd = &cobra.Command{
14: 	Use:   "mark-read [entry-id]",
15: 	Short: "Mark entries as read",
16: 	Long:  "Mark a single entry as read by ID, or use --before to mark all entries older than a date",
17: 	Args:  cobra.MaximumNArgs(1),
18: 	RunE: func(cmd *cobra.Command, args []string) error {
19: 		before, _ := cmd.Flags().GetString("before")
20: 		// Single entry mode
21: 		if len(args) == 1 {
22: 			if before != "" {
23: 				return fmt.Errorf("cannot use --before with an entry ID")
24: 			}
25: 			entryRef := args[0]
26: 			// Get entry by ID or prefix
27: 			entry, err := db.GetEntryByID(dbConn, entryRef)
28: 			if err != nil {
29: 				// Only try prefix match if entry was not found (not for other DB errors)
30: 				if !errors.Is(err, sql.ErrNoRows) {
31: 					return fmt.Errorf("failed to get entry: %w", err)
32: 				}
33: 				entry, err = db.GetEntryByPrefix(dbConn, entryRef)
34: 				if err != nil {
35: 					return fmt.Errorf("entry not found: %s", entryRef)
36: 				}
37: 			}
38: 			if entry.Read {
39: 				fmt.Println("Entry is already marked as read")
40: 				return nil
41: 			}
42: 			if err := db.MarkEntryRead(dbConn, entry.ID); err != nil {
43: 				return fmt.Errorf("failed to mark entry as read: %w", err)
44: 			}
45: 			title := "Untitled"
46: 			if entry.Title != nil {
47: 				title = *entry.Title
48: 			}
49: 			fmt.Printf("Marked as read: %s\n", title)
50: 			return nil
51: 		}
52: 		// Bulk mode requires --before
53: 		if before == "" {
54: 			return fmt.Errorf("provide an entry ID or use --before for bulk marking")
55: 		}
56: 		// Parse the period
57: 		cutoff, ok := timeutil.ParsePeriod(before)
58: 		if !ok {
59: 			// Try parsing as ISO date
60: 			parsed, err := time.Parse("2006-01-02", before)
61: 			if err != nil {
62: 				return fmt.Errorf("invalid period %q: use yesterday, week, month, or YYYY-MM-DD", before)
63: 			}
64: 			cutoff = parsed
65: 		}
66: 		// Mark entries as read
67: 		count, err := db.MarkEntriesReadBefore(dbConn, cutoff)
68: 		if err != nil {
69: 			return fmt.Errorf("failed to mark entries as read: %w", err)
70: 		}
71: 		if count == 0 {
72: 			fmt.Println("No entries to mark as read")
73: 		} else {
74: 			fmt.Printf("Marked %d entries as read\n", count)
75: 		}
76: 		return nil
77: 	},
78: }
79: func init() {
80: 	rootCmd.AddCommand(markReadCmd)
81: 	markReadCmd.Flags().StringP("before", "b", "", "mark entries older than: yesterday, week, month, or YYYY-MM-DD")
82: }
</file>

<file path="cmd/digest/open.go">
 1: // ABOUTME: Open command for launching entry links in browser
 2: // ABOUTME: Opens the entry's link and marks the entry as read
 3: package main
 4: import (
 5: 	"fmt"
 6: 	"net/url"
 7: 	"os/exec"
 8: 	"runtime"
 9: 	"github.com/spf13/cobra"
10: 	"github.com/harper/digest/internal/db"
11: )
12: var openCmd = &cobra.Command{
13: 	Use:   "open <entry-prefix>",
14: 	Short: "Open entry link in browser and mark as read",
15: 	Long:  "Open an entry's link in your default browser and mark the entry as read by providing its ID prefix (minimum 6 characters)",
16: 	Args:  cobra.ExactArgs(1),
17: 	RunE: func(cmd *cobra.Command, args []string) error {
18: 		// Get entry by prefix
19: 		entry, err := db.GetEntryByPrefix(dbConn, args[0])
20: 		if err != nil {
21: 			return fmt.Errorf("failed to find entry: %w", err)
22: 		}
23: 		// Check that link is not nil/empty
24: 		if entry.Link == nil || *entry.Link == "" {
25: 			return fmt.Errorf("entry has no link")
26: 		}
27: 		// Validate URL format and scheme for security
28: 		parsedURL, err := url.Parse(*entry.Link)
29: 		if err != nil {
30: 			return fmt.Errorf("entry has malformed link: %w", err)
31: 		}
32: 		if parsedURL.Scheme != "http" && parsedURL.Scheme != "https" {
33: 			return fmt.Errorf("entry link must be http or https, got: %s", parsedURL.Scheme)
34: 		}
35: 		// Open browser with validated URL
36: 		if err := openBrowser(parsedURL.String()); err != nil {
37: 			return fmt.Errorf("failed to open browser: %w", err)
38: 		}
39: 		// Mark as read
40: 		if err := db.MarkEntryRead(dbConn, entry.ID); err != nil {
41: 			return fmt.Errorf("failed to mark entry as read: %w", err)
42: 		}
43: 		// Print confirmation with title
44: 		title := "Untitled"
45: 		if entry.Title != nil {
46: 			title = *entry.Title
47: 		}
48: 		fmt.Printf("✓ Opened and marked as read: %s\n", title)
49: 		return nil
50: 	},
51: }
52: // openBrowser opens a URL in the default browser for the current platform
53: func openBrowser(urlStr string) error {
54: 	var cmd *exec.Cmd
55: 	switch runtime.GOOS {
56: 	case "darwin":
57: 		cmd = exec.Command("open", urlStr)
58: 	case "linux":
59: 		cmd = exec.Command("xdg-open", urlStr)
60: 	case "windows":
61: 		cmd = exec.Command("rundll32", "url.dll,FileProtocolHandler", urlStr)
62: 	default:
63: 		return fmt.Errorf("unsupported platform: %s", runtime.GOOS)
64: 	}
65: 	// Start the browser
66: 	if err := cmd.Start(); err != nil {
67: 		return fmt.Errorf("failed to start browser: %w", err)
68: 	}
69: 	// Reap the process asynchronously to prevent zombie processes
70: 	go cmd.Wait()
71: 	return nil
72: }
73: func init() {
74: 	rootCmd.AddCommand(openCmd)
75: }
</file>

<file path="cmd/digest/root.go">
 1: // ABOUTME: Root Cobra command and global flags
 2: // ABOUTME: Sets up CLI structure and initializes database/OPML
 3: package main
 4: import (
 5: 	"database/sql"
 6: 	"fmt"
 7: 	"os"
 8: 	"github.com/spf13/cobra"
 9: 	"github.com/harper/digest/internal/db"
10: 	"github.com/harper/digest/internal/opml"
11: )
12: var (
13: 	dbPath   string
14: 	opmlPath string
15: 	dbConn   *sql.DB
16: 	opmlDoc  *opml.Document
17: )
18: var rootCmd = &cobra.Command{
19: 	Use:   "digest",
20: 	Short: "RSS/Atom feed tracker with MCP integration",
21: 	Long: `
22: ██████╗ ██╗ ██████╗ ███████╗███████╗████████╗
23: ██╔══██╗██║██╔════╝ ██╔════╝██╔════╝╚══██╔══╝
24: ██║  ██║██║██║  ███╗█████╗  ███████╗   ██║
25: ██║  ██║██║██║   ██║██╔══╝  ╚════██║   ██║
26: ██████╔╝██║╚██████╔╝███████╗███████║   ██║
27: ╚═════╝ ╚═╝ ╚═════╝ ╚══════╝╚══════╝   ╚═╝
28: RSS/Atom feed tracker for humans and AI agents.
29: Track feeds, sync content, and expose via MCP for Claude.`,
30: 	PersistentPreRunE: func(cmd *cobra.Command, args []string) error {
31: 		// Set default paths if not provided
32: 		if dbPath == "" {
33: 			dbPath = db.GetDefaultDBPath()
34: 		}
35: 		if opmlPath == "" {
36: 			opmlPath = db.GetDefaultOPMLPath()
37: 		}
38: 		// Initialize database
39: 		var err error
40: 		dbConn, err = db.InitDB(dbPath)
41: 		if err != nil {
42: 			return fmt.Errorf("failed to initialize database: %w", err)
43: 		}
44: 		// Load or create OPML document
45: 		if _, err := os.Stat(opmlPath); os.IsNotExist(err) {
46: 			opmlDoc = opml.NewDocument("digest feeds")
47: 		} else {
48: 			opmlDoc, err = opml.ParseFile(opmlPath)
49: 			if err != nil {
50: 				return fmt.Errorf("failed to load OPML: %w", err)
51: 			}
52: 		}
53: 		return nil
54: 	},
55: 	PersistentPostRunE: func(cmd *cobra.Command, args []string) error {
56: 		if dbConn != nil {
57: 			if err := dbConn.Close(); err != nil {
58: 				return fmt.Errorf("failed to close database: %w", err)
59: 			}
60: 		}
61: 		return nil
62: 	},
63: }
64: func Execute() error {
65: 	return rootCmd.Execute()
66: }
67: func init() {
68: 	rootCmd.PersistentFlags().StringVar(&dbPath, "db", "", "database file path (default: ~/.local/share/digest/digest.db)")
69: 	rootCmd.PersistentFlags().StringVar(&opmlPath, "opml", "", "OPML file path (default: ~/.local/share/digest/feeds.opml)")
70: }
71: func saveOPML() error {
72: 	if opmlDoc == nil {
73: 		return fmt.Errorf("OPML document not initialized")
74: 	}
75: 	if err := opmlDoc.WriteFile(opmlPath); err != nil {
76: 		return fmt.Errorf("failed to write OPML file: %w", err)
77: 	}
78: 	return nil
79: }
</file>

<file path="internal/db/db.go">
 1: // ABOUTME: Database connection management and initialization
 2: // ABOUTME: Handles SQLite connection, XDG paths, and migrations
 3: package db
 4: import (
 5: 	"database/sql"
 6: 	"fmt"
 7: 	"os"
 8: 	"path/filepath"
 9: 	_ "modernc.org/sqlite"
10: )
11: func InitDB(dbPath string) (*sql.DB, error) {
12: 	dir := filepath.Dir(dbPath)
13: 	// Use 0700 (owner only) for privacy - RSS reading habits are personal data
14: 	if err := os.MkdirAll(dir, 0700); err != nil {
15: 		return nil, fmt.Errorf("failed to create database directory: %w", err)
16: 	}
17: 	db, err := sql.Open("sqlite", dbPath)
18: 	if err != nil {
19: 		return nil, fmt.Errorf("failed to open database: %w", err)
20: 	}
21: 	if _, err := db.Exec("PRAGMA foreign_keys = ON"); err != nil {
22: 		_ = db.Close()
23: 		return nil, fmt.Errorf("failed to enable foreign keys: %w", err)
24: 	}
25: 	if err := runMigrations(db); err != nil {
26: 		_ = db.Close()
27: 		return nil, fmt.Errorf("failed to run migrations: %w", err)
28: 	}
29: 	return db, nil
30: }
31: func GetDefaultDBPath() string {
32: 	return filepath.Join(getDataDir(), "digest", "digest.db")
33: }
34: func GetDefaultOPMLPath() string {
35: 	return filepath.Join(getDataDir(), "digest", "feeds.opml")
36: }
37: func getDataDir() string {
38: 	if dataDir := os.Getenv("XDG_DATA_HOME"); dataDir != "" {
39: 		return dataDir
40: 	}
41: 	homeDir, err := os.UserHomeDir()
42: 	if err != nil {
43: 		return "."
44: 	}
45: 	return filepath.Join(homeDir, ".local", "share")
46: }
47: func runMigrations(db *sql.DB) error {
48: 	schema := `
49: 	CREATE TABLE IF NOT EXISTS feeds (
50: 		id TEXT PRIMARY KEY,
51: 		url TEXT UNIQUE NOT NULL,
52: 		title TEXT,
53: 		etag TEXT,
54: 		last_modified TEXT,
55: 		last_fetched_at DATETIME,
56: 		last_error TEXT,
57: 		error_count INTEGER DEFAULT 0,
58: 		created_at DATETIME NOT NULL
59: 	);
60: 	CREATE TABLE IF NOT EXISTS entries (
61: 		id TEXT PRIMARY KEY,
62: 		feed_id TEXT NOT NULL,
63: 		guid TEXT NOT NULL,
64: 		title TEXT,
65: 		link TEXT,
66: 		author TEXT,
67: 		published_at DATETIME,
68: 		content TEXT,
69: 		read BOOLEAN DEFAULT FALSE,
70: 		read_at DATETIME,
71: 		created_at DATETIME NOT NULL,
72: 		FOREIGN KEY (feed_id) REFERENCES feeds(id) ON DELETE CASCADE,
73: 		UNIQUE(feed_id, guid)
74: 	);
75: 	CREATE INDEX IF NOT EXISTS idx_entries_feed_id ON entries(feed_id);
76: 	CREATE INDEX IF NOT EXISTS idx_entries_read ON entries(read);
77: 	CREATE INDEX IF NOT EXISTS idx_entries_published_at ON entries(published_at);
78: 	CREATE TABLE IF NOT EXISTS entry_tags (
79: 		entry_id TEXT NOT NULL,
80: 		tag TEXT NOT NULL,
81: 		PRIMARY KEY (entry_id, tag),
82: 		FOREIGN KEY (entry_id) REFERENCES entries(id) ON DELETE CASCADE
83: 	);
84: 	`
85: 	_, err := db.Exec(schema)
86: 	return err
87: }
</file>

<file path="internal/db/entries_test.go">
  1: // ABOUTME: Tests for entry database operations
  2: // ABOUTME: Validates CRUD operations for entries table
  3: package db
  4: import (
  5: 	"testing"
  6: 	"time"
  7: 	"github.com/harper/digest/internal/models"
  8: )
  9: func TestCreateEntry(t *testing.T) {
 10: 	conn := setupTestDB(t)
 11: 	defer conn.Close()
 12: 	// Create a feed first
 13: 	feed := models.NewFeed("https://example.com/feed.xml")
 14: 	err := CreateFeed(conn, feed)
 15: 	if err != nil {
 16: 		t.Fatalf("CreateFeed failed: %v", err)
 17: 	}
 18: 	// Create an entry
 19: 	entry := models.NewEntry(feed.ID, "entry-guid-123", "Test Entry")
 20: 	err = CreateEntry(conn, entry)
 21: 	if err != nil {
 22: 		t.Fatalf("CreateEntry failed: %v", err)
 23: 	}
 24: 	// Verify it exists by ID
 25: 	got, err := GetEntryByID(conn, entry.ID)
 26: 	if err != nil {
 27: 		t.Fatalf("GetEntryByID failed: %v", err)
 28: 	}
 29: 	if got.ID != entry.ID {
 30: 		t.Errorf("expected ID %s, got %s", entry.ID, got.ID)
 31: 	}
 32: 	if got.GUID != entry.GUID {
 33: 		t.Errorf("expected GUID %s, got %s", entry.GUID, got.GUID)
 34: 	}
 35: 	if got.Read != false {
 36: 		t.Errorf("expected Read to be false, got %v", got.Read)
 37: 	}
 38: }
 39: func TestCreateEntry_Duplicate(t *testing.T) {
 40: 	conn := setupTestDB(t)
 41: 	defer conn.Close()
 42: 	// Create a feed first
 43: 	feed := models.NewFeed("https://example.com/feed.xml")
 44: 	err := CreateFeed(conn, feed)
 45: 	if err != nil {
 46: 		t.Fatalf("CreateFeed failed: %v", err)
 47: 	}
 48: 	// Create an entry
 49: 	entry1 := models.NewEntry(feed.ID, "entry-guid-123", "Test Entry 1")
 50: 	err = CreateEntry(conn, entry1)
 51: 	if err != nil {
 52: 		t.Fatalf("CreateEntry failed: %v", err)
 53: 	}
 54: 	// Try to create another entry with same feed_id and guid
 55: 	entry2 := models.NewEntry(feed.ID, "entry-guid-123", "Test Entry 2")
 56: 	err = CreateEntry(conn, entry2)
 57: 	if err == nil {
 58: 		t.Error("expected CreateEntry to fail with duplicate feed_id+guid, but it succeeded")
 59: 	}
 60: }
 61: func TestListEntries_Unread(t *testing.T) {
 62: 	conn := setupTestDB(t)
 63: 	defer conn.Close()
 64: 	// Create a feed first
 65: 	feed := models.NewFeed("https://example.com/feed.xml")
 66: 	err := CreateFeed(conn, feed)
 67: 	if err != nil {
 68: 		t.Fatalf("CreateFeed failed: %v", err)
 69: 	}
 70: 	// Create entries
 71: 	entry1 := models.NewEntry(feed.ID, "guid-1", "Entry 1")
 72: 	entry2 := models.NewEntry(feed.ID, "guid-2", "Entry 2")
 73: 	entry3 := models.NewEntry(feed.ID, "guid-3", "Entry 3")
 74: 	_ = CreateEntry(conn, entry1)
 75: 	_ = CreateEntry(conn, entry2)
 76: 	_ = CreateEntry(conn, entry3)
 77: 	// Mark entry2 as read
 78: 	err = MarkEntryRead(conn, entry2.ID)
 79: 	if err != nil {
 80: 		t.Fatalf("MarkEntryRead failed: %v", err)
 81: 	}
 82: 	// List unread entries only
 83: 	unreadOnly := true
 84: 	entries, err := ListEntries(conn, &feed.ID, nil, &unreadOnly, nil, nil, nil)
 85: 	if err != nil {
 86: 		t.Fatalf("ListEntries failed: %v", err)
 87: 	}
 88: 	if len(entries) != 2 {
 89: 		t.Errorf("expected 2 unread entries, got %d", len(entries))
 90: 	}
 91: 	// Verify the unread entries are entry1 and entry3
 92: 	for _, e := range entries {
 93: 		if e.ID == entry2.ID {
 94: 			t.Error("expected entry2 to be filtered out as it's marked read")
 95: 		}
 96: 	}
 97: }
 98: func TestMarkEntryRead(t *testing.T) {
 99: 	conn := setupTestDB(t)
100: 	defer conn.Close()
101: 	// Create a feed first
102: 	feed := models.NewFeed("https://example.com/feed.xml")
103: 	err := CreateFeed(conn, feed)
104: 	if err != nil {
105: 		t.Fatalf("CreateFeed failed: %v", err)
106: 	}
107: 	// Create an entry
108: 	entry := models.NewEntry(feed.ID, "guid-1", "Entry 1")
109: 	err = CreateEntry(conn, entry)
110: 	if err != nil {
111: 		t.Fatalf("CreateEntry failed: %v", err)
112: 	}
113: 	// Mark as read
114: 	err = MarkEntryRead(conn, entry.ID)
115: 	if err != nil {
116: 		t.Fatalf("MarkEntryRead failed: %v", err)
117: 	}
118: 	// Verify it's marked as read
119: 	got, err := GetEntryByID(conn, entry.ID)
120: 	if err != nil {
121: 		t.Fatalf("GetEntryByID failed: %v", err)
122: 	}
123: 	if got.Read != true {
124: 		t.Errorf("expected Read to be true, got %v", got.Read)
125: 	}
126: 	if got.ReadAt == nil {
127: 		t.Error("expected ReadAt to be set, got nil")
128: 	}
129: }
130: func TestEntryExists(t *testing.T) {
131: 	conn := setupTestDB(t)
132: 	defer conn.Close()
133: 	// Create a feed first
134: 	feed := models.NewFeed("https://example.com/feed.xml")
135: 	err := CreateFeed(conn, feed)
136: 	if err != nil {
137: 		t.Fatalf("CreateFeed failed: %v", err)
138: 	}
139: 	// Create an entry
140: 	entry := models.NewEntry(feed.ID, "guid-1", "Entry 1")
141: 	err = CreateEntry(conn, entry)
142: 	if err != nil {
143: 		t.Fatalf("CreateEntry failed: %v", err)
144: 	}
145: 	// Check if entry exists
146: 	exists, err := EntryExists(conn, feed.ID, "guid-1")
147: 	if err != nil {
148: 		t.Fatalf("EntryExists failed: %v", err)
149: 	}
150: 	if !exists {
151: 		t.Error("expected entry to exist, but EntryExists returned false")
152: 	}
153: 	// Check if non-existent entry exists
154: 	exists, err = EntryExists(conn, feed.ID, "non-existent-guid")
155: 	if err != nil {
156: 		t.Fatalf("EntryExists failed: %v", err)
157: 	}
158: 	if exists {
159: 		t.Error("expected entry not to exist, but EntryExists returned true")
160: 	}
161: }
162: func TestCountUnreadEntries(t *testing.T) {
163: 	conn := setupTestDB(t)
164: 	defer conn.Close()
165: 	// Create two feeds
166: 	feed1 := models.NewFeed("https://example.com/feed1.xml")
167: 	feed2 := models.NewFeed("https://example.com/feed2.xml")
168: 	_ = CreateFeed(conn, feed1)
169: 	_ = CreateFeed(conn, feed2)
170: 	// Create entries for feed1
171: 	entry1 := models.NewEntry(feed1.ID, "guid-1", "Entry 1")
172: 	entry2 := models.NewEntry(feed1.ID, "guid-2", "Entry 2")
173: 	entry3 := models.NewEntry(feed1.ID, "guid-3", "Entry 3")
174: 	_ = CreateEntry(conn, entry1)
175: 	_ = CreateEntry(conn, entry2)
176: 	_ = CreateEntry(conn, entry3)
177: 	// Create entries for feed2
178: 	entry4 := models.NewEntry(feed2.ID, "guid-4", "Entry 4")
179: 	entry5 := models.NewEntry(feed2.ID, "guid-5", "Entry 5")
180: 	_ = CreateEntry(conn, entry4)
181: 	_ = CreateEntry(conn, entry5)
182: 	// Mark one entry from feed1 as read
183: 	_ = MarkEntryRead(conn, entry2.ID)
184: 	// Count unread entries for feed1
185: 	count, err := CountUnreadEntries(conn, &feed1.ID)
186: 	if err != nil {
187: 		t.Fatalf("CountUnreadEntries failed: %v", err)
188: 	}
189: 	if count != 2 {
190: 		t.Errorf("expected 2 unread entries for feed1, got %d", count)
191: 	}
192: 	// Count all unread entries
193: 	count, err = CountUnreadEntries(conn, nil)
194: 	if err != nil {
195: 		t.Fatalf("CountUnreadEntries failed: %v", err)
196: 	}
197: 	if count != 4 {
198: 		t.Errorf("expected 4 total unread entries, got %d", count)
199: 	}
200: }
201: func TestGetEntryByPrefix(t *testing.T) {
202: 	conn := setupTestDB(t)
203: 	defer conn.Close()
204: 	// Create a feed first
205: 	feed := models.NewFeed("https://example.com/feed.xml")
206: 	err := CreateFeed(conn, feed)
207: 	if err != nil {
208: 		t.Fatalf("CreateFeed failed: %v", err)
209: 	}
210: 	// Create an entry
211: 	entry := models.NewEntry(feed.ID, "guid-1", "Entry 1")
212: 	err = CreateEntry(conn, entry)
213: 	if err != nil {
214: 		t.Fatalf("CreateEntry failed: %v", err)
215: 	}
216: 	// Use first 8 chars of UUID
217: 	prefix := entry.ID[:8]
218: 	got, err := GetEntryByPrefix(conn, prefix)
219: 	if err != nil {
220: 		t.Fatalf("GetEntryByPrefix failed: %v", err)
221: 	}
222: 	if got.ID != entry.ID {
223: 		t.Errorf("expected ID %s, got %s", entry.ID, got.ID)
224: 	}
225: }
226: func TestMarkEntryUnread(t *testing.T) {
227: 	conn := setupTestDB(t)
228: 	defer conn.Close()
229: 	// Create a feed first
230: 	feed := models.NewFeed("https://example.com/feed.xml")
231: 	err := CreateFeed(conn, feed)
232: 	if err != nil {
233: 		t.Fatalf("CreateFeed failed: %v", err)
234: 	}
235: 	// Create an entry
236: 	entry := models.NewEntry(feed.ID, "guid-1", "Entry 1")
237: 	err = CreateEntry(conn, entry)
238: 	if err != nil {
239: 		t.Fatalf("CreateEntry failed: %v", err)
240: 	}
241: 	// Mark as read
242: 	err = MarkEntryRead(conn, entry.ID)
243: 	if err != nil {
244: 		t.Fatalf("MarkEntryRead failed: %v", err)
245: 	}
246: 	// Mark as unread
247: 	err = MarkEntryUnread(conn, entry.ID)
248: 	if err != nil {
249: 		t.Fatalf("MarkEntryUnread failed: %v", err)
250: 	}
251: 	// Verify it's marked as unread
252: 	got, err := GetEntryByID(conn, entry.ID)
253: 	if err != nil {
254: 		t.Fatalf("GetEntryByID failed: %v", err)
255: 	}
256: 	if got.Read != false {
257: 		t.Errorf("expected Read to be false, got %v", got.Read)
258: 	}
259: 	if got.ReadAt != nil {
260: 		t.Errorf("expected ReadAt to be nil, got %v", got.ReadAt)
261: 	}
262: }
263: func TestListEntries_WithFilters(t *testing.T) {
264: 	conn := setupTestDB(t)
265: 	defer conn.Close()
266: 	// Create a feed
267: 	feed := models.NewFeed("https://example.com/feed.xml")
268: 	_ = CreateFeed(conn, feed)
269: 	// Create entries with different published times
270: 	now := time.Now()
271: 	past := now.Add(-24 * time.Hour)
272: 	entry1 := models.NewEntry(feed.ID, "guid-1", "Entry 1")
273: 	entry1.PublishedAt = &past
274: 	entry2 := models.NewEntry(feed.ID, "guid-2", "Entry 2")
275: 	entry2.PublishedAt = &now
276: 	entry3 := models.NewEntry(feed.ID, "guid-3", "Entry 3")
277: 	entry3.PublishedAt = &now
278: 	_ = CreateEntry(conn, entry1)
279: 	_ = CreateEntry(conn, entry2)
280: 	_ = CreateEntry(conn, entry3)
281: 	// Test with since filter
282: 	sinceTime := now.Add(-1 * time.Hour)
283: 	entries, err := ListEntries(conn, nil, nil, nil, &sinceTime, nil, nil)
284: 	if err != nil {
285: 		t.Fatalf("ListEntries failed: %v", err)
286: 	}
287: 	if len(entries) != 2 {
288: 		t.Errorf("expected 2 entries since %v, got %d", sinceTime, len(entries))
289: 	}
290: 	// Test with limit
291: 	limit := 2
292: 	entries, err = ListEntries(conn, nil, nil, nil, nil, nil, &limit)
293: 	if err != nil {
294: 		t.Fatalf("ListEntries failed: %v", err)
295: 	}
296: 	if len(entries) != 2 {
297: 		t.Errorf("expected 2 entries with limit, got %d", len(entries))
298: 	}
299: 	// Test with until filter (should only return entry1 which is in the past)
300: 	untilTime := now.Add(-12 * time.Hour)
301: 	entries, err = ListEntries(conn, nil, nil, nil, nil, &untilTime, nil)
302: 	if err != nil {
303: 		t.Fatalf("ListEntries with until failed: %v", err)
304: 	}
305: 	if len(entries) != 1 {
306: 		t.Errorf("expected 1 entry before %v, got %d", untilTime, len(entries))
307: 	}
308: 	// Test with since AND until (yesterday's window)
309: 	sinceYesterday := now.Add(-25 * time.Hour)
310: 	untilYesterday := now.Add(-23 * time.Hour)
311: 	entries, err = ListEntries(conn, nil, nil, nil, &sinceYesterday, &untilYesterday, nil)
312: 	if err != nil {
313: 		t.Fatalf("ListEntries with since and until failed: %v", err)
314: 	}
315: 	if len(entries) != 1 {
316: 		t.Errorf("expected 1 entry in yesterday window, got %d", len(entries))
317: 	}
318: }
319: func TestListEntries_ByFeedIDs(t *testing.T) {
320: 	conn := setupTestDB(t)
321: 	defer conn.Close()
322: 	// Create two feeds
323: 	feed1 := models.NewFeed("https://example.com/feed1.xml")
324: 	feed2 := models.NewFeed("https://example.com/feed2.xml")
325: 	feed3 := models.NewFeed("https://example.com/feed3.xml")
326: 	_ = CreateFeed(conn, feed1)
327: 	_ = CreateFeed(conn, feed2)
328: 	_ = CreateFeed(conn, feed3)
329: 	// Create entries for each feed
330: 	entry1 := models.NewEntry(feed1.ID, "guid-1", "Entry from Feed 1")
331: 	entry2 := models.NewEntry(feed2.ID, "guid-2", "Entry from Feed 2")
332: 	entry3 := models.NewEntry(feed3.ID, "guid-3", "Entry from Feed 3")
333: 	_ = CreateEntry(conn, entry1)
334: 	_ = CreateEntry(conn, entry2)
335: 	_ = CreateEntry(conn, entry3)
336: 	// Test filtering by multiple feed IDs
337: 	feedIDs := []string{feed1.ID, feed2.ID}
338: 	entries, err := ListEntries(conn, nil, feedIDs, nil, nil, nil, nil)
339: 	if err != nil {
340: 		t.Fatalf("ListEntries with feedIDs failed: %v", err)
341: 	}
342: 	if len(entries) != 2 {
343: 		t.Errorf("expected 2 entries from feed1 and feed2, got %d", len(entries))
344: 	}
345: 	// Verify only entries from feed1 and feed2 are returned
346: 	for _, entry := range entries {
347: 		if entry.FeedID != feed1.ID && entry.FeedID != feed2.ID {
348: 			t.Errorf("unexpected entry from feed %s", entry.FeedID)
349: 		}
350: 	}
351: 	// Test with single feed in array (should work same as feedID)
352: 	feedIDs = []string{feed3.ID}
353: 	entries, err = ListEntries(conn, nil, feedIDs, nil, nil, nil, nil)
354: 	if err != nil {
355: 		t.Fatalf("ListEntries with single feedID in array failed: %v", err)
356: 	}
357: 	if len(entries) != 1 {
358: 		t.Errorf("expected 1 entry from feed3, got %d", len(entries))
359: 	}
360: 	// Test feedIDs takes precedence over feedID
361: 	entries, err = ListEntries(conn, &feed1.ID, []string{feed2.ID, feed3.ID}, nil, nil, nil, nil)
362: 	if err != nil {
363: 		t.Fatalf("ListEntries with both feedID and feedIDs failed: %v", err)
364: 	}
365: 	// Should return entries from feed2 and feed3, not feed1
366: 	if len(entries) != 2 {
367: 		t.Errorf("expected 2 entries (feedIDs should override feedID), got %d", len(entries))
368: 	}
369: 	for _, entry := range entries {
370: 		if entry.FeedID == feed1.ID {
371: 			t.Error("feedIDs should take precedence over feedID")
372: 		}
373: 	}
374: }
375: func TestMarkEntriesReadBefore(t *testing.T) {
376: 	conn := setupTestDB(t)
377: 	defer conn.Close()
378: 	// Create a feed
379: 	feed := models.NewFeed("https://example.com/feed.xml")
380: 	_ = CreateFeed(conn, feed)
381: 	// Create entries with different published times
382: 	now := time.Now()
383: 	twoDaysAgo := now.Add(-48 * time.Hour)
384: 	yesterday := now.Add(-24 * time.Hour)
385: 	entry1 := models.NewEntry(feed.ID, "guid-old", "Old Entry")
386: 	entry1.PublishedAt = &twoDaysAgo
387: 	entry2 := models.NewEntry(feed.ID, "guid-yesterday", "Yesterday Entry")
388: 	entry2.PublishedAt = &yesterday
389: 	entry3 := models.NewEntry(feed.ID, "guid-today", "Today Entry")
390: 	entry3.PublishedAt = &now
391: 	_ = CreateEntry(conn, entry1)
392: 	_ = CreateEntry(conn, entry2)
393: 	_ = CreateEntry(conn, entry3)
394: 	// Mark entries older than yesterday as read
395: 	cutoff := now.Add(-24 * time.Hour)
396: 	count, err := MarkEntriesReadBefore(conn, cutoff)
397: 	if err != nil {
398: 		t.Fatalf("MarkEntriesReadBefore failed: %v", err)
399: 	}
400: 	if count != 1 {
401: 		t.Errorf("expected 1 entry marked as read, got %d", count)
402: 	}
403: 	// Verify the old entry is marked as read
404: 	oldEntry, _ := GetEntryByID(conn, entry1.ID)
405: 	if !oldEntry.Read {
406: 		t.Error("expected old entry to be marked as read")
407: 	}
408: 	// Verify the yesterday entry is still unread
409: 	yesterdayEntry, _ := GetEntryByID(conn, entry2.ID)
410: 	if yesterdayEntry.Read {
411: 		t.Error("expected yesterday entry to still be unread")
412: 	}
413: 	// Verify today's entry is still unread
414: 	todayEntry, _ := GetEntryByID(conn, entry3.ID)
415: 	if todayEntry.Read {
416: 		t.Error("expected today entry to still be unread")
417: 	}
418: 	// Mark remaining entries as read before now
419: 	count, err = MarkEntriesReadBefore(conn, now.Add(time.Hour))
420: 	if err != nil {
421: 		t.Fatalf("MarkEntriesReadBefore second call failed: %v", err)
422: 	}
423: 	if count != 2 {
424: 		t.Errorf("expected 2 entries marked as read, got %d", count)
425: 	}
426: 	// Verify all entries are now read
427: 	unreadCount, _ := CountUnreadEntries(conn, nil)
428: 	if unreadCount != 0 {
429: 		t.Errorf("expected 0 unread entries, got %d", unreadCount)
430: 	}
431: }
</file>

<file path="internal/db/feeds.go">
  1: // ABOUTME: Feed database operations
  2: // ABOUTME: CRUD operations for the feeds table
  3: package db
  4: import (
  5: 	"database/sql"
  6: 	"fmt"
  7: 	"strings"
  8: 	"time"
  9: 	"github.com/harper/digest/internal/models"
 10: )
 11: func CreateFeed(db *sql.DB, feed *models.Feed) error {
 12: 	_, err := db.Exec(`
 13: 		INSERT INTO feeds (id, url, title, etag, last_modified, last_fetched_at, last_error, error_count, created_at)
 14: 		VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)`,
 15: 		feed.ID, feed.URL, feed.Title, feed.ETag, feed.LastModified,
 16: 		feed.LastFetchedAt, feed.LastError, feed.ErrorCount, feed.CreatedAt,
 17: 	)
 18: 	return err
 19: }
 20: func GetFeedByURL(db *sql.DB, url string) (*models.Feed, error) {
 21: 	feed := &models.Feed{}
 22: 	err := db.QueryRow(`
 23: 		SELECT id, url, title, etag, last_modified, last_fetched_at, last_error, error_count, created_at
 24: 		FROM feeds WHERE url = ?`, url,
 25: 	).Scan(&feed.ID, &feed.URL, &feed.Title, &feed.ETag, &feed.LastModified,
 26: 		&feed.LastFetchedAt, &feed.LastError, &feed.ErrorCount, &feed.CreatedAt)
 27: 	if err != nil {
 28: 		return nil, err
 29: 	}
 30: 	return feed, nil
 31: }
 32: func GetFeedByPrefix(db *sql.DB, prefix string) (*models.Feed, error) {
 33: 	if len(prefix) < 6 {
 34: 		return nil, fmt.Errorf("prefix must be at least 6 characters")
 35: 	}
 36: 	// Escape SQL wildcards in prefix
 37: 	escapedPrefix := strings.ReplaceAll(prefix, "%", "\\%")
 38: 	escapedPrefix = strings.ReplaceAll(escapedPrefix, "_", "\\_")
 39: 	rows, err := db.Query(`
 40: 		SELECT id, url, title, etag, last_modified, last_fetched_at, last_error, error_count, created_at
 41: 		FROM feeds WHERE id LIKE ? ESCAPE '\'`, escapedPrefix+"%",
 42: 	)
 43: 	if err != nil {
 44: 		return nil, err
 45: 	}
 46: 	defer rows.Close()
 47: 	var feeds []*models.Feed
 48: 	for rows.Next() {
 49: 		feed := &models.Feed{}
 50: 		if err := rows.Scan(&feed.ID, &feed.URL, &feed.Title, &feed.ETag, &feed.LastModified,
 51: 			&feed.LastFetchedAt, &feed.LastError, &feed.ErrorCount, &feed.CreatedAt); err != nil {
 52: 			return nil, err
 53: 		}
 54: 		feeds = append(feeds, feed)
 55: 	}
 56: 	if err := rows.Err(); err != nil {
 57: 		return nil, fmt.Errorf("error iterating feeds: %w", err)
 58: 	}
 59: 	if len(feeds) == 0 {
 60: 		return nil, fmt.Errorf("no feed found with prefix %s", prefix)
 61: 	}
 62: 	if len(feeds) > 1 {
 63: 		return nil, fmt.Errorf("ambiguous prefix %s matches %d feeds", prefix, len(feeds))
 64: 	}
 65: 	return feeds[0], nil
 66: }
 67: func GetFeedByID(db *sql.DB, id string) (*models.Feed, error) {
 68: 	feed := &models.Feed{}
 69: 	err := db.QueryRow(`
 70: 		SELECT id, url, title, etag, last_modified, last_fetched_at, last_error, error_count, created_at
 71: 		FROM feeds WHERE id = ?`, id,
 72: 	).Scan(&feed.ID, &feed.URL, &feed.Title, &feed.ETag, &feed.LastModified,
 73: 		&feed.LastFetchedAt, &feed.LastError, &feed.ErrorCount, &feed.CreatedAt)
 74: 	if err != nil {
 75: 		return nil, err
 76: 	}
 77: 	return feed, nil
 78: }
 79: func ListFeeds(db *sql.DB) ([]*models.Feed, error) {
 80: 	rows, err := db.Query(`
 81: 		SELECT id, url, title, etag, last_modified, last_fetched_at, last_error, error_count, created_at
 82: 		FROM feeds ORDER BY created_at DESC`)
 83: 	if err != nil {
 84: 		return nil, err
 85: 	}
 86: 	defer rows.Close()
 87: 	var feeds []*models.Feed
 88: 	for rows.Next() {
 89: 		feed := &models.Feed{}
 90: 		if err := rows.Scan(&feed.ID, &feed.URL, &feed.Title, &feed.ETag, &feed.LastModified,
 91: 			&feed.LastFetchedAt, &feed.LastError, &feed.ErrorCount, &feed.CreatedAt); err != nil {
 92: 			return nil, err
 93: 		}
 94: 		feeds = append(feeds, feed)
 95: 	}
 96: 	if err := rows.Err(); err != nil {
 97: 		return nil, fmt.Errorf("error iterating feeds: %w", err)
 98: 	}
 99: 	return feeds, nil
100: }
101: func UpdateFeed(db *sql.DB, feed *models.Feed) error {
102: 	_, err := db.Exec(`
103: 		UPDATE feeds SET
104: 			title = ?, etag = ?, last_modified = ?, last_fetched_at = ?,
105: 			last_error = ?, error_count = ?
106: 		WHERE id = ?`,
107: 		feed.Title, feed.ETag, feed.LastModified, feed.LastFetchedAt,
108: 		feed.LastError, feed.ErrorCount, feed.ID,
109: 	)
110: 	return err
111: }
112: func DeleteFeed(db *sql.DB, id string) error {
113: 	_, err := db.Exec("DELETE FROM feeds WHERE id = ?", id)
114: 	return err
115: }
116: func UpdateFeedFetchState(db *sql.DB, feedID string, etag, lastModified *string, fetchedAt time.Time) error {
117: 	_, err := db.Exec(`
118: 		UPDATE feeds SET etag = ?, last_modified = ?, last_fetched_at = ?, last_error = NULL, error_count = 0
119: 		WHERE id = ?`,
120: 		etag, lastModified, fetchedAt, feedID,
121: 	)
122: 	return err
123: }
124: func UpdateFeedError(db *sql.DB, feedID string, errMsg string) error {
125: 	_, err := db.Exec(`
126: 		UPDATE feeds SET last_error = ?, error_count = error_count + 1
127: 		WHERE id = ?`,
128: 		errMsg, feedID,
129: 	)
130: 	return err
131: }
</file>

<file path="internal/models/feed_test.go">
  1: // ABOUTME: Test suite for Feed model, validating feed creation and cache header management
  2: // ABOUTME: Ensures feed instances have proper IDs, timestamps, and can store HTTP caching metadata
  3: package models
  4: import (
  5: 	"testing"
  6: 	"time"
  7: )
  8: func TestNewFeed(t *testing.T) {
  9: 	url := "https://example.com/feed.xml"
 10: 	feed := NewFeed(url)
 11: 	// Verify URL is set correctly
 12: 	if feed.URL != url {
 13: 		t.Errorf("expected URL to be %q, got %q", url, feed.URL)
 14: 	}
 15: 	// Verify ID is generated (non-empty)
 16: 	if feed.ID == "" {
 17: 		t.Error("expected feed ID to be generated, got empty string")
 18: 	}
 19: 	// Verify CreatedAt is set (not zero)
 20: 	if feed.CreatedAt.IsZero() {
 21: 		t.Error("expected CreatedAt to be set, got zero time")
 22: 	}
 23: 	// Verify CreatedAt is recent (within last second)
 24: 	now := time.Now()
 25: 	if feed.CreatedAt.After(now) || feed.CreatedAt.Before(now.Add(-time.Second)) {
 26: 		t.Errorf("expected CreatedAt to be recent, got %v", feed.CreatedAt)
 27: 	}
 28: }
 29: func TestFeed_SetCacheHeaders(t *testing.T) {
 30: 	feed := NewFeed("https://example.com/feed.xml")
 31: 	etag := `"abc123"`
 32: 	lastModified := "Mon, 02 Jan 2006 15:04:05 GMT"
 33: 	feed.SetCacheHeaders(etag, lastModified)
 34: 	// Verify ETag is set
 35: 	if feed.ETag == nil || *feed.ETag != etag {
 36: 		t.Errorf("expected ETag to be %q, got %v", etag, feed.ETag)
 37: 	}
 38: 	// Verify LastModified is set
 39: 	if feed.LastModified == nil || *feed.LastModified != lastModified {
 40: 		t.Errorf("expected LastModified to be %q, got %v", lastModified, feed.LastModified)
 41: 	}
 42: }
 43: func TestNewEntry(t *testing.T) {
 44: 	feedID := "feed-123"
 45: 	guid := "entry-guid-456"
 46: 	title := "Test Entry Title"
 47: 	entry := NewEntry(feedID, guid, title)
 48: 	// Verify FeedID is set correctly
 49: 	if entry.FeedID != feedID {
 50: 		t.Errorf("expected FeedID to be %q, got %q", feedID, entry.FeedID)
 51: 	}
 52: 	// Verify GUID is set correctly
 53: 	if entry.GUID != guid {
 54: 		t.Errorf("expected GUID to be %q, got %q", guid, entry.GUID)
 55: 	}
 56: 	// Verify Title is set correctly
 57: 	if entry.Title == nil || *entry.Title != title {
 58: 		t.Errorf("expected Title to be %q, got %v", title, entry.Title)
 59: 	}
 60: 	// Verify ID is generated (non-empty)
 61: 	if entry.ID == "" {
 62: 		t.Error("expected entry ID to be generated, got empty string")
 63: 	}
 64: 	// Verify CreatedAt is set (not zero)
 65: 	if entry.CreatedAt.IsZero() {
 66: 		t.Error("expected CreatedAt to be set, got zero time")
 67: 	}
 68: 	// Verify CreatedAt is recent (within last second)
 69: 	now := time.Now()
 70: 	if entry.CreatedAt.After(now) || entry.CreatedAt.Before(now.Add(-time.Second)) {
 71: 		t.Errorf("expected CreatedAt to be recent, got %v", entry.CreatedAt)
 72: 	}
 73: 	// Verify Read is initially false
 74: 	if entry.Read {
 75: 		t.Error("expected Read to be false initially, got true")
 76: 	}
 77: 	// Verify ReadAt is initially nil
 78: 	if entry.ReadAt != nil {
 79: 		t.Errorf("expected ReadAt to be nil initially, got %v", entry.ReadAt)
 80: 	}
 81: }
 82: func TestEntry_MarkRead(t *testing.T) {
 83: 	entry := NewEntry("feed-123", "guid-456", "Test Entry")
 84: 	// Mark the entry as read
 85: 	entry.MarkRead()
 86: 	// Verify Read is now true
 87: 	if !entry.Read {
 88: 		t.Error("expected Read to be true after MarkRead, got false")
 89: 	}
 90: 	// Verify ReadAt is set (not nil)
 91: 	if entry.ReadAt == nil {
 92: 		t.Error("expected ReadAt to be set after MarkRead, got nil")
 93: 	}
 94: 	// Verify ReadAt is recent (within last second)
 95: 	now := time.Now()
 96: 	if entry.ReadAt.After(now) || entry.ReadAt.Before(now.Add(-time.Second)) {
 97: 		t.Errorf("expected ReadAt to be recent, got %v", entry.ReadAt)
 98: 	}
 99: }
100: func TestEntry_MarkUnread(t *testing.T) {
101: 	entry := NewEntry("feed-123", "guid-456", "Test Entry")
102: 	// First mark as read
103: 	entry.MarkRead()
104: 	// Verify it's marked as read
105: 	if !entry.Read {
106: 		t.Error("expected Read to be true after MarkRead, got false")
107: 	}
108: 	// Now mark as unread
109: 	entry.MarkUnread()
110: 	// Verify Read is now false
111: 	if entry.Read {
112: 		t.Error("expected Read to be false after MarkUnread, got true")
113: 	}
114: 	// Verify ReadAt is now nil
115: 	if entry.ReadAt != nil {
116: 		t.Errorf("expected ReadAt to be nil after MarkUnread, got %v", entry.ReadAt)
117: 	}
118: }
</file>

<file path="internal/opml/opml_test.go">
  1: // ABOUTME: Test suite for OPML parsing, writing, and manipulation
  2: // ABOUTME: Covers parsing XML, adding feeds, folder management, and round-trip integrity
  3: package opml
  4: import (
  5: 	"bytes"
  6: 	"path/filepath"
  7: 	"testing"
  8: )
  9: func TestParseOPML(t *testing.T) {
 10: 	opmlData := `<?xml version="1.0" encoding="UTF-8"?>
 11: <opml version="2.0">
 12:   <head>
 13:     <title>My Feeds</title>
 14:   </head>
 15:   <body>
 16:     <outline text="Tech News">
 17:       <outline type="rss" text="Hacker News" xmlUrl="https://hnrss.org/frontpage" />
 18:       <outline type="rss" text="TechCrunch" xmlUrl="https://techcrunch.com/feed/" />
 19:     </outline>
 20:     <outline text="Blogs">
 21:       <outline type="rss" text="Joel on Software" xmlUrl="https://www.joelonsoftware.com/feed/" />
 22:     </outline>
 23:     <outline type="rss" text="No Folder Feed" xmlUrl="https://example.com/feed" />
 24:   </body>
 25: </opml>`
 26: 	doc, err := Parse(bytes.NewBufferString(opmlData))
 27: 	if err != nil {
 28: 		t.Fatalf("Parse() error = %v", err)
 29: 	}
 30: 	if doc.Title != "My Feeds" {
 31: 		t.Errorf("Title = %q, want %q", doc.Title, "My Feeds")
 32: 	}
 33: 	feeds := doc.AllFeeds()
 34: 	if len(feeds) != 4 {
 35: 		t.Fatalf("AllFeeds() returned %d feeds, want 4", len(feeds))
 36: 	}
 37: 	// Check Tech News folder
 38: 	techFeeds := doc.FeedsInFolder("Tech News")
 39: 	if len(techFeeds) != 2 {
 40: 		t.Errorf("FeedsInFolder('Tech News') = %d feeds, want 2", len(techFeeds))
 41: 	}
 42: 	// Check Blogs folder
 43: 	blogFeeds := doc.FeedsInFolder("Blogs")
 44: 	if len(blogFeeds) != 1 {
 45: 		t.Errorf("FeedsInFolder('Blogs') = %d feeds, want 1", len(blogFeeds))
 46: 	}
 47: 	// Check root level feed
 48: 	rootFeeds := doc.FeedsInFolder("")
 49: 	if len(rootFeeds) != 1 {
 50: 		t.Errorf("FeedsInFolder('') = %d feeds, want 1", len(rootFeeds))
 51: 	}
 52: 	// Check folders list
 53: 	folders := doc.Folders()
 54: 	expectedFolders := map[string]bool{"Tech News": true, "Blogs": true}
 55: 	if len(folders) != len(expectedFolders) {
 56: 		t.Errorf("Folders() returned %d folders, want %d", len(folders), len(expectedFolders))
 57: 	}
 58: 	for _, folder := range folders {
 59: 		if !expectedFolders[folder] {
 60: 			t.Errorf("Unexpected folder: %q", folder)
 61: 		}
 62: 	}
 63: }
 64: func TestOPML_AddFeed(t *testing.T) {
 65: 	doc := NewDocument("Test Document")
 66: 	err := doc.AddFeed("https://example.com/feed", "Example Feed", "")
 67: 	if err != nil {
 68: 		t.Fatalf("AddFeed() error = %v", err)
 69: 	}
 70: 	feeds := doc.AllFeeds()
 71: 	if len(feeds) != 1 {
 72: 		t.Fatalf("AllFeeds() = %d feeds, want 1", len(feeds))
 73: 	}
 74: 	feed := feeds[0]
 75: 	if feed.URL != "https://example.com/feed" {
 76: 		t.Errorf("feed.URL = %q, want %q", feed.URL, "https://example.com/feed")
 77: 	}
 78: 	if feed.Title != "Example Feed" {
 79: 		t.Errorf("feed.Title = %q, want %q", feed.Title, "Example Feed")
 80: 	}
 81: 	if feed.Folder != "" {
 82: 		t.Errorf("feed.Folder = %q, want empty string", feed.Folder)
 83: 	}
 84: }
 85: func TestOPML_AddFeedToFolder(t *testing.T) {
 86: 	doc := NewDocument("Test Document")
 87: 	err := doc.AddFeed("https://example.com/feed1", "Feed 1", "Tech")
 88: 	if err != nil {
 89: 		t.Fatalf("AddFeed() error = %v", err)
 90: 	}
 91: 	err = doc.AddFeed("https://example.com/feed2", "Feed 2", "Tech")
 92: 	if err != nil {
 93: 		t.Fatalf("AddFeed() error = %v", err)
 94: 	}
 95: 	err = doc.AddFeed("https://example.com/feed3", "Feed 3", "News")
 96: 	if err != nil {
 97: 		t.Fatalf("AddFeed() error = %v", err)
 98: 	}
 99: 	// Verify feeds in Tech folder
100: 	techFeeds := doc.FeedsInFolder("Tech")
101: 	if len(techFeeds) != 2 {
102: 		t.Fatalf("FeedsInFolder('Tech') = %d feeds, want 2", len(techFeeds))
103: 	}
104: 	// Verify feeds in News folder
105: 	newsFeeds := doc.FeedsInFolder("News")
106: 	if len(newsFeeds) != 1 {
107: 		t.Fatalf("FeedsInFolder('News') = %d feeds, want 1", len(newsFeeds))
108: 	}
109: 	// Verify total feeds
110: 	allFeeds := doc.AllFeeds()
111: 	if len(allFeeds) != 3 {
112: 		t.Errorf("AllFeeds() = %d feeds, want 3", len(allFeeds))
113: 	}
114: 	// Verify folders
115: 	folders := doc.Folders()
116: 	if len(folders) != 2 {
117: 		t.Fatalf("Folders() = %d folders, want 2", len(folders))
118: 	}
119: }
120: func TestOPML_RoundTrip(t *testing.T) {
121: 	// Create a document
122: 	doc := NewDocument("Round Trip Test")
123: 	doc.AddFeed("https://example.com/feed1", "Feed 1", "Folder A")
124: 	doc.AddFeed("https://example.com/feed2", "Feed 2", "Folder A")
125: 	doc.AddFeed("https://example.com/feed3", "Feed 3", "Folder B")
126: 	doc.AddFeed("https://example.com/feed4", "Feed 4", "")
127: 	// Write to file
128: 	tmpDir := t.TempDir()
129: 	tmpFile := filepath.Join(tmpDir, "test.opml")
130: 	err := doc.WriteFile(tmpFile)
131: 	if err != nil {
132: 		t.Fatalf("WriteFile() error = %v", err)
133: 	}
134: 	// Parse it back
135: 	doc2, err := ParseFile(tmpFile)
136: 	if err != nil {
137: 		t.Fatalf("ParseFile() error = %v", err)
138: 	}
139: 	// Verify title
140: 	if doc2.Title != doc.Title {
141: 		t.Errorf("Title = %q, want %q", doc2.Title, doc.Title)
142: 	}
143: 	// Verify feeds
144: 	feeds1 := doc.AllFeeds()
145: 	feeds2 := doc2.AllFeeds()
146: 	if len(feeds2) != len(feeds1) {
147: 		t.Fatalf("AllFeeds() = %d feeds, want %d", len(feeds2), len(feeds1))
148: 	}
149: 	// Create maps for comparison
150: 	feedMap1 := make(map[string]Feed)
151: 	for _, f := range feeds1 {
152: 		feedMap1[f.URL] = f
153: 	}
154: 	feedMap2 := make(map[string]Feed)
155: 	for _, f := range feeds2 {
156: 		feedMap2[f.URL] = f
157: 	}
158: 	// Compare feeds
159: 	for url, f1 := range feedMap1 {
160: 		f2, ok := feedMap2[url]
161: 		if !ok {
162: 			t.Errorf("Feed %q not found after round trip", url)
163: 			continue
164: 		}
165: 		if f1.Title != f2.Title {
166: 			t.Errorf("Feed %q: Title = %q, want %q", url, f2.Title, f1.Title)
167: 		}
168: 		if f1.Folder != f2.Folder {
169: 			t.Errorf("Feed %q: Folder = %q, want %q", url, f2.Folder, f1.Folder)
170: 		}
171: 	}
172: 	// Verify folders
173: 	folders1 := doc.Folders()
174: 	folders2 := doc2.Folders()
175: 	if len(folders2) != len(folders1) {
176: 		t.Errorf("Folders() = %d, want %d", len(folders2), len(folders1))
177: 	}
178: }
179: func TestOPML_RemoveFeed(t *testing.T) {
180: 	doc := NewDocument("Test Document")
181: 	doc.AddFeed("https://example.com/feed1", "Feed 1", "Tech")
182: 	doc.AddFeed("https://example.com/feed2", "Feed 2", "Tech")
183: 	doc.AddFeed("https://example.com/feed3", "Feed 3", "News")
184: 	// Remove a feed
185: 	err := doc.RemoveFeed("https://example.com/feed2")
186: 	if err != nil {
187: 		t.Fatalf("RemoveFeed() error = %v", err)
188: 	}
189: 	// Verify it's removed
190: 	feeds := doc.AllFeeds()
191: 	if len(feeds) != 2 {
192: 		t.Fatalf("AllFeeds() = %d feeds, want 2", len(feeds))
193: 	}
194: 	// Verify the right feed was removed
195: 	for _, feed := range feeds {
196: 		if feed.URL == "https://example.com/feed2" {
197: 			t.Error("Feed 2 should have been removed")
198: 		}
199: 	}
200: 	// Verify Tech folder still has one feed
201: 	techFeeds := doc.FeedsInFolder("Tech")
202: 	if len(techFeeds) != 1 {
203: 		t.Errorf("FeedsInFolder('Tech') = %d feeds, want 1", len(techFeeds))
204: 	}
205: }
206: func TestOPML_AddFolder(t *testing.T) {
207: 	doc := NewDocument("Test Document")
208: 	// Add a folder
209: 	err := doc.AddFolder("New Folder")
210: 	if err != nil {
211: 		t.Fatalf("AddFolder() error = %v", err)
212: 	}
213: 	// Verify folder exists
214: 	folders := doc.Folders()
215: 	found := false
216: 	for _, f := range folders {
217: 		if f == "New Folder" {
218: 			found = true
219: 			break
220: 		}
221: 	}
222: 	if !found {
223: 		t.Error("Folder 'New Folder' not found")
224: 	}
225: 	// Add the same folder again (should be idempotent)
226: 	err = doc.AddFolder("New Folder")
227: 	if err != nil {
228: 		t.Fatalf("AddFolder() second call error = %v", err)
229: 	}
230: 	// Verify still only one instance
231: 	folders = doc.Folders()
232: 	count := 0
233: 	for _, f := range folders {
234: 		if f == "New Folder" {
235: 			count++
236: 		}
237: 	}
238: 	if count != 1 {
239: 		t.Errorf("Found %d instances of 'New Folder', want 1", count)
240: 	}
241: }
242: func TestOPML_Write(t *testing.T) {
243: 	doc := NewDocument("Write Test")
244: 	doc.AddFeed("https://example.com/feed", "Example Feed", "Tech")
245: 	var buf bytes.Buffer
246: 	err := doc.Write(&buf)
247: 	if err != nil {
248: 		t.Fatalf("Write() error = %v", err)
249: 	}
250: 	output := buf.String()
251: 	if output == "" {
252: 		t.Error("Write() produced empty output")
253: 	}
254: 	// Basic validation that it looks like OPML
255: 	if !bytes.Contains(buf.Bytes(), []byte("<?xml")) {
256: 		t.Error("Output missing XML declaration")
257: 	}
258: 	if !bytes.Contains(buf.Bytes(), []byte("<opml")) {
259: 		t.Error("Output missing <opml> tag")
260: 	}
261: 	if !bytes.Contains(buf.Bytes(), []byte("Write Test")) {
262: 		t.Error("Output missing title")
263: 	}
264: 	if !bytes.Contains(buf.Bytes(), []byte("https://example.com/feed")) {
265: 		t.Error("Output missing feed URL")
266: 	}
267: }
268: func TestOPML_MoveFeed(t *testing.T) {
269: 	doc := NewDocument("Move Test")
270: 	// Add feeds to different folders
271: 	doc.AddFeed("https://example.com/feed1", "Feed 1", "Tech")
272: 	doc.AddFeed("https://example.com/feed2", "Feed 2", "News")
273: 	doc.AddFeed("https://example.com/feed3", "Feed 3", "")
274: 	// Verify initial state
275: 	techFeeds := doc.FeedsInFolder("Tech")
276: 	if len(techFeeds) != 1 {
277: 		t.Fatalf("expected 1 feed in Tech, got %d", len(techFeeds))
278: 	}
279: 	newsFeeds := doc.FeedsInFolder("News")
280: 	if len(newsFeeds) != 1 {
281: 		t.Fatalf("expected 1 feed in News, got %d", len(newsFeeds))
282: 	}
283: 	// Move feed from Tech to News
284: 	err := doc.MoveFeed("https://example.com/feed1", "News")
285: 	if err != nil {
286: 		t.Fatalf("MoveFeed() error = %v", err)
287: 	}
288: 	// Verify feed moved
289: 	techFeeds = doc.FeedsInFolder("Tech")
290: 	if len(techFeeds) != 0 {
291: 		t.Errorf("expected 0 feeds in Tech after move, got %d", len(techFeeds))
292: 	}
293: 	newsFeeds = doc.FeedsInFolder("News")
294: 	if len(newsFeeds) != 2 {
295: 		t.Errorf("expected 2 feeds in News after move, got %d", len(newsFeeds))
296: 	}
297: 	// Move feed to root level
298: 	err = doc.MoveFeed("https://example.com/feed2", "")
299: 	if err != nil {
300: 		t.Fatalf("MoveFeed() to root error = %v", err)
301: 	}
302: 	rootFeeds := doc.FeedsInFolder("")
303: 	if len(rootFeeds) != 2 {
304: 		t.Errorf("expected 2 feeds at root level, got %d", len(rootFeeds))
305: 	}
306: 	// Move feed to new folder (should create it)
307: 	err = doc.MoveFeed("https://example.com/feed3", "Sports")
308: 	if err != nil {
309: 		t.Fatalf("MoveFeed() to new folder error = %v", err)
310: 	}
311: 	sportsFeeds := doc.FeedsInFolder("Sports")
312: 	if len(sportsFeeds) != 1 {
313: 		t.Errorf("expected 1 feed in Sports, got %d", len(sportsFeeds))
314: 	}
315: 	// Try to move non-existent feed
316: 	err = doc.MoveFeed("https://example.com/nonexistent", "Tech")
317: 	if err == nil {
318: 		t.Error("expected error when moving non-existent feed")
319: 	}
320: }
</file>

<file path="test/integration_test.go">
  1: // ABOUTME: Integration tests for full RSS feed workflow
  2: // ABOUTME: Tests end-to-end scenarios including fetch, parse, OPML, and caching
  3: package test
  4: import (
  5: 	"path/filepath"
  6: 	"testing"
  7: 	"time"
  8: 	"github.com/harper/digest/internal/db"
  9: 	"github.com/harper/digest/internal/fetch"
 10: 	"github.com/harper/digest/internal/models"
 11: 	"github.com/harper/digest/internal/opml"
 12: 	"github.com/harper/digest/internal/parse"
 13: )
 14: // TestFullWorkflow tests the complete RSS feed workflow from fetch to database
 15: func TestFullWorkflow(t *testing.T) {
 16: 	// Create temp directory for database and OPML
 17: 	tmpDir := t.TempDir()
 18: 	dbPath := filepath.Join(tmpDir, "test.db")
 19: 	opmlPath := filepath.Join(tmpDir, "feeds.opml")
 20: 	// Initialize database
 21: 	database, err := db.InitDB(dbPath)
 22: 	if err != nil {
 23: 		t.Fatalf("failed to initialize database: %v", err)
 24: 	}
 25: 	defer database.Close()
 26: 	// Create OPML document
 27: 	doc := opml.NewDocument("Test Feeds")
 28: 	// Add xkcd feed to database and OPML
 29: 	feedURL := "https://xkcd.com/rss.xml"
 30: 	feed := models.NewFeed(feedURL)
 31: 	feed.Title = stringPtr("XKCD")
 32: 	if err := db.CreateFeed(database, feed); err != nil {
 33: 		t.Fatalf("failed to create feed in database: %v", err)
 34: 	}
 35: 	if err := doc.AddFeed(feedURL, "XKCD", "Comics"); err != nil {
 36: 		t.Fatalf("failed to add feed to OPML: %v", err)
 37: 	}
 38: 	// Write OPML to file
 39: 	if err := doc.WriteFile(opmlPath); err != nil {
 40: 		t.Fatalf("failed to write OPML file: %v", err)
 41: 	}
 42: 	// Fetch the feed
 43: 	t.Logf("Fetching feed from %s", feedURL)
 44: 	result, err := fetch.Fetch(feedURL, nil, nil)
 45: 	if err != nil {
 46: 		t.Fatalf("failed to fetch feed: %v", err)
 47: 	}
 48: 	if result.NotModified {
 49: 		t.Fatal("unexpected NotModified response on initial fetch")
 50: 	}
 51: 	if len(result.Body) == 0 {
 52: 		t.Fatal("fetched feed body is empty")
 53: 	}
 54: 	t.Logf("Fetched %d bytes", len(result.Body))
 55: 	// Parse the feed
 56: 	parsedFeed, err := parse.Parse(result.Body)
 57: 	if err != nil {
 58: 		t.Fatalf("failed to parse feed: %v", err)
 59: 	}
 60: 	if parsedFeed.Title == "" {
 61: 		t.Error("parsed feed has no title")
 62: 	}
 63: 	if len(parsedFeed.Entries) == 0 {
 64: 		t.Fatal("parsed feed has no entries")
 65: 	}
 66: 	t.Logf("Parsed feed: %s with %d entries", parsedFeed.Title, len(parsedFeed.Entries))
 67: 	// Create entries in database
 68: 	entriesCreated := 0
 69: 	for _, parsedEntry := range parsedFeed.Entries {
 70: 		// Check if entry already exists
 71: 		exists, err := db.EntryExists(database, feed.ID, parsedEntry.GUID)
 72: 		if err != nil {
 73: 			t.Fatalf("failed to check entry existence: %v", err)
 74: 		}
 75: 		if exists {
 76: 			continue
 77: 		}
 78: 		// Create new entry
 79: 		entry := models.NewEntry(feed.ID, parsedEntry.GUID, parsedEntry.Title)
 80: 		entry.Link = &parsedEntry.Link
 81: 		entry.Author = &parsedEntry.Author
 82: 		entry.PublishedAt = parsedEntry.PublishedAt
 83: 		entry.Content = &parsedEntry.Content
 84: 		if err := db.CreateEntry(database, entry); err != nil {
 85: 			t.Fatalf("failed to create entry: %v", err)
 86: 		}
 87: 		entriesCreated++
 88: 	}
 89: 	t.Logf("Created %d entries in database", entriesCreated)
 90: 	// Verify database contains entries
 91: 	entries, err := db.ListEntries(database, &feed.ID, nil, nil, nil, nil, nil)
 92: 	if err != nil {
 93: 		t.Fatalf("failed to list entries: %v", err)
 94: 	}
 95: 	if len(entries) != entriesCreated {
 96: 		t.Errorf("expected %d entries, got %d", entriesCreated, len(entries))
 97: 	}
 98: 	// Count unread entries before marking any as read
 99: 	unreadBefore, err := db.CountUnreadEntries(database, nil)
100: 	if err != nil {
101: 		t.Fatalf("failed to count unread entries: %v", err)
102: 	}
103: 	if unreadBefore != entriesCreated {
104: 		t.Errorf("expected %d unread entries, got %d", entriesCreated, unreadBefore)
105: 	}
106: 	// Mark first entry as read
107: 	if len(entries) > 0 {
108: 		firstEntry := entries[0]
109: 		if err := db.MarkEntryRead(database, firstEntry.ID); err != nil {
110: 			t.Fatalf("failed to mark entry as read: %v", err)
111: 		}
112: 		// Verify unread count decreased
113: 		unreadAfter, err := db.CountUnreadEntries(database, nil)
114: 		if err != nil {
115: 			t.Fatalf("failed to count unread entries after marking read: %v", err)
116: 		}
117: 		expectedUnread := unreadBefore - 1
118: 		if unreadAfter != expectedUnread {
119: 			t.Errorf("expected %d unread entries after marking one read, got %d", expectedUnread, unreadAfter)
120: 		}
121: 		// Verify the entry is actually marked as read
122: 		updatedEntry, err := db.GetEntryByID(database, firstEntry.ID)
123: 		if err != nil {
124: 			t.Fatalf("failed to get updated entry: %v", err)
125: 		}
126: 		if !updatedEntry.Read {
127: 			t.Error("entry should be marked as read")
128: 		}
129: 		if updatedEntry.ReadAt == nil {
130: 			t.Error("entry ReadAt timestamp should be set")
131: 		}
132: 	}
133: 	t.Log("Full workflow test completed successfully")
134: }
135: // TestOPMLRoundTrip tests OPML creation, writing, and reading
136: func TestOPMLRoundTrip(t *testing.T) {
137: 	tmpDir := t.TempDir()
138: 	opmlPath := filepath.Join(tmpDir, "test_roundtrip.opml")
139: 	// Create document with folders and feeds
140: 	doc := opml.NewDocument("Test RSS Feeds")
141: 	// Add some folders
142: 	if err := doc.AddFolder("Tech"); err != nil {
143: 		t.Fatalf("failed to add Tech folder: %v", err)
144: 	}
145: 	if err := doc.AddFolder("Comics"); err != nil {
146: 		t.Fatalf("failed to add Comics folder: %v", err)
147: 	}
148: 	// Add feeds to folders
149: 	if err := doc.AddFeed("https://example.com/tech/feed.xml", "Tech Blog", "Tech"); err != nil {
150: 		t.Fatalf("failed to add feed to Tech folder: %v", err)
151: 	}
152: 	if err := doc.AddFeed("https://xkcd.com/rss.xml", "XKCD", "Comics"); err != nil {
153: 		t.Fatalf("failed to add feed to Comics folder: %v", err)
154: 	}
155: 	// Add a root-level feed (no folder)
156: 	if err := doc.AddFeed("https://example.com/root.xml", "Root Feed", ""); err != nil {
157: 		t.Fatalf("failed to add root feed: %v", err)
158: 	}
159: 	// Write to temp file
160: 	if err := doc.WriteFile(opmlPath); err != nil {
161: 		t.Fatalf("failed to write OPML file: %v", err)
162: 	}
163: 	// Parse back from file
164: 	loadedDoc, err := opml.ParseFile(opmlPath)
165: 	if err != nil {
166: 		t.Fatalf("failed to parse OPML file: %v", err)
167: 	}
168: 	// Verify document title
169: 	if loadedDoc.Title != "Test RSS Feeds" {
170: 		t.Errorf("expected title 'Test RSS Feeds', got %s", loadedDoc.Title)
171: 	}
172: 	// Verify folders
173: 	folders := loadedDoc.Folders()
174: 	if len(folders) != 2 {
175: 		t.Errorf("expected 2 folders, got %d", len(folders))
176: 	}
177: 	// Verify all feeds preserved
178: 	allFeeds := loadedDoc.AllFeeds()
179: 	if len(allFeeds) != 3 {
180: 		t.Errorf("expected 3 feeds, got %d", len(allFeeds))
181: 	}
182: 	// Verify feeds in specific folder
183: 	techFeeds := loadedDoc.FeedsInFolder("Tech")
184: 	if len(techFeeds) != 1 {
185: 		t.Errorf("expected 1 feed in Tech folder, got %d", len(techFeeds))
186: 	}
187: 	if len(techFeeds) > 0 && techFeeds[0].URL != "https://example.com/tech/feed.xml" {
188: 		t.Errorf("unexpected feed URL in Tech folder: %s", techFeeds[0].URL)
189: 	}
190: 	// Verify root-level feeds
191: 	rootFeeds := loadedDoc.FeedsInFolder("")
192: 	if len(rootFeeds) != 1 {
193: 		t.Errorf("expected 1 root-level feed, got %d", len(rootFeeds))
194: 	}
195: 	t.Log("OPML round-trip test completed successfully")
196: }
197: // TestCachedSync tests HTTP caching with ETag and conditional requests
198: func TestCachedSync(t *testing.T) {
199: 	feedURL := "https://xkcd.com/rss.xml"
200: 	// First fetch without cache headers
201: 	t.Logf("Fetching %s for the first time", feedURL)
202: 	result1, err := fetch.Fetch(feedURL, nil, nil)
203: 	if err != nil {
204: 		t.Fatalf("first fetch failed: %v", err)
205: 	}
206: 	if result1.NotModified {
207: 		t.Error("first fetch should not be NotModified")
208: 	}
209: 	t.Logf("First fetch: %d bytes, ETag=%q, Last-Modified=%q",
210: 		len(result1.Body), result1.ETag, result1.LastModified)
211: 	// Check if feed supports caching
212: 	if result1.ETag == "" && result1.LastModified == "" {
213: 		t.Log("Feed does not support ETag or Last-Modified headers (caching not available)")
214: 		t.Log("This is informational - the feed server doesn't provide caching headers")
215: 		return
216: 	}
217: 	// Wait a moment to ensure we're not hitting rate limits
218: 	time.Sleep(2 * time.Second)
219: 	// Second fetch with cache headers
220: 	t.Log("Fetching again with cache headers")
221: 	var etag *string
222: 	var lastModified *string
223: 	if result1.ETag != "" {
224: 		etag = &result1.ETag
225: 	}
226: 	if result1.LastModified != "" {
227: 		lastModified = &result1.LastModified
228: 	}
229: 	result2, err := fetch.Fetch(feedURL, etag, lastModified)
230: 	if err != nil {
231: 		t.Fatalf("second fetch failed: %v", err)
232: 	}
233: 	// The server should return 304 Not Modified if content hasn't changed
234: 	// However, this depends on the server behavior and timing
235: 	if result2.NotModified {
236: 		t.Log("Feed returned 304 Not Modified - caching working as expected")
237: 	} else {
238: 		t.Logf("Feed returned fresh content (not cached): %d bytes", len(result2.Body))
239: 		t.Log("This might indicate the feed was updated between requests")
240: 		// Verify we got valid content
241: 		if len(result2.Body) == 0 {
242: 			t.Error("second fetch should return body if not cached")
243: 		}
244: 	}
245: 	t.Log("Cached sync test completed successfully")
246: }
247: // Helper function to create string pointer
248: func stringPtr(s string) *string {
249: 	return &s
250: }
</file>

<file path=".golangci.yml">
 1: # ABOUTME: golangci-lint v2 configuration for digest project
 2: # ABOUTME: Enables linters for bugs, performance, security, style, and maintainability
 3: version: "2"
 4: run:
 5:   tests: true
 6: linters:
 7:   enable:
 8:     - bodyclose
 9:     - contextcheck
10:     - copyloopvar
11:     - errchkjson
12:     - gocritic
13:     - gocyclo
14:     - misspell
15:     - nilerr
16:     - nilnil
17:     - prealloc
18:     - unconvert
19:     - unparam
20:     - wastedassign
21:     - whitespace
22:   disable:
23:     - errcheck
24:     - errorlint
25:     - exhaustive
26:     - exhaustruct
27:     - funlen
28:     - gocognit
29:     - goconst
30:     - gosec
31:     - makezero
32:     - nestif
33:     - paralleltest
34:     - tagliatelle
35:     - testpackage
36:     - thelper
37:     - varnamelen
38:     - wrapcheck
39:   settings:
40:     gocyclo:
41:       min-complexity: 20
42:     govet:
43:       disable:
44:         - shadow
45:         - fieldalignment
46:       enable-all: true
47:     misspell:
48:       locale: US
49:     staticcheck:
50:       checks:
51:         - all
52:         - -ST1000  # Package comments not required (we use ABOUTME comments)
53:   exclusions:
54:     generated: lax
55:     rules:
56:       - linters:
57:           - gocritic
58:           - gocyclo
59:         path: _test\.go
60:     paths:
61:       - third_party$
62:       - builtin$
63:       - examples$
64: issues:
65:   max-issues-per-linter: 0
66:   max-same-issues: 0
67: formatters:
68:   exclusions:
69:     generated: lax
70:     paths:
71:       - third_party$
72:       - builtin$
73:       - examples$
</file>

<file path=".goreleaser.yml">
 1: # ABOUTME: GoReleaser configuration for digest RSS/Atom feed tracker
 2: # ABOUTME: Builds for macOS due to CGO requirements for SQLite
 3: version: 2
 4: before:
 5:   hooks:
 6:     - go mod tidy
 7:     - go test ./...
 8: builds:
 9:   - id: digest
10:     binary: digest
11:     main: ./cmd/digest
12:     env:
13:       - CGO_ENABLED=1
14:     # Build for macOS only due to CGO requirements for SQLite
15:     # Cross-compilation to Linux with CGO is complex and requires Docker
16:     # For Linux builds, users can build from source or we can add Docker-based builds later
17:     goos:
18:       - darwin
19:     goarch:
20:       - amd64
21:       - arm64
22:     ldflags:
23:       - -s -w
24:       - -X main.Version={{.Version}}
25:       - -X main.Commit={{.Commit}}
26:       - -X main.BuildDate={{.Date}}
27:     tags:
28:       - sqlite_omit_load_extension
29: archives:
30:   - id: digest
31:     formats: [tar.gz]
32:     name_template: >-
33:       {{ .ProjectName }}_
34:       {{- .Version }}_
35:       {{- title .Os }}_
36:       {{- if eq .Arch "amd64" }}x86_64
37:       {{- else if eq .Arch "386" }}i386
38:       {{- else }}{{ .Arch }}{{ end }}
39:     format_overrides:
40:       - goos: windows
41:         formats: [zip]
42:     files:
43:       - LICENSE*
44: checksum:
45:   name_template: 'checksums.txt'
46: snapshot:
47:   version_template: "{{ incpatch .Version }}-next"
48: changelog:
49:   sort: asc
50:   use: github
51:   filters:
52:     exclude:
53:       - '^docs:'
54:       - '^test:'
55:       - '^chore:'
56:       - 'merge conflict'
57:       - Merge pull request
58:       - Merge remote-tracking branch
59:       - Merge branch
60:   groups:
61:     - title: 'Features'
62:       regexp: "^.*feat[(\\w)]*:+.*$"
63:       order: 0
64:     - title: 'Bug fixes'
65:       regexp: "^.*fix[(\\w)]*:+.*$"
66:       order: 1
67:     - title: 'Other'
68:       order: 999
69: brews:
70:   - repository:
71:       owner: harperreed
72:       name: homebrew-tap
73:       token: "{{ .Env.HOMEBREW_TAP_TOKEN }}"
74:     directory: Formula
75:     homepage: https://github.com/harperreed/digest
76:     description: "RSS/Atom feed tracker for humans and AI agents"
77:     license: MIT
78:     commit_author:
79:       name: goreleaserbot
80:       email: bot@goreleaser.com
81:     install: |
82:       bin.install "digest"
83:     test: |
84:       system "#{bin}/digest", "version"
</file>

<file path="README.md">
  1: # Digest
  2:
  3: A fast, lightweight RSS/Atom feed reader with both CLI and MCP (Model Context Protocol) server interfaces.
  4:
  5: ## Features
  6:
  7: ### Feed Management
  8: - **Add feeds** with optional folder/category organization
  9: - **Remove feeds** (cascades to delete all entries)
 10: - **Move feeds** between folders for reorganization
 11: - **Auto-discover** feed URLs from website URLs
 12: - **OPML import/export** for feed subscriptions
 13:
 14: ### Entry Tracking
 15: - **Sync feeds** with HTTP caching (ETag, Last-Modified) for efficiency
 16: - **List entries** with filtering by feed, category, read status, and date
 17: - **Smart date filters**: `today`, `yesterday`, `week`, `month`
 18: - **Read articles** with beautiful markdown rendering (HTML auto-converted)
 19: - **Mark as read/unread** - individual entries or bulk by date
 20:
 21: ### MCP Server
 22: Full MCP integration for AI agents to manage feeds:
 23:
 24: | Tool | Description |
 25: |------|-------------|
 26: | `list_feeds` | List all subscribed feeds with metadata |
 27: | `add_feed` | Add a new feed with optional folder |
 28: | `remove_feed` | Remove a feed and all its entries |
 29: | `move_feed` | Move a feed to a different folder |
 30: | `sync_feeds` | Fetch new entries from feeds |
 31: | `list_entries` | List entries with date/read filters |
 32: | `get_entry` | Get full article content as markdown |
 33: | `mark_read` | Mark an entry as read |
 34: | `mark_unread` | Mark an entry as unread |
 35: | `bulk_mark_read` | Mark all entries before a date as read |
 36:
 37: ### MCP Resources
 38: | Resource | Description |
 39: |----------|-------------|
 40: | `digest://feeds` | All subscribed feeds |
 41: | `digest://entries/unread` | Unread entries |
 42: | `digest://entries/today` | Today's entries |
 43: | `digest://stats` | Feed statistics |
 44:
 45: ### MCP Prompts
 46: Workflow templates for common RSS management tasks:
 47:
 48: | Prompt | Description |
 49: |--------|-------------|
 50: | `daily-digest` | Morning routine to summarize today's entries, prioritize content, and generate a digest |
 51: | `catch-up` | Efficiently process backlog after time away - triage, prioritize, declare bankruptcy on low-value feeds |
 52: | `curate-feeds` | Quarterly review to remove low-value feeds, identify gaps, and optimize subscriptions |
 53:
 54: **Example prompt usage:**
 55: ```
 56: # Morning digest
 57: Use the daily-digest prompt to catch up on today's news
 58:
 59: # After vacation
 60: Use the catch-up prompt with days=14 to process two weeks of entries
 61:
 62: # Feed cleanup
 63: Use the curate-feeds prompt to optimize my subscriptions
 64: ```
 65:
 66: ## Installation
 67:
 68: ```bash
 69: # From source
 70: go install github.com/harper/digest/cmd/digest@latest
 71:
 72: # Or clone and build
 73: git clone https://github.com/harperreed/digest.git
 74: cd digest
 75: make build
 76: ```
 77:
 78: ## CLI Usage
 79:
 80: ```bash
 81: # Add a feed
 82: digest feed add https://example.com/feed.xml
 83: digest feed add https://example.com/feed.xml --folder "Tech"
 84:
 85: # Discover feed from website
 86: digest feed discover https://example.com
 87:
 88: # List feeds
 89: digest feed list
 90:
 91: # Move feed to category
 92: digest feed move https://example.com/feed.xml "News"
 93:
 94: # Sync all feeds
 95: digest sync
 96:
 97: # List entries
 98: digest list                    # All unread entries
 99: digest list --all              # Include read entries
100: digest list --today            # Today's entries
101: digest list --category "Tech"  # Entries from Tech folder
102:
103: # Read an article (supports ID prefix)
104: digest read abc12345
105:
106: # Mark as read
107: digest mark-read abc12345              # Single entry
108: digest mark-read --before yesterday    # Bulk mark
109: ```
110:
111: ## MCP Server Usage
112:
113: ### Configuration
114:
115: Add to your Claude Desktop config (`~/Library/Application Support/Claude/claude_desktop_config.json`):
116:
117: ```json
118: {
119:   "mcpServers": {
120:     "digest": {
121:       "command": "/path/to/digest",
122:       "args": ["mcp"]
123:     }
124:   }
125: }
126: ```
127:
128: ### Example Agent Workflows
129:
130: ```
131: # Get today's news
132: list_entries { "since": "today", "unread_only": true }
133:
134: # Read an article
135: get_entry { "entry_id": "abc12345" }
136:
137: # Organize feeds
138: move_feed { "url": "https://example.com/feed", "folder": "Tech Blogs" }
139:
140: # Catch up on old articles
141: bulk_mark_read { "before": "week" }
142: ```
143:
144: ## Data Storage
145:
146: - **Database**: `~/.config/digest/digest.db` (SQLite)
147: - **Subscriptions**: `~/.config/digest/feeds.opml` (OPML)
148:
149: ## Development
150:
151: ```bash
152: # Run tests
153: make test
154:
155: # Run linter
156: make lint
157:
158: # Build
159: make build
160:
161: # Run MCP server
162: ./digest mcp
163: ```
164:
165: ## License
166:
167: MIT
</file>

<file path="cmd/digest/list.go">
  1: // ABOUTME: List command for viewing feed entries with filtering options
  2: // ABOUTME: Displays entries with read status, title, and published date using color formatting
  3: package main
  4: import (
  5: 	"fmt"
  6: 	"time"
  7: 	"github.com/fatih/color"
  8: 	"github.com/spf13/cobra"
  9: 	"github.com/harper/digest/internal/db"
 10: 	"github.com/harper/digest/internal/timeutil"
 11: )
 12: var listCmd = &cobra.Command{
 13: 	Use:     "list",
 14: 	Aliases: []string{"ls", "l"},
 15: 	Short:   "List feed entries",
 16: 	Long:    "List feed entries with optional filtering by feed and read status",
 17: 	RunE: func(cmd *cobra.Command, args []string) error {
 18: 		all, _ := cmd.Flags().GetBool("all")
 19: 		feedFilter, _ := cmd.Flags().GetString("feed")
 20: 		category, _ := cmd.Flags().GetString("category")
 21: 		limit, _ := cmd.Flags().GetInt("limit")
 22: 		today, _ := cmd.Flags().GetBool("today")
 23: 		yesterday, _ := cmd.Flags().GetBool("yesterday")
 24: 		week, _ := cmd.Flags().GetBool("week")
 25: 		// Get feedID if --feed is specified, or feedIDs if --category is specified
 26: 		var feedID *string
 27: 		var feedIDs []string
 28: 		if feedFilter != "" && category != "" {
 29: 			return fmt.Errorf("cannot use --feed and --category together")
 30: 		}
 31: 		if feedFilter != "" {
 32: 			// Try exact URL match first
 33: 			feed, err := db.GetFeedByURL(dbConn, feedFilter)
 34: 			if err != nil {
 35: 				// Try prefix match
 36: 				feed, err = db.GetFeedByPrefix(dbConn, feedFilter)
 37: 				if err != nil {
 38: 					return fmt.Errorf("failed to find feed: %w", err)
 39: 				}
 40: 			}
 41: 			feedID = &feed.ID
 42: 		}
 43: 		if category != "" {
 44: 			// Get all feeds in this category from OPML
 45: 			categoryFeeds := opmlDoc.FeedsInFolder(category)
 46: 			if len(categoryFeeds) == 0 {
 47: 				return fmt.Errorf("no feeds found in category %q", category)
 48: 			}
 49: 			// Get feed IDs from database
 50: 			for _, opmlFeed := range categoryFeeds {
 51: 				dbFeed, err := db.GetFeedByURL(dbConn, opmlFeed.URL)
 52: 				if err != nil {
 53: 					continue // Skip feeds not in database
 54: 				}
 55: 				feedIDs = append(feedIDs, dbFeed.ID)
 56: 			}
 57: 			if len(feedIDs) == 0 {
 58: 				return fmt.Errorf("no synced feeds found in category %q", category)
 59: 			}
 60: 		}
 61: 		// Set unreadOnly based on --all flag
 62: 		unreadOnly := !all
 63: 		// Calculate date filters based on smart view flags
 64: 		var since, until *time.Time
 65: 		if today {
 66: 			s := timeutil.StartOfToday()
 67: 			since = &s
 68: 		} else if yesterday {
 69: 			s := timeutil.StartOfYesterday()
 70: 			u := timeutil.EndOfYesterday()
 71: 			since = &s
 72: 			until = &u
 73: 		} else if week {
 74: 			s := timeutil.StartOfWeek()
 75: 			since = &s
 76: 		}
 77: 		// List entries
 78: 		entries, err := db.ListEntries(dbConn, feedID, feedIDs, &unreadOnly, since, until, &limit)
 79: 		if err != nil {
 80: 			return fmt.Errorf("failed to list entries: %w", err)
 81: 		}
 82: 		if len(entries) == 0 {
 83: 			fmt.Println("No entries found")
 84: 			return nil
 85: 		}
 86: 		// Create color functions
 87: 		faint := color.New(color.Faint).SprintFunc()
 88: 		// Display entries
 89: 		for _, entry := range entries {
 90: 			// ID (first 8 chars, faint) - with bounds check for safety
 91: 			idShort := entry.ID
 92: 			if len(idShort) > 8 {
 93: 				idShort = idShort[:8]
 94: 			}
 95: 			fmt.Print(faint(idShort))
 96: 			fmt.Print(" ")
 97: 			// Read status (checkmark or space)
 98: 			if entry.Read {
 99: 				fmt.Print("✓ ")
100: 			} else {
101: 				fmt.Print("  ")
102: 			}
103: 			// Title
104: 			title := "Untitled"
105: 			if entry.Title != nil {
106: 				title = *entry.Title
107: 			}
108: 			fmt.Print(title)
109: 			// Published date (RFC822 format, faint)
110: 			if entry.PublishedAt != nil {
111: 				dateStr := entry.PublishedAt.Format("02 Jan 06 15:04 MST")
112: 				fmt.Print(" ")
113: 				fmt.Print(faint(dateStr))
114: 			}
115: 			fmt.Println()
116: 		}
117: 		return nil
118: 	},
119: }
120: func init() {
121: 	rootCmd.AddCommand(listCmd)
122: 	listCmd.Flags().BoolP("all", "a", false, "show all entries including read")
123: 	listCmd.Flags().StringP("feed", "f", "", "filter by feed URL or prefix")
124: 	listCmd.Flags().StringP("category", "c", "", "filter by feed category/folder")
125: 	listCmd.Flags().IntP("limit", "n", 20, "max entries to show")
126: 	listCmd.Flags().Bool("today", false, "show only today's entries")
127: 	listCmd.Flags().Bool("yesterday", false, "show only yesterday's entries")
128: 	listCmd.Flags().Bool("week", false, "show only this week's entries")
129: 	listCmd.MarkFlagsMutuallyExclusive("today", "yesterday", "week")
130: 	listCmd.MarkFlagsMutuallyExclusive("feed", "category")
131: }
</file>

<file path="cmd/digest/read.go">
  1: // ABOUTME: Read command for viewing article content
  2: // ABOUTME: Displays full article details with markdown rendering and marks as read
  3: package main
  4: import (
  5: 	"database/sql"
  6: 	"errors"
  7: 	"fmt"
  8: 	"strings"
  9: 	"github.com/charmbracelet/glamour"
 10: 	"github.com/fatih/color"
 11: 	"github.com/spf13/cobra"
 12: 	"github.com/harper/digest/internal/content"
 13: 	"github.com/harper/digest/internal/db"
 14: )
 15: var readCmd = &cobra.Command{
 16: 	Use:   "read <entry-id>",
 17: 	Short: "Read an article",
 18: 	Long:  "Display the full content of an article and mark it as read",
 19: 	Args:  cobra.ExactArgs(1),
 20: 	RunE: func(cmd *cobra.Command, args []string) error {
 21: 		entryRef := args[0]
 22: 		noMark, _ := cmd.Flags().GetBool("no-mark")
 23: 		// Get entry by ID or prefix
 24: 		entry, err := db.GetEntryByID(dbConn, entryRef)
 25: 		if err != nil {
 26: 			// Only try prefix match if entry was not found (not for other DB errors)
 27: 			if !errors.Is(err, sql.ErrNoRows) {
 28: 				return fmt.Errorf("failed to get entry: %w", err)
 29: 			}
 30: 			entry, err = db.GetEntryByPrefix(dbConn, entryRef)
 31: 			if err != nil {
 32: 				return fmt.Errorf("entry not found: %s", entryRef)
 33: 			}
 34: 		}
 35: 		// Get feed for context
 36: 		feed, err := db.GetFeedByID(dbConn, entry.FeedID)
 37: 		if err != nil {
 38: 			return fmt.Errorf("failed to get feed: %w", err)
 39: 		}
 40: 		// Color helpers
 41: 		bold := color.New(color.Bold).SprintFunc()
 42: 		faint := color.New(color.Faint).SprintFunc()
 43: 		cyan := color.New(color.FgCyan).SprintFunc()
 44: 		// Display article header
 45: 		fmt.Println(strings.Repeat("─", 60))
 46: 		// Title
 47: 		title := "Untitled"
 48: 		if entry.Title != nil {
 49: 			title = *entry.Title
 50: 		}
 51: 		fmt.Printf("%s\n\n", bold(title))
 52: 		// Feed
 53: 		feedTitle := feed.URL
 54: 		if feed.Title != nil {
 55: 			feedTitle = *feed.Title
 56: 		}
 57: 		fmt.Printf("%s %s\n", faint("Feed:"), feedTitle)
 58: 		// Author
 59: 		if entry.Author != nil && *entry.Author != "" {
 60: 			fmt.Printf("%s %s\n", faint("Author:"), *entry.Author)
 61: 		}
 62: 		// Published date
 63: 		if entry.PublishedAt != nil {
 64: 			fmt.Printf("%s %s\n", faint("Published:"), entry.PublishedAt.Format("Mon, 02 Jan 2006 15:04 MST"))
 65: 		}
 66: 		// Link
 67: 		if entry.Link != nil {
 68: 			fmt.Printf("%s %s\n", faint("Link:"), cyan(*entry.Link))
 69: 		}
 70: 		fmt.Println(strings.Repeat("─", 60))
 71: 		// Content
 72: 		if entry.Content != nil && *entry.Content != "" {
 73: 			// Convert HTML to markdown if needed
 74: 			markdown := content.ToMarkdown(*entry.Content)
 75: 			// Render with glamour for terminal display
 76: 			rendered, err := glamour.Render(markdown, "dark")
 77: 			if err != nil {
 78: 				// Fall back to plain markdown if rendering fails
 79: 				fmt.Printf("%s\n", faint("(markdown rendering unavailable, showing plain text)"))
 80: 				fmt.Printf("\n%s\n", markdown)
 81: 			} else {
 82: 				fmt.Print(rendered)
 83: 			}
 84: 		} else {
 85: 			fmt.Println("\n(No content available)")
 86: 		}
 87: 		fmt.Println()
 88: 		// Mark as read unless --no-mark flag is set
 89: 		if !noMark && !entry.Read {
 90: 			if err := db.MarkEntryRead(dbConn, entry.ID); err != nil {
 91: 				return fmt.Errorf("failed to mark entry as read: %w", err)
 92: 			}
 93: 			fmt.Printf("%s\n", faint("Marked as read"))
 94: 		}
 95: 		return nil
 96: 	},
 97: }
 98: func init() {
 99: 	rootCmd.AddCommand(readCmd)
100: 	readCmd.Flags().Bool("no-mark", false, "don't mark the article as read")
101: }
</file>

<file path="cmd/digest/sync.go">
  1: // ABOUTME: Sync command to fetch new entries from RSS/Atom feeds with HTTP caching support
  2: // ABOUTME: Handles batch syncing of all feeds or individual feed sync with colored progress output
  3: package main
  4: import (
  5: 	"fmt"
  6: 	"time"
  7: 	"github.com/fatih/color"
  8: 	"github.com/spf13/cobra"
  9: 	"github.com/harper/digest/internal/db"
 10: 	"github.com/harper/digest/internal/fetch"
 11: 	"github.com/harper/digest/internal/models"
 12: 	"github.com/harper/digest/internal/parse"
 13: )
 14: var syncCmd = &cobra.Command{
 15: 	Use:   "sync [url]",
 16: 	Short: "Fetch new entries from feeds",
 17: 	Long: `Fetch new entries from all subscribed feeds or a specific feed by URL.
 18: Uses HTTP caching headers (ETag, Last-Modified) to avoid re-fetching unchanged content.
 19: Use --force to ignore cache headers and fetch unconditionally.`,
 20: 	Args: cobra.MaximumNArgs(1),
 21: 	RunE: func(cmd *cobra.Command, args []string) error {
 22: 		force, _ := cmd.Flags().GetBool("force")
 23: 		// Get all feeds from database
 24: 		feeds, err := db.ListFeeds(dbConn)
 25: 		if err != nil {
 26: 			return fmt.Errorf("failed to list feeds: %w", err)
 27: 		}
 28: 		if len(feeds) == 0 {
 29: 			fmt.Println("No feeds found. Add a feed with 'digest feed add <url>'")
 30: 			return nil
 31: 		}
 32: 		// Filter to specific URL if provided
 33: 		if len(args) == 1 {
 34: 			targetURL := args[0]
 35: 			filtered := []*models.Feed{}
 36: 			for _, feed := range feeds {
 37: 				if feed.URL == targetURL {
 38: 					filtered = append(filtered, feed)
 39: 					break
 40: 				}
 41: 			}
 42: 			if len(filtered) == 0 {
 43: 				return fmt.Errorf("feed not found: %s", targetURL)
 44: 			}
 45: 			feeds = filtered
 46: 		}
 47: 		// Sync each feed
 48: 		totalNew := 0
 49: 		totalCached := 0
 50: 		totalErrors := 0
 51: 		green := color.New(color.FgGreen).SprintFunc()
 52: 		red := color.New(color.FgRed).SprintFunc()
 53: 		faint := color.New(color.Faint).SprintFunc()
 54: 		for _, feed := range feeds {
 55: 			displayName := feedDisplayName(feed)
 56: 			fmt.Printf("Syncing %s... ", displayName)
 57: 			newCount, wasCached, err := syncFeed(feed, force)
 58: 			if err != nil {
 59: 				fmt.Printf("%s %s\n", red("✗"), err.Error())
 60: 				totalErrors++
 61: 				continue
 62: 			}
 63: 			if wasCached {
 64: 				fmt.Printf("%s (cached)\n", faint("—"))
 65: 				totalCached++
 66: 			} else if newCount > 0 {
 67: 				fmt.Printf("%s %d new\n", green("✓"), newCount)
 68: 				totalNew += newCount
 69: 			} else {
 70: 				fmt.Printf("%s no new entries\n", green("✓"))
 71: 			}
 72: 		}
 73: 		// Print summary
 74: 		fmt.Println()
 75: 		fmt.Printf("Summary: %d feed(s) synced\n", len(feeds))
 76: 		if totalNew > 0 {
 77: 			fmt.Printf("  %s %d new entries\n", green("✓"), totalNew)
 78: 		}
 79: 		if totalCached > 0 {
 80: 			fmt.Printf("  %s %d cached (not modified)\n", faint("—"), totalCached)
 81: 		}
 82: 		if totalErrors > 0 {
 83: 			fmt.Printf("  %s %d errors\n", red("✗"), totalErrors)
 84: 		}
 85: 		return nil
 86: 	},
 87: }
 88: // syncFeed fetches and processes a single feed, returning the count of new entries
 89: func syncFeed(feed *models.Feed, force bool) (newCount int, wasCached bool, err error) {
 90: 	// Get cache headers from feed (skip if force)
 91: 	var etag, lastModified *string
 92: 	if !force {
 93: 		etag = feed.ETag
 94: 		lastModified = feed.LastModified
 95: 	}
 96: 	// Fetch the feed
 97: 	result, err := fetch.Fetch(feed.URL, etag, lastModified)
 98: 	if err != nil {
 99: 		// Update error state in database
100: 		if updateErr := db.UpdateFeedError(dbConn, feed.ID, err.Error()); updateErr != nil {
101: 			return 0, false, fmt.Errorf("fetch failed (%v) and error update failed: %w", err, updateErr)
102: 		}
103: 		return 0, false, err
104: 	}
105: 	// Handle 304 Not Modified
106: 	if result.NotModified {
107: 		return 0, true, nil
108: 	}
109: 	// Parse the feed
110: 	parsed, err := parse.Parse(result.Body)
111: 	if err != nil {
112: 		errMsg := fmt.Sprintf("failed to parse feed: %v", err)
113: 		if updateErr := db.UpdateFeedError(dbConn, feed.ID, errMsg); updateErr != nil {
114: 			return 0, false, fmt.Errorf("parse failed (%v) and error update failed: %w", err, updateErr)
115: 		}
116: 		return 0, false, fmt.Errorf("failed to parse feed: %w", err)
117: 	}
118: 	// Update feed title if empty and persist to database
119: 	titleUpdated := false
120: 	if feed.Title == nil || *feed.Title == "" {
121: 		feed.Title = &parsed.Title
122: 		titleUpdated = true
123: 	}
124: 	// Process entries
125: 	newCount = 0
126: 	for _, parsedEntry := range parsed.Entries {
127: 		// Check if entry already exists
128: 		exists, err := db.EntryExists(dbConn, feed.ID, parsedEntry.GUID)
129: 		if err != nil {
130: 			return newCount, false, fmt.Errorf("failed to check entry existence: %w", err)
131: 		}
132: 		if exists {
133: 			continue
134: 		}
135: 		// Create new entry
136: 		entry := models.NewEntry(feed.ID, parsedEntry.GUID, parsedEntry.Title)
137: 		entry.Link = &parsedEntry.Link
138: 		entry.Author = &parsedEntry.Author
139: 		entry.PublishedAt = parsedEntry.PublishedAt
140: 		entry.Content = &parsedEntry.Content
141: 		if err := db.CreateEntry(dbConn, entry); err != nil {
142: 			return newCount, false, fmt.Errorf("failed to create entry: %w", err)
143: 		}
144: 		newCount++
145: 	}
146: 	// Update feed fetch state
147: 	fetchedAt := time.Now()
148: 	if err := db.UpdateFeedFetchState(dbConn, feed.ID, &result.ETag, &result.LastModified, fetchedAt); err != nil {
149: 		return newCount, false, fmt.Errorf("failed to update feed state: %w", err)
150: 	}
151: 	// If title was updated, persist to database
152: 	if titleUpdated {
153: 		if err := db.UpdateFeed(dbConn, feed); err != nil {
154: 			return newCount, false, fmt.Errorf("failed to update feed title: %w", err)
155: 		}
156: 	}
157: 	return newCount, false, nil
158: }
159: // feedDisplayName returns a human-readable name for the feed
160: func feedDisplayName(feed *models.Feed) string {
161: 	if feed.Title != nil && *feed.Title != "" {
162: 		return *feed.Title
163: 	}
164: 	return feed.URL
165: }
166: func init() {
167: 	rootCmd.AddCommand(syncCmd)
168: 	syncCmd.Flags().BoolP("force", "f", false, "ignore cache headers and force fetch")
169: }
</file>

<file path="internal/mcp/resources.go">
  1: // ABOUTME: MCP resource providers for digest
  2: // ABOUTME: Exposes read-only views of feeds, entries, and statistics
  3: package mcp
  4: import (
  5: 	"context"
  6: 	"encoding/json"
  7: 	"fmt"
  8: 	"time"
  9: 	"github.com/harper/digest/internal/db"
 10: 	"github.com/harper/digest/internal/timeutil"
 11: 	"github.com/mark3labs/mcp-go/mcp"
 12: )
 13: // ResourceData is the standard response format for all resources.
 14: type ResourceData struct {
 15: 	Metadata ResourceMetadata  `json:"metadata"`
 16: 	Data     interface{}       `json:"data"`
 17: 	Links    map[string]string `json:"links"`
 18: }
 19: // ResourceMetadata contains metadata about the resource response.
 20: type ResourceMetadata struct {
 21: 	Timestamp   time.Time      `json:"timestamp"`
 22: 	Count       int            `json:"count"`
 23: 	ResourceURI string         `json:"resource_uri"`
 24: 	Filters     map[string]any `json:"filters,omitempty"`
 25: }
 26: func (s *Server) registerResources() {
 27: 	// Feed resources
 28: 	s.registerFeedsResource()
 29: 	// Entry resources
 30: 	s.registerEntriesUnreadResource()
 31: 	s.registerEntriesTodayResource()
 32: 	// Statistics resource
 33: 	s.registerStatsResource()
 34: }
 35: func (s *Server) registerFeedsResource() {
 36: 	s.mcpServer.AddResource(
 37: 		mcp.Resource{
 38: 			URI:         "digest://feeds",
 39: 			Name:        "All Feeds",
 40: 			Description: "List all subscribed RSS/Atom feeds with metadata including title, URL, last fetch time, and error status",
 41: 			MIMEType:    "application/json",
 42: 		},
 43: 		func(ctx context.Context, request mcp.ReadResourceRequest) ([]mcp.ResourceContents, error) {
 44: 			feeds, err := db.ListFeeds(s.db)
 45: 			if err != nil {
 46: 				return nil, fmt.Errorf("failed to list feeds: %w", err)
 47: 			}
 48: 			// Convert to output format
 49: 			feedOutputs := make([]map[string]interface{}, 0, len(feeds))
 50: 			for _, feed := range feeds {
 51: 				output := map[string]interface{}{
 52: 					"id":          feed.ID,
 53: 					"url":         feed.URL,
 54: 					"created_at":  feed.CreatedAt,
 55: 					"error_count": feed.ErrorCount,
 56: 				}
 57: 				if feed.Title != nil {
 58: 					output["title"] = *feed.Title
 59: 				}
 60: 				if feed.ETag != nil {
 61: 					output["etag"] = *feed.ETag
 62: 				}
 63: 				if feed.LastModified != nil {
 64: 					output["last_modified"] = *feed.LastModified
 65: 				}
 66: 				if feed.LastFetchedAt != nil {
 67: 					output["last_fetched_at"] = *feed.LastFetchedAt
 68: 				}
 69: 				if feed.LastError != nil {
 70: 					output["last_error"] = *feed.LastError
 71: 				}
 72: 				feedOutputs = append(feedOutputs, output)
 73: 			}
 74: 			resourceData := ResourceData{
 75: 				Metadata: ResourceMetadata{
 76: 					Timestamp:   time.Now(),
 77: 					Count:       len(feedOutputs),
 78: 					ResourceURI: "digest://feeds",
 79: 				},
 80: 				Data: feedOutputs,
 81: 				Links: map[string]string{
 82: 					"unread_entries": "digest://entries/unread",
 83: 					"today_entries":  "digest://entries/today",
 84: 					"stats":          "digest://stats",
 85: 				},
 86: 			}
 87: 			jsonBytes, err := json.MarshalIndent(resourceData, "", "  ")
 88: 			if err != nil {
 89: 				return nil, fmt.Errorf("failed to marshal resource data: %w", err)
 90: 			}
 91: 			return []mcp.ResourceContents{
 92: 				&mcp.TextResourceContents{
 93: 					URI:      request.Params.URI,
 94: 					MIMEType: "application/json",
 95: 					Text:     string(jsonBytes),
 96: 				},
 97: 			}, nil
 98: 		},
 99: 	)
100: }
101: func (s *Server) registerEntriesUnreadResource() {
102: 	s.mcpServer.AddResource(
103: 		mcp.Resource{
104: 			URI:         "digest://entries/unread",
105: 			Name:        "Unread Entries",
106: 			Description: "List all unread feed entries across all subscribed feeds, sorted by published date (newest first)",
107: 			MIMEType:    "application/json",
108: 		},
109: 		func(ctx context.Context, request mcp.ReadResourceRequest) ([]mcp.ResourceContents, error) {
110: 			unreadOnly := true
111: 			entries, err := db.ListEntries(s.db, nil, nil, &unreadOnly, nil, nil, nil)
112: 			if err != nil {
113: 				return nil, fmt.Errorf("failed to list unread entries: %w", err)
114: 			}
115: 			// Convert to output format
116: 			entryOutputs := make([]map[string]interface{}, 0, len(entries))
117: 			for _, entry := range entries {
118: 				output := map[string]interface{}{
119: 					"id":         entry.ID,
120: 					"feed_id":    entry.FeedID,
121: 					"guid":       entry.GUID,
122: 					"read":       entry.Read,
123: 					"created_at": entry.CreatedAt,
124: 				}
125: 				if entry.Title != nil {
126: 					output["title"] = *entry.Title
127: 				}
128: 				if entry.Link != nil {
129: 					output["link"] = *entry.Link
130: 				}
131: 				if entry.Author != nil {
132: 					output["author"] = *entry.Author
133: 				}
134: 				if entry.PublishedAt != nil {
135: 					output["published_at"] = *entry.PublishedAt
136: 				}
137: 				if entry.Content != nil {
138: 					output["content"] = *entry.Content
139: 				}
140: 				if entry.ReadAt != nil {
141: 					output["read_at"] = *entry.ReadAt
142: 				}
143: 				entryOutputs = append(entryOutputs, output)
144: 			}
145: 			resourceData := ResourceData{
146: 				Metadata: ResourceMetadata{
147: 					Timestamp:   time.Now(),
148: 					Count:       len(entryOutputs),
149: 					ResourceURI: "digest://entries/unread",
150: 					Filters: map[string]any{
151: 						"read": false,
152: 					},
153: 				},
154: 				Data: entryOutputs,
155: 				Links: map[string]string{
156: 					"all_feeds":     "digest://feeds",
157: 					"today_entries": "digest://entries/today",
158: 					"stats":         "digest://stats",
159: 				},
160: 			}
161: 			jsonBytes, err := json.MarshalIndent(resourceData, "", "  ")
162: 			if err != nil {
163: 				return nil, fmt.Errorf("failed to marshal resource data: %w", err)
164: 			}
165: 			return []mcp.ResourceContents{
166: 				&mcp.TextResourceContents{
167: 					URI:      request.Params.URI,
168: 					MIMEType: "application/json",
169: 					Text:     string(jsonBytes),
170: 				},
171: 			}, nil
172: 		},
173: 	)
174: }
175: func (s *Server) registerEntriesTodayResource() {
176: 	s.mcpServer.AddResource(
177: 		mcp.Resource{
178: 			URI:         "digest://entries/today",
179: 			Name:        "Today's Entries",
180: 			Description: "List all feed entries published today (since midnight local time), regardless of read status",
181: 			MIMEType:    "application/json",
182: 		},
183: 		func(ctx context.Context, request mcp.ReadResourceRequest) ([]mcp.ResourceContents, error) {
184: 			// Calculate start of today (midnight local time) - consistent with CLI and timeutil
185: 			startOfDay := timeutil.StartOfToday()
186: 			entries, err := db.ListEntries(s.db, nil, nil, nil, &startOfDay, nil, nil)
187: 			if err != nil {
188: 				return nil, fmt.Errorf("failed to list today's entries: %w", err)
189: 			}
190: 			// Convert to output format
191: 			entryOutputs := make([]map[string]interface{}, 0, len(entries))
192: 			for _, entry := range entries {
193: 				output := map[string]interface{}{
194: 					"id":         entry.ID,
195: 					"feed_id":    entry.FeedID,
196: 					"guid":       entry.GUID,
197: 					"read":       entry.Read,
198: 					"created_at": entry.CreatedAt,
199: 				}
200: 				if entry.Title != nil {
201: 					output["title"] = *entry.Title
202: 				}
203: 				if entry.Link != nil {
204: 					output["link"] = *entry.Link
205: 				}
206: 				if entry.Author != nil {
207: 					output["author"] = *entry.Author
208: 				}
209: 				if entry.PublishedAt != nil {
210: 					output["published_at"] = *entry.PublishedAt
211: 				}
212: 				if entry.Content != nil {
213: 					output["content"] = *entry.Content
214: 				}
215: 				if entry.ReadAt != nil {
216: 					output["read_at"] = *entry.ReadAt
217: 				}
218: 				entryOutputs = append(entryOutputs, output)
219: 			}
220: 			resourceData := ResourceData{
221: 				Metadata: ResourceMetadata{
222: 					Timestamp:   time.Now(),
223: 					Count:       len(entryOutputs),
224: 					ResourceURI: "digest://entries/today",
225: 					Filters: map[string]any{
226: 						"published_since": startOfDay,
227: 					},
228: 				},
229: 				Data: entryOutputs,
230: 				Links: map[string]string{
231: 					"all_feeds":      "digest://feeds",
232: 					"unread_entries": "digest://entries/unread",
233: 					"stats":          "digest://stats",
234: 				},
235: 			}
236: 			jsonBytes, err := json.MarshalIndent(resourceData, "", "  ")
237: 			if err != nil {
238: 				return nil, fmt.Errorf("failed to marshal resource data: %w", err)
239: 			}
240: 			return []mcp.ResourceContents{
241: 				&mcp.TextResourceContents{
242: 					URI:      request.Params.URI,
243: 					MIMEType: "application/json",
244: 					Text:     string(jsonBytes),
245: 				},
246: 			}, nil
247: 		},
248: 	)
249: }
250: func (s *Server) registerStatsResource() {
251: 	s.mcpServer.AddResource(
252: 		mcp.Resource{
253: 			URI:         "digest://stats",
254: 			Name:        "Feed Statistics",
255: 			Description: "Overview statistics including feed counts, entry counts (total, unread), last sync times, and per-feed breakdowns",
256: 			MIMEType:    "application/json",
257: 		},
258: 		func(ctx context.Context, request mcp.ReadResourceRequest) ([]mcp.ResourceContents, error) {
259: 			stats, err := s.calculateStats()
260: 			if err != nil {
261: 				return nil, fmt.Errorf("failed to calculate stats: %w", err)
262: 			}
263: 			resourceData := ResourceData{
264: 				Metadata: ResourceMetadata{
265: 					Timestamp:   time.Now(),
266: 					Count:       0, // Stats don't have a count
267: 					ResourceURI: "digest://stats",
268: 				},
269: 				Data: stats,
270: 				Links: map[string]string{
271: 					"all_feeds":      "digest://feeds",
272: 					"unread_entries": "digest://entries/unread",
273: 					"today_entries":  "digest://entries/today",
274: 				},
275: 			}
276: 			jsonBytes, err := json.MarshalIndent(resourceData, "", "  ")
277: 			if err != nil {
278: 				return nil, fmt.Errorf("failed to marshal resource data: %w", err)
279: 			}
280: 			return []mcp.ResourceContents{
281: 				&mcp.TextResourceContents{
282: 					URI:      request.Params.URI,
283: 					MIMEType: "application/json",
284: 					Text:     string(jsonBytes),
285: 				},
286: 			}, nil
287: 		},
288: 	)
289: }
290: // StatsData represents the statistics summary.
291: type StatsData struct {
292: 	Summary  StatsSummary `json:"summary"`
293: 	ByFeed   []FeedStats  `json:"by_feed"`
294: 	LastSync *SyncInfo    `json:"last_sync,omitempty"`
295: }
296: // StatsSummary contains overall counts.
297: type StatsSummary struct {
298: 	TotalFeeds   int `json:"total_feeds"`
299: 	TotalEntries int `json:"total_entries"`
300: 	UnreadCount  int `json:"unread_count"`
301: }
302: // FeedStats contains per-feed statistics.
303: type FeedStats struct {
304: 	FeedID      string     `json:"feed_id"`
305: 	FeedTitle   string     `json:"feed_title"`
306: 	FeedURL     string     `json:"feed_url"`
307: 	EntryCount  int        `json:"entry_count"`
308: 	UnreadCount int        `json:"unread_count"`
309: 	LastFetched *time.Time `json:"last_fetched,omitempty"`
310: 	ErrorCount  int        `json:"error_count"`
311: 	HasErrors   bool       `json:"has_errors"`
312: }
313: // SyncInfo represents information about the last sync.
314: type SyncInfo struct {
315: 	LastFetchedAt *time.Time `json:"last_fetched_at,omitempty"`
316: 	FeedID        string     `json:"feed_id"`
317: 	FeedTitle     string     `json:"feed_title"`
318: }
319: func (s *Server) calculateStats() (*StatsData, error) {
320: 	// Fetch all feeds
321: 	feeds, err := db.ListFeeds(s.db)
322: 	if err != nil {
323: 		return nil, fmt.Errorf("failed to list feeds: %w", err)
324: 	}
325: 	// Fetch all entries
326: 	allEntries, err := db.ListEntries(s.db, nil, nil, nil, nil, nil, nil)
327: 	if err != nil {
328: 		return nil, fmt.Errorf("failed to list entries: %w", err)
329: 	}
330: 	// Calculate summary stats
331: 	summary := StatsSummary{
332: 		TotalFeeds:   len(feeds),
333: 		TotalEntries: len(allEntries),
334: 	}
335: 	// Count unread across all feeds
336: 	unreadCount, err := db.CountUnreadEntries(s.db, nil)
337: 	if err != nil {
338: 		return nil, fmt.Errorf("failed to count unread entries: %w", err)
339: 	}
340: 	summary.UnreadCount = unreadCount
341: 	// Build per-feed stats
342: 	byFeed := make([]FeedStats, 0, len(feeds))
343: 	var lastSync *SyncInfo
344: 	for _, feed := range feeds {
345: 		// Count entries for this feed
346: 		feedEntries, err := db.ListEntries(s.db, &feed.ID, nil, nil, nil, nil, nil)
347: 		if err != nil {
348: 			return nil, fmt.Errorf("failed to list entries for feed %s: %w", feed.ID, err)
349: 		}
350: 		// Count unread for this feed
351: 		feedUnreadCount, err := db.CountUnreadEntries(s.db, &feed.ID)
352: 		if err != nil {
353: 			return nil, fmt.Errorf("failed to count unread for feed %s: %w", feed.ID, err)
354: 		}
355: 		feedTitle := "Untitled Feed"
356: 		if feed.Title != nil {
357: 			feedTitle = *feed.Title
358: 		}
359: 		feedStat := FeedStats{
360: 			FeedID:      feed.ID,
361: 			FeedTitle:   feedTitle,
362: 			FeedURL:     feed.URL,
363: 			EntryCount:  len(feedEntries),
364: 			UnreadCount: feedUnreadCount,
365: 			LastFetched: feed.LastFetchedAt,
366: 			ErrorCount:  feed.ErrorCount,
367: 			HasErrors:   feed.LastError != nil,
368: 		}
369: 		byFeed = append(byFeed, feedStat)
370: 		// Track most recent sync
371: 		if feed.LastFetchedAt != nil {
372: 			if lastSync == nil || feed.LastFetchedAt.After(*lastSync.LastFetchedAt) {
373: 				lastSync = &SyncInfo{
374: 					LastFetchedAt: feed.LastFetchedAt,
375: 					FeedID:        feed.ID,
376: 					FeedTitle:     feedTitle,
377: 				}
378: 			}
379: 		}
380: 	}
381: 	return &StatsData{
382: 		Summary:  summary,
383: 		ByFeed:   byFeed,
384: 		LastSync: lastSync,
385: 	}, nil
386: }
</file>

<file path="internal/opml/opml.go">
  1: // ABOUTME: OPML parsing and writing library for RSS feed subscriptions
  2: // ABOUTME: Supports folders, feed management, and round-trip XML serialization
  3: package opml
  4: import (
  5: 	"encoding/xml"
  6: 	"fmt"
  7: 	"io"
  8: 	"os"
  9: )
 10: // Document represents an OPML document with a title and hierarchical outlines
 11: type Document struct {
 12: 	Title    string
 13: 	Outlines []Outline
 14: }
 15: // Outline represents a node in the OPML tree structure
 16: // Can be either a folder (with Children) or a feed (with XMLURL)
 17: type Outline struct {
 18: 	Text     string
 19: 	Title    string
 20: 	Type     string
 21: 	XMLURL   string
 22: 	Children []Outline
 23: }
 24: // Feed is a convenience struct representing a single RSS feed with folder information
 25: type Feed struct {
 26: 	URL    string
 27: 	Title  string
 28: 	Folder string
 29: }
 30: // XML structs for parsing and writing OPML files
 31: type opmlXML struct {
 32: 	XMLName xml.Name `xml:"opml"`
 33: 	Version string   `xml:"version,attr"`
 34: 	Head    headXML  `xml:"head"`
 35: 	Body    bodyXML  `xml:"body"`
 36: }
 37: type headXML struct {
 38: 	Title string `xml:"title"`
 39: }
 40: type bodyXML struct {
 41: 	Outlines []outlineXML `xml:"outline"`
 42: }
 43: type outlineXML struct {
 44: 	Text     string       `xml:"text,attr"`
 45: 	Title    string       `xml:"title,attr,omitempty"`
 46: 	Type     string       `xml:"type,attr,omitempty"`
 47: 	XMLURL   string       `xml:"xmlUrl,attr,omitempty"`
 48: 	Children []outlineXML `xml:"outline,omitempty"`
 49: }
 50: // NewDocument creates a new empty OPML document with the given title
 51: func NewDocument(title string) *Document {
 52: 	return &Document{
 53: 		Title:    title,
 54: 		Outlines: []Outline{},
 55: 	}
 56: }
 57: // Parse reads OPML data from an io.Reader and returns a Document
 58: func Parse(r io.Reader) (*Document, error) {
 59: 	var opml opmlXML
 60: 	decoder := xml.NewDecoder(r)
 61: 	if err := decoder.Decode(&opml); err != nil {
 62: 		return nil, fmt.Errorf("failed to decode OPML: %w", err)
 63: 	}
 64: 	doc := &Document{
 65: 		Title:    opml.Head.Title,
 66: 		Outlines: make([]Outline, len(opml.Body.Outlines)),
 67: 	}
 68: 	for i, outline := range opml.Body.Outlines {
 69: 		doc.Outlines[i] = convertOutlineFromXML(outline)
 70: 	}
 71: 	return doc, nil
 72: }
 73: // ParseFile reads OPML data from a file and returns a Document
 74: func ParseFile(path string) (*Document, error) {
 75: 	file, err := os.Open(path)
 76: 	if err != nil {
 77: 		return nil, fmt.Errorf("failed to open file: %w", err)
 78: 	}
 79: 	defer file.Close()
 80: 	return Parse(file)
 81: }
 82: // AllFeeds returns a flat list of all feeds in the document with their folder information
 83: func (d *Document) AllFeeds() []Feed {
 84: 	var feeds []Feed
 85: 	for _, outline := range d.Outlines {
 86: 		feeds = append(feeds, collectFeeds(outline, "")...)
 87: 	}
 88: 	return feeds
 89: }
 90: // Folders returns a list of all folder names in the document
 91: func (d *Document) Folders() []string {
 92: 	folderSet := make(map[string]bool)
 93: 	for _, outline := range d.Outlines {
 94: 		if outline.XMLURL == "" {
 95: 			folderSet[outline.Text] = true
 96: 		}
 97: 	}
 98: 	folders := make([]string, 0, len(folderSet))
 99: 	for folder := range folderSet {
100: 		folders = append(folders, folder)
101: 	}
102: 	return folders
103: }
104: // FeedsInFolder returns all feeds in a specific folder
105: // Pass empty string to get root-level feeds
106: func (d *Document) FeedsInFolder(folder string) []Feed {
107: 	var feeds []Feed
108: 	if folder == "" {
109: 		// Root level feeds
110: 		for _, outline := range d.Outlines {
111: 			if outline.XMLURL != "" {
112: 				feeds = append(feeds, Feed{
113: 					URL:    outline.XMLURL,
114: 					Title:  getOutlineTitle(outline),
115: 					Folder: "",
116: 				})
117: 			}
118: 		}
119: 	} else {
120: 		// Feeds in specific folder
121: 		for _, outline := range d.Outlines {
122: 			if outline.Text == folder && outline.XMLURL == "" {
123: 				for _, child := range outline.Children {
124: 					if child.XMLURL != "" {
125: 						feeds = append(feeds, Feed{
126: 							URL:    child.XMLURL,
127: 							Title:  getOutlineTitle(child),
128: 							Folder: folder,
129: 						})
130: 					}
131: 				}
132: 			}
133: 		}
134: 	}
135: 	return feeds
136: }
137: // AddFolder adds a folder to the document (idempotent)
138: func (d *Document) AddFolder(name string) error {
139: 	// Check if folder already exists
140: 	for _, outline := range d.Outlines {
141: 		if outline.Text == name && outline.XMLURL == "" {
142: 			return nil // Already exists
143: 		}
144: 	}
145: 	// Add new folder
146: 	d.Outlines = append(d.Outlines, Outline{
147: 		Text:     name,
148: 		Children: []Outline{},
149: 	})
150: 	return nil
151: }
152: // AddFeed adds a feed to the document, optionally in a folder
153: // Creates the folder if it doesn't exist
154: // Returns an error if a feed with the same URL already exists
155: func (d *Document) AddFeed(url, title, folder string) error {
156: 	// Check if feed already exists
157: 	allFeeds := d.AllFeeds()
158: 	for _, f := range allFeeds {
159: 		if f.URL == url {
160: 			return fmt.Errorf("feed with URL %s already exists", url)
161: 		}
162: 	}
163: 	feed := Outline{
164: 		Text:   title,
165: 		Title:  title,
166: 		Type:   "rss",
167: 		XMLURL: url,
168: 	}
169: 	if folder == "" {
170: 		// Add to root
171: 		d.Outlines = append(d.Outlines, feed)
172: 	} else {
173: 		// Find or create folder
174: 		folderIndex := -1
175: 		for i, outline := range d.Outlines {
176: 			if outline.Text == folder && outline.XMLURL == "" {
177: 				folderIndex = i
178: 				break
179: 			}
180: 		}
181: 		if folderIndex == -1 {
182: 			// Create folder
183: 			d.Outlines = append(d.Outlines, Outline{
184: 				Text:     folder,
185: 				Children: []Outline{feed},
186: 			})
187: 		} else {
188: 			// Add to existing folder
189: 			d.Outlines[folderIndex].Children = append(d.Outlines[folderIndex].Children, feed)
190: 		}
191: 	}
192: 	return nil
193: }
194: // MoveFeed moves a feed to a different folder
195: // Pass empty string for newFolder to move to root level
196: func (d *Document) MoveFeed(url, newFolder string) error {
197: 	// Find the feed and get its title
198: 	var feed *Feed
199: 	for _, f := range d.AllFeeds() {
200: 		if f.URL == url {
201: 			feed = &f
202: 			break
203: 		}
204: 	}
205: 	if feed == nil {
206: 		return fmt.Errorf("feed not found: %s", url)
207: 	}
208: 	// Remove from current location
209: 	if err := d.RemoveFeed(url); err != nil {
210: 		return fmt.Errorf("failed to remove feed: %w", err)
211: 	}
212: 	// Add to new location (use existing title)
213: 	d.addFeedInternal(url, feed.Title, newFolder)
214: 	return nil
215: }
216: // addFeedInternal adds a feed without checking for duplicates
217: func (d *Document) addFeedInternal(url, title, folder string) {
218: 	feed := Outline{
219: 		Text:   title,
220: 		Title:  title,
221: 		Type:   "rss",
222: 		XMLURL: url,
223: 	}
224: 	if folder == "" {
225: 		// Add to root
226: 		d.Outlines = append(d.Outlines, feed)
227: 	} else {
228: 		// Find or create folder
229: 		folderIndex := -1
230: 		for i, outline := range d.Outlines {
231: 			if outline.Text == folder && outline.XMLURL == "" {
232: 				folderIndex = i
233: 				break
234: 			}
235: 		}
236: 		if folderIndex == -1 {
237: 			// Create folder
238: 			d.Outlines = append(d.Outlines, Outline{
239: 				Text:     folder,
240: 				Children: []Outline{feed},
241: 			})
242: 		} else {
243: 			// Add to existing folder
244: 			d.Outlines[folderIndex].Children = append(d.Outlines[folderIndex].Children, feed)
245: 		}
246: 	}
247: }
248: // RemoveFeed removes a feed from the document by URL
249: func (d *Document) RemoveFeed(url string) error {
250: 	// Check root level
251: 	for i, outline := range d.Outlines {
252: 		if outline.XMLURL == url {
253: 			d.Outlines = append(d.Outlines[:i], d.Outlines[i+1:]...)
254: 			return nil
255: 		}
256: 	}
257: 	// Check folders
258: 	for i, outline := range d.Outlines {
259: 		if outline.XMLURL == "" && len(outline.Children) > 0 {
260: 			for j, child := range outline.Children {
261: 				if child.XMLURL == url {
262: 					d.Outlines[i].Children = append(
263: 						d.Outlines[i].Children[:j],
264: 						d.Outlines[i].Children[j+1:]...,
265: 					)
266: 					return nil
267: 				}
268: 			}
269: 		}
270: 	}
271: 	return fmt.Errorf("feed not found: %s", url)
272: }
273: // Write writes the OPML document to an io.Writer
274: func (d *Document) Write(w io.Writer) error {
275: 	opml := opmlXML{
276: 		Version: "2.0",
277: 		Head: headXML{
278: 			Title: d.Title,
279: 		},
280: 		Body: bodyXML{
281: 			Outlines: make([]outlineXML, len(d.Outlines)),
282: 		},
283: 	}
284: 	for i, outline := range d.Outlines {
285: 		opml.Body.Outlines[i] = convertOutlineToXML(outline)
286: 	}
287: 	encoder := xml.NewEncoder(w)
288: 	encoder.Indent("", "  ")
289: 	if _, err := w.Write([]byte(xml.Header)); err != nil {
290: 		return fmt.Errorf("failed to write XML header: %w", err)
291: 	}
292: 	if err := encoder.Encode(opml); err != nil {
293: 		return fmt.Errorf("failed to encode OPML: %w", err)
294: 	}
295: 	return nil
296: }
297: // WriteFile writes the OPML document to a file
298: func (d *Document) WriteFile(path string) error {
299: 	file, err := os.Create(path)
300: 	if err != nil {
301: 		return fmt.Errorf("failed to create file: %w", err)
302: 	}
303: 	defer file.Close()
304: 	return d.Write(file)
305: }
306: // Helper functions
307: func convertOutlineFromXML(x outlineXML) Outline {
308: 	o := Outline{
309: 		Text:     x.Text,
310: 		Title:    x.Title,
311: 		Type:     x.Type,
312: 		XMLURL:   x.XMLURL,
313: 		Children: make([]Outline, len(x.Children)),
314: 	}
315: 	for i, child := range x.Children {
316: 		o.Children[i] = convertOutlineFromXML(child)
317: 	}
318: 	return o
319: }
320: func convertOutlineToXML(o Outline) outlineXML {
321: 	x := outlineXML{
322: 		Text:     o.Text,
323: 		Title:    o.Title,
324: 		Type:     o.Type,
325: 		XMLURL:   o.XMLURL,
326: 		Children: make([]outlineXML, len(o.Children)),
327: 	}
328: 	for i, child := range o.Children {
329: 		x.Children[i] = convertOutlineToXML(child)
330: 	}
331: 	return x
332: }
333: func collectFeeds(outline Outline, folder string) []Feed {
334: 	var feeds []Feed
335: 	if outline.XMLURL != "" {
336: 		// This is a feed
337: 		feeds = append(feeds, Feed{
338: 			URL:    outline.XMLURL,
339: 			Title:  getOutlineTitle(outline),
340: 			Folder: folder,
341: 		})
342: 	}
343: 	// Recurse into children
344: 	childFolder := folder
345: 	if outline.XMLURL == "" && len(outline.Children) > 0 {
346: 		// This is a folder
347: 		childFolder = outline.Text
348: 	}
349: 	for _, child := range outline.Children {
350: 		feeds = append(feeds, collectFeeds(child, childFolder)...)
351: 	}
352: 	return feeds
353: }
354: func getOutlineTitle(outline Outline) string {
355: 	if outline.Title != "" {
356: 		return outline.Title
357: 	}
358: 	return outline.Text
359: }
</file>

<file path=".github/workflows/ci.yml">
 1: # ABOUTME: GitHub Actions CI workflow for digest
 2: # ABOUTME: Runs tests, linting, and builds on multiple Go versions
 3: name: CI
 4: on:
 5:   push:
 6:     branches: [ main ]
 7:   pull_request:
 8:     branches: [ main ]
 9: jobs:
10:   test:
11:     name: Test
12:     runs-on: ubuntu-latest
13:     strategy:
14:       matrix:
15:         go-version: ['1.23', '1.24']
16:     steps:
17:       - name: Check out code
18:         uses: actions/checkout@v4
19:         with:
20:           fetch-depth: 0
21:       - name: Set up Go
22:         uses: actions/setup-go@v5
23:         with:
24:           go-version: ${{ matrix.go-version }}
25:           cache: true
26:       - name: Cache Go modules
27:         uses: actions/cache@v4
28:         with:
29:           path: ~/go/pkg/mod
30:           key: ${{ runner.os }}-go-${{ hashFiles('**/go.sum') }}
31:           restore-keys: |
32:             ${{ runner.os }}-go-
33:       - name: Download dependencies
34:         run: go mod download
35:       - name: Run tests
36:         run: go test -race -v ./...
37:       - name: Run tests with coverage
38:         if: matrix.go-version == '1.24'
39:         run: go test -race -coverprofile=coverage.out -covermode=atomic ./...
40:       - name: Upload coverage to Codecov
41:         if: matrix.go-version == '1.24'
42:         uses: codecov/codecov-action@v4
43:         with:
44:           file: ./coverage.out
45:           flags: unittests
46:           fail_ci_if_error: false
47:   lint:
48:     name: Lint
49:     runs-on: ubuntu-latest
50:     steps:
51:       - name: Check out code
52:         uses: actions/checkout@v4
53:       - name: Set up Go
54:         uses: actions/setup-go@v5
55:         with:
56:           go-version: '1.24'
57:           cache: true
58:       - name: Run golangci-lint
59:         uses: golangci/golangci-lint-action@v7
60:         with:
61:           version: v2.7.2
62:           args: --timeout=10m
63:   build:
64:     name: Build
65:     runs-on: ubuntu-latest
66:     steps:
67:       - name: Check out code
68:         uses: actions/checkout@v4
69:       - name: Set up Go
70:         uses: actions/setup-go@v5
71:         with:
72:           go-version: '1.24'
73:           cache: true
74:       - name: Build binary
75:         run: make build
76:       - name: Verify binary
77:         run: ./digest --help
78:       - name: Upload binary artifact
79:         uses: actions/upload-artifact@v4
80:         with:
81:           name: digest-linux-amd64
82:           path: digest
83:           retention-days: 7
</file>

<file path="cmd/digest/feed.go">
  1: // ABOUTME: Feed management commands for adding, listing, and removing RSS/Atom feeds
  2: // ABOUTME: Handles feed CRUD operations and syncs changes to both database and OPML file
  3: package main
  4: import (
  5: 	"database/sql"
  6: 	"errors"
  7: 	"fmt"
  8: 	"github.com/spf13/cobra"
  9: 	"github.com/harper/digest/internal/db"
 10: 	"github.com/harper/digest/internal/discover"
 11: 	"github.com/harper/digest/internal/models"
 12: )
 13: var feedCmd = &cobra.Command{
 14: 	Use:     "feed",
 15: 	Aliases: []string{"f"},
 16: 	Short:   "Manage RSS/Atom feeds",
 17: 	Long:    "Add, list, and remove RSS/Atom feeds from your subscriptions",
 18: }
 19: var feedAddCmd = &cobra.Command{
 20: 	Use:   "add <url>",
 21: 	Short: "Add a new RSS/Atom feed",
 22: 	Long:  "Add a new feed to your subscriptions and sync to OPML. Automatically discovers feed URLs from HTML pages.",
 23: 	Args:  cobra.ExactArgs(1),
 24: 	RunE: func(cmd *cobra.Command, args []string) error {
 25: 		inputURL := args[0]
 26: 		folder, _ := cmd.Flags().GetString("folder")
 27: 		title, _ := cmd.Flags().GetString("title")
 28: 		noDiscover, _ := cmd.Flags().GetBool("no-discover")
 29: 		var feedURL, feedTitle string
 30: 		if noDiscover {
 31: 			// Skip discovery, use URL as-is
 32: 			feedURL = inputURL
 33: 			feedTitle = title
 34: 		} else {
 35: 			// Discover feed from URL
 36: 			fmt.Printf("Discovering feed at %s...\n", inputURL)
 37: 			discovered, err := discover.Discover(inputURL)
 38: 			if err != nil {
 39: 				return fmt.Errorf("could not find feed at %s: %w", inputURL, err)
 40: 			}
 41: 			feedURL = discovered.URL
 42: 			if title != "" {
 43: 				feedTitle = title
 44: 			} else {
 45: 				feedTitle = discovered.Title
 46: 			}
 47: 			// Inform user if URL changed
 48: 			if feedURL != inputURL {
 49: 				fmt.Printf("Found feed: %s\n", feedURL)
 50: 			}
 51: 		}
 52: 		// Check if feed already exists
 53: 		existingFeed, err := db.GetFeedByURL(dbConn, feedURL)
 54: 		if err != nil && !errors.Is(err, sql.ErrNoRows) {
 55: 			return fmt.Errorf("failed to check for existing feed: %w", err)
 56: 		}
 57: 		if existingFeed != nil {
 58: 			return fmt.Errorf("feed already exists: %s", feedURL)
 59: 		}
 60: 		// Create new feed
 61: 		feed := models.NewFeed(feedURL)
 62: 		if feedTitle != "" {
 63: 			feed.Title = &feedTitle
 64: 		}
 65: 		// Save to database
 66: 		if err := db.CreateFeed(dbConn, feed); err != nil {
 67: 			return fmt.Errorf("failed to create feed in database: %w", err)
 68: 		}
 69: 		// Add to OPML
 70: 		opmlTitle := feedTitle
 71: 		if opmlTitle == "" {
 72: 			opmlTitle = feedURL
 73: 		}
 74: 		if err := opmlDoc.AddFeed(feedURL, opmlTitle, folder); err != nil {
 75: 			return fmt.Errorf("failed to add feed to OPML: %w", err)
 76: 		}
 77: 		// Save OPML
 78: 		if err := saveOPML(); err != nil {
 79: 			return fmt.Errorf("failed to save OPML: %w", err)
 80: 		}
 81: 		if folder != "" {
 82: 			fmt.Printf("Added feed to folder '%s': %s\n", folder, feedTitle)
 83: 		} else {
 84: 			fmt.Printf("Added feed: %s\n", feedTitle)
 85: 		}
 86: 		fmt.Printf("Feed ID: %s\n", feed.ID)
 87: 		return nil
 88: 	},
 89: }
 90: var feedListCmd = &cobra.Command{
 91: 	Use:     "list",
 92: 	Aliases: []string{"ls"},
 93: 	Short:   "List all feeds",
 94: 	Long:    "List all subscribed feeds with their folders",
 95: 	RunE: func(cmd *cobra.Command, args []string) error {
 96: 		feeds := opmlDoc.AllFeeds()
 97: 		if len(feeds) == 0 {
 98: 			fmt.Println("No feeds found. Add a feed with 'digest feed add <url>'")
 99: 			return nil
100: 		}
101: 		fmt.Printf("Found %d feed(s):\n\n", len(feeds))
102: 		for _, feed := range feeds {
103: 			if feed.Folder != "" {
104: 				fmt.Printf("[%s] %s\n", feed.Folder, feed.Title)
105: 			} else {
106: 				fmt.Printf("%s\n", feed.Title)
107: 			}
108: 			fmt.Printf("  URL: %s\n\n", feed.URL)
109: 		}
110: 		return nil
111: 	},
112: }
113: var feedRemoveCmd = &cobra.Command{
114: 	Use:   "remove <url>",
115: 	Short: "Remove a feed",
116: 	Long:  "Remove a feed from your subscriptions and OPML",
117: 	Args:  cobra.ExactArgs(1),
118: 	RunE: func(cmd *cobra.Command, args []string) error {
119: 		url := args[0]
120: 		// Get feed from database
121: 		feed, err := db.GetFeedByURL(dbConn, url)
122: 		if err != nil {
123: 			if errors.Is(err, sql.ErrNoRows) {
124: 				return fmt.Errorf("feed not found: %s", url)
125: 			}
126: 			return fmt.Errorf("failed to get feed: %w", err)
127: 		}
128: 		// Delete from database
129: 		if err := db.DeleteFeed(dbConn, feed.ID); err != nil {
130: 			return fmt.Errorf("failed to delete feed from database: %w", err)
131: 		}
132: 		// Remove from OPML
133: 		if err := opmlDoc.RemoveFeed(url); err != nil {
134: 			return fmt.Errorf("failed to remove feed from OPML: %w", err)
135: 		}
136: 		// Save OPML
137: 		if err := saveOPML(); err != nil {
138: 			return fmt.Errorf("failed to save OPML: %w", err)
139: 		}
140: 		fmt.Printf("Removed feed: %s\n", url)
141: 		return nil
142: 	},
143: }
144: var feedMoveCmd = &cobra.Command{
145: 	Use:   "move <url> <category>",
146: 	Short: "Move a feed to a different category",
147: 	Long:  "Move a feed to a different category/folder. Use empty quotes \"\" for root level.",
148: 	Args:  cobra.ExactArgs(2),
149: 	RunE: func(cmd *cobra.Command, args []string) error {
150: 		url := args[0]
151: 		newFolder := args[1]
152: 		// Verify feed exists in database
153: 		_, err := db.GetFeedByURL(dbConn, url)
154: 		if err != nil {
155: 			if errors.Is(err, sql.ErrNoRows) {
156: 				return fmt.Errorf("feed not found: %s", url)
157: 			}
158: 			return fmt.Errorf("failed to get feed: %w", err)
159: 		}
160: 		// Move feed in OPML
161: 		if err := opmlDoc.MoveFeed(url, newFolder); err != nil {
162: 			return fmt.Errorf("failed to move feed: %w", err)
163: 		}
164: 		// Save OPML
165: 		if err := saveOPML(); err != nil {
166: 			return fmt.Errorf("failed to save OPML: %w", err)
167: 		}
168: 		if newFolder == "" {
169: 			fmt.Printf("Moved feed to root level: %s\n", url)
170: 		} else {
171: 			fmt.Printf("Moved feed to '%s': %s\n", newFolder, url)
172: 		}
173: 		return nil
174: 	},
175: }
176: func init() {
177: 	rootCmd.AddCommand(feedCmd)
178: 	feedCmd.AddCommand(feedAddCmd)
179: 	feedCmd.AddCommand(feedListCmd)
180: 	feedCmd.AddCommand(feedRemoveCmd)
181: 	feedCmd.AddCommand(feedMoveCmd)
182: 	feedAddCmd.Flags().StringP("folder", "f", "", "folder to organize feed in")
183: 	feedAddCmd.Flags().StringP("title", "t", "", "feed title (defaults to discovered title)")
184: 	feedAddCmd.Flags().Bool("no-discover", false, "skip feed discovery and use URL as-is")
185: }
</file>

<file path="internal/db/entries.go">
  1: // ABOUTME: Entry database operations and CRUD functions
  2: // ABOUTME: Handles creating, reading, updating entries with flexible filtering
  3: package db
  4: import (
  5: 	"database/sql"
  6: 	"fmt"
  7: 	"strings"
  8: 	"time"
  9: 	"github.com/harper/digest/internal/models"
 10: )
 11: // CreateEntry inserts a new entry into the database
 12: func CreateEntry(db *sql.DB, entry *models.Entry) error {
 13: 	query := `
 14: 		INSERT INTO entries (id, feed_id, guid, title, link, author, published_at, content, read, read_at, created_at)
 15: 		VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
 16: 	`
 17: 	_, err := db.Exec(query,
 18: 		entry.ID,
 19: 		entry.FeedID,
 20: 		entry.GUID,
 21: 		entry.Title,
 22: 		entry.Link,
 23: 		entry.Author,
 24: 		entry.PublishedAt,
 25: 		entry.Content,
 26: 		entry.Read,
 27: 		entry.ReadAt,
 28: 		entry.CreatedAt,
 29: 	)
 30: 	if err != nil {
 31: 		return fmt.Errorf("failed to create entry: %w", err)
 32: 	}
 33: 	return nil
 34: }
 35: // GetEntryByID retrieves an entry by its ID
 36: func GetEntryByID(db *sql.DB, id string) (*models.Entry, error) {
 37: 	query := `
 38: 		SELECT id, feed_id, guid, title, link, author, published_at, content, read, read_at, created_at
 39: 		FROM entries
 40: 		WHERE id = ?
 41: 	`
 42: 	entry := &models.Entry{}
 43: 	err := db.QueryRow(query, id).Scan(
 44: 		&entry.ID,
 45: 		&entry.FeedID,
 46: 		&entry.GUID,
 47: 		&entry.Title,
 48: 		&entry.Link,
 49: 		&entry.Author,
 50: 		&entry.PublishedAt,
 51: 		&entry.Content,
 52: 		&entry.Read,
 53: 		&entry.ReadAt,
 54: 		&entry.CreatedAt,
 55: 	)
 56: 	if err != nil {
 57: 		return nil, fmt.Errorf("failed to get entry: %w", err)
 58: 	}
 59: 	return entry, nil
 60: }
 61: // GetEntryByPrefix finds an entry by ID prefix (minimum 6 characters)
 62: // Returns an error if the prefix is ambiguous (matches multiple entries)
 63: // UUIDs only contain hex characters (0-9, a-f) and hyphens, so no SQL wildcard escaping is needed
 64: func GetEntryByPrefix(db *sql.DB, prefix string) (*models.Entry, error) {
 65: 	if len(prefix) < 6 {
 66: 		return nil, fmt.Errorf("prefix must be at least 6 characters")
 67: 	}
 68: 	rows, err := db.Query(`
 69: 		SELECT id, feed_id, guid, title, link, author, published_at, content, read, read_at, created_at
 70: 		FROM entries
 71: 		WHERE id LIKE ?`,
 72: 		prefix+"%",
 73: 	)
 74: 	if err != nil {
 75: 		return nil, fmt.Errorf("failed to query entries by prefix: %w", err)
 76: 	}
 77: 	defer rows.Close()
 78: 	var entries []*models.Entry
 79: 	for rows.Next() {
 80: 		entry := &models.Entry{}
 81: 		if err := rows.Scan(
 82: 			&entry.ID,
 83: 			&entry.FeedID,
 84: 			&entry.GUID,
 85: 			&entry.Title,
 86: 			&entry.Link,
 87: 			&entry.Author,
 88: 			&entry.PublishedAt,
 89: 			&entry.Content,
 90: 			&entry.Read,
 91: 			&entry.ReadAt,
 92: 			&entry.CreatedAt,
 93: 		); err != nil {
 94: 			return nil, fmt.Errorf("failed to scan entry: %w", err)
 95: 		}
 96: 		entries = append(entries, entry)
 97: 	}
 98: 	if err := rows.Err(); err != nil {
 99: 		return nil, fmt.Errorf("error iterating entries: %w", err)
100: 	}
101: 	if len(entries) == 0 {
102: 		return nil, fmt.Errorf("no entry found with prefix %s", prefix)
103: 	}
104: 	if len(entries) > 1 {
105: 		return nil, fmt.Errorf("ambiguous prefix %s matches %d entries", prefix, len(entries))
106: 	}
107: 	return entries[0], nil
108: }
109: // ListEntries retrieves entries with flexible filtering
110: // All parameters are optional via pointers:
111: // - feedID: filter by specific feed (nil = all feeds)
112: // - feedIDs: filter by multiple feeds (nil = ignored, takes precedence over feedID)
113: // - unreadOnly: filter by read status (nil = all entries)
114: // - since: filter by published_at >= since (nil = no time filter)
115: // - until: filter by published_at < until (nil = no upper time bound)
116: // - limit: maximum number of results (nil = no limit)
117: func ListEntries(db *sql.DB, feedID *string, feedIDs []string, unreadOnly *bool, since *time.Time, until *time.Time, limit *int) ([]*models.Entry, error) {
118: 	query := `
119: 		SELECT id, feed_id, guid, title, link, author, published_at, content, read, read_at, created_at
120: 		FROM entries
121: 		WHERE 1=1
122: 	`
123: 	args := []interface{}{}
124: 	// feedIDs takes precedence over feedID
125: 	if len(feedIDs) > 0 {
126: 		placeholders := make([]string, len(feedIDs))
127: 		for i, id := range feedIDs {
128: 			placeholders[i] = "?"
129: 			args = append(args, id)
130: 		}
131: 		query += " AND feed_id IN (" + strings.Join(placeholders, ",") + ")"
132: 	} else if feedID != nil {
133: 		query += " AND feed_id = ?"
134: 		args = append(args, *feedID)
135: 	}
136: 	if unreadOnly != nil && *unreadOnly {
137: 		query += " AND read = FALSE"
138: 	}
139: 	if since != nil {
140: 		query += " AND published_at >= ?"
141: 		args = append(args, *since)
142: 	}
143: 	if until != nil {
144: 		query += " AND published_at < ?"
145: 		args = append(args, *until)
146: 	}
147: 	query += " ORDER BY published_at DESC"
148: 	if limit != nil {
149: 		query += " LIMIT ?"
150: 		args = append(args, *limit)
151: 	}
152: 	rows, err := db.Query(query, args...)
153: 	if err != nil {
154: 		return nil, fmt.Errorf("failed to list entries: %w", err)
155: 	}
156: 	defer rows.Close()
157: 	entries := []*models.Entry{}
158: 	for rows.Next() {
159: 		entry := &models.Entry{}
160: 		err := rows.Scan(
161: 			&entry.ID,
162: 			&entry.FeedID,
163: 			&entry.GUID,
164: 			&entry.Title,
165: 			&entry.Link,
166: 			&entry.Author,
167: 			&entry.PublishedAt,
168: 			&entry.Content,
169: 			&entry.Read,
170: 			&entry.ReadAt,
171: 			&entry.CreatedAt,
172: 		)
173: 		if err != nil {
174: 			return nil, fmt.Errorf("failed to scan entry: %w", err)
175: 		}
176: 		entries = append(entries, entry)
177: 	}
178: 	if err := rows.Err(); err != nil {
179: 		return nil, fmt.Errorf("error iterating entries: %w", err)
180: 	}
181: 	return entries, nil
182: }
183: // MarkEntryRead marks an entry as read and sets read_at to current time
184: func MarkEntryRead(db *sql.DB, id string) error {
185: 	query := `
186: 		UPDATE entries
187: 		SET read = TRUE, read_at = ?
188: 		WHERE id = ?
189: 	`
190: 	now := time.Now()
191: 	_, err := db.Exec(query, now, id)
192: 	if err != nil {
193: 		return fmt.Errorf("failed to mark entry as read: %w", err)
194: 	}
195: 	return nil
196: }
197: // MarkEntryUnread marks an entry as unread and clears the read_at timestamp
198: func MarkEntryUnread(db *sql.DB, id string) error {
199: 	query := `
200: 		UPDATE entries
201: 		SET read = FALSE, read_at = NULL
202: 		WHERE id = ?
203: 	`
204: 	_, err := db.Exec(query, id)
205: 	if err != nil {
206: 		return fmt.Errorf("failed to mark entry as unread: %w", err)
207: 	}
208: 	return nil
209: }
210: // MarkEntriesReadBefore marks all unread entries published before the given time as read
211: // Returns the number of entries that were marked as read
212: func MarkEntriesReadBefore(db *sql.DB, before time.Time) (int64, error) {
213: 	query := `
214: 		UPDATE entries
215: 		SET read = TRUE, read_at = ?
216: 		WHERE published_at < ? AND read = FALSE
217: 	`
218: 	now := time.Now()
219: 	result, err := db.Exec(query, now, before)
220: 	if err != nil {
221: 		return 0, fmt.Errorf("failed to mark entries as read: %w", err)
222: 	}
223: 	count, err := result.RowsAffected()
224: 	if err != nil {
225: 		return 0, fmt.Errorf("failed to get affected rows: %w", err)
226: 	}
227: 	return count, nil
228: }
229: // EntryExists checks if an entry exists with the given feed_id and guid
230: func EntryExists(db *sql.DB, feedID, guid string) (bool, error) {
231: 	query := `
232: 		SELECT COUNT(*) FROM entries
233: 		WHERE feed_id = ? AND guid = ?
234: 	`
235: 	var count int
236: 	err := db.QueryRow(query, feedID, guid).Scan(&count)
237: 	if err != nil {
238: 		return false, fmt.Errorf("failed to check entry existence: %w", err)
239: 	}
240: 	return count > 0, nil
241: }
242: // CountUnreadEntries counts unread entries, optionally filtered by feedID
243: // If feedID is nil, counts all unread entries across all feeds
244: func CountUnreadEntries(db *sql.DB, feedID *string) (int, error) {
245: 	query := `
246: 		SELECT COUNT(*) FROM entries
247: 		WHERE read = FALSE
248: 	`
249: 	args := []interface{}{}
250: 	if feedID != nil {
251: 		query += " AND feed_id = ?"
252: 		args = append(args, *feedID)
253: 	}
254: 	var count int
255: 	err := db.QueryRow(query, args...).Scan(&count)
256: 	if err != nil {
257: 		return 0, fmt.Errorf("failed to count unread entries: %w", err)
258: 	}
259: 	return count, nil
260: }
</file>

<file path="internal/mcp/server.go">
 1: // ABOUTME: MCP server implementation for digest
 2: // ABOUTME: Provides tools, resources, and prompts for AI agents to interact with RSS feeds
 3: package mcp
 4: import (
 5: 	"database/sql"
 6: 	"github.com/harper/digest/internal/opml"
 7: 	"github.com/mark3labs/mcp-go/server"
 8: )
 9: // Server wraps the MCP server with digest-specific context
10: type Server struct {
11: 	mcpServer *server.MCPServer
12: 	db        *sql.DB
13: 	opmlDoc   *opml.Document
14: 	opmlPath  string
15: }
16: // NewServer creates a new MCP server instance
17: func NewServer(db *sql.DB, opmlDoc *opml.Document, opmlPath string) *Server {
18: 	s := &Server{
19: 		db:       db,
20: 		opmlDoc:  opmlDoc,
21: 		opmlPath: opmlPath,
22: 	}
23: 	// Create MCP server
24: 	s.mcpServer = server.NewMCPServer(
25: 		"digest",
26: 		"1.0.0",
27: 		server.WithToolCapabilities(true),
28: 		server.WithResourceCapabilities(true, false),
29: 		server.WithPromptCapabilities(true),
30: 	)
31: 	// Register handlers (stubs for now)
32: 	s.registerTools()
33: 	s.registerResources()
34: 	s.registerPrompts()
35: 	return s
36: }
37: // ServeStdio starts the MCP server on stdio
38: func (s *Server) ServeStdio() error {
39: 	return server.ServeStdio(s.mcpServer)
40: }
41: // registerTools is implemented in tools.go
42: // registerResources is implemented in resources.go
43: // registerPrompts is implemented in prompts.go
</file>

<file path="internal/mcp/tools.go">
  1: // ABOUTME: MCP tool definitions and handlers for feed and entry operations
  2: // ABOUTME: Provides tools for managing RSS feeds, syncing content, and tracking read/unread entries
  3: package mcp
  4: import (
  5: 	"context"
  6: 	"encoding/json"
  7: 	"fmt"
  8: 	"net/url"
  9: 	"time"
 10: 	"github.com/harper/digest/internal/content"
 11: 	"github.com/harper/digest/internal/db"
 12: 	"github.com/harper/digest/internal/fetch"
 13: 	"github.com/harper/digest/internal/models"
 14: 	"github.com/harper/digest/internal/parse"
 15: 	"github.com/harper/digest/internal/timeutil"
 16: 	"github.com/mark3labs/mcp-go/mcp"
 17: )
 18: // Type definitions for input/output structures
 19: type ListFeedsInput struct{}
 20: type FeedOutput struct {
 21: 	ID            string     `json:"id"`
 22: 	URL           string     `json:"url"`
 23: 	Title         *string    `json:"title,omitempty"`
 24: 	Folder        string     `json:"folder,omitempty"`
 25: 	LastFetchedAt *time.Time `json:"last_fetched_at,omitempty"`
 26: 	LastError     *string    `json:"last_error,omitempty"`
 27: 	ErrorCount    int        `json:"error_count"`
 28: 	CreatedAt     time.Time  `json:"created_at"`
 29: }
 30: type ListFeedsOutput struct {
 31: 	Feeds   []FeedOutput `json:"feeds"`
 32: 	Count   int          `json:"count"`
 33: 	Folders []string     `json:"folders"`
 34: }
 35: type AddFeedInput struct {
 36: 	URL    string  `json:"url"`
 37: 	Title  *string `json:"title,omitempty"`
 38: 	Folder *string `json:"folder,omitempty"`
 39: }
 40: type RemoveFeedInput struct {
 41: 	URL string `json:"url"`
 42: }
 43: type RemoveFeedOutput struct {
 44: 	Success bool   `json:"success"`
 45: 	Message string `json:"message"`
 46: 	URL     string `json:"url"`
 47: }
 48: type MoveFeedInput struct {
 49: 	URL    string `json:"url"`
 50: 	Folder string `json:"folder"`
 51: }
 52: type MoveFeedOutput struct {
 53: 	Success   bool   `json:"success"`
 54: 	Message   string `json:"message"`
 55: 	URL       string `json:"url"`
 56: 	OldFolder string `json:"old_folder"`
 57: 	NewFolder string `json:"new_folder"`
 58: }
 59: type SyncFeedsInput struct {
 60: 	URL   *string `json:"url,omitempty"`
 61: 	Force *bool   `json:"force,omitempty"`
 62: }
 63: type SyncResult struct {
 64: 	FeedID     string  `json:"feed_id"`
 65: 	FeedTitle  string  `json:"feed_title"`
 66: 	NewEntries int     `json:"new_entries"`
 67: 	WasCached  bool    `json:"was_cached"`
 68: 	Error      *string `json:"error,omitempty"`
 69: }
 70: type SyncFeedsOutput struct {
 71: 	Results     []SyncResult `json:"results"`
 72: 	TotalFeeds  int          `json:"total_feeds"`
 73: 	TotalNew    int          `json:"total_new"`
 74: 	TotalCached int          `json:"total_cached"`
 75: 	TotalErrors int          `json:"total_errors"`
 76: }
 77: type ListEntriesInput struct {
 78: 	FeedID     *string `json:"feed_id,omitempty"`
 79: 	UnreadOnly *bool   `json:"unread_only,omitempty"`
 80: 	Since      *string `json:"since,omitempty"`
 81: 	Until      *string `json:"until,omitempty"`
 82: 	Limit      *int    `json:"limit,omitempty"`
 83: }
 84: type EntryOutput struct {
 85: 	ID          string     `json:"id"`
 86: 	FeedID      string     `json:"feed_id"`
 87: 	Title       *string    `json:"title,omitempty"`
 88: 	Link        *string    `json:"link,omitempty"`
 89: 	Author      *string    `json:"author,omitempty"`
 90: 	PublishedAt *time.Time `json:"published_at,omitempty"`
 91: 	Read        bool       `json:"read"`
 92: 	ReadAt      *time.Time `json:"read_at,omitempty"`
 93: 	CreatedAt   time.Time  `json:"created_at"`
 94: }
 95: type ListEntriesOutput struct {
 96: 	Entries []EntryOutput  `json:"entries"`
 97: 	Count   int            `json:"count"`
 98: 	Filters map[string]any `json:"filters"`
 99: }
100: type MarkReadInput struct {
101: 	EntryID string `json:"entry_id"`
102: }
103: type MarkUnreadInput struct {
104: 	EntryID string `json:"entry_id"`
105: }
106: type BulkMarkReadInput struct {
107: 	Before string `json:"before"`
108: }
109: type BulkMarkReadOutput struct {
110: 	Count   int64     `json:"count"`
111: 	Before  time.Time `json:"before"`
112: 	Message string    `json:"message"`
113: }
114: type GetEntryInput struct {
115: 	EntryID string `json:"entry_id"`
116: }
117: type GetEntryOutput struct {
118: 	ID          string     `json:"id"`
119: 	FeedID      string     `json:"feed_id"`
120: 	FeedTitle   string     `json:"feed_title,omitempty"`
121: 	Title       *string    `json:"title,omitempty"`
122: 	Link        *string    `json:"link,omitempty"`
123: 	Author      *string    `json:"author,omitempty"`
124: 	PublishedAt *time.Time `json:"published_at,omitempty"`
125: 	Content     *string    `json:"content,omitempty"`
126: 	Read        bool       `json:"read"`
127: 	ReadAt      *time.Time `json:"read_at,omitempty"`
128: 	CreatedAt   time.Time  `json:"created_at"`
129: }
130: // Tool registration
131: func (s *Server) registerTools() {
132: 	s.registerListFeedsTool()
133: 	s.registerAddFeedTool()
134: 	s.registerRemoveFeedTool()
135: 	s.registerMoveFeedTool()
136: 	s.registerSyncFeedsTool()
137: 	s.registerListEntriesTool()
138: 	s.registerGetEntryTool()
139: 	s.registerMarkReadTool()
140: 	s.registerMarkUnreadTool()
141: 	s.registerBulkMarkReadTool()
142: }
143: func (s *Server) registerListFeedsTool() {
144: 	tool := mcp.Tool{
145: 		Name:        "list_feeds",
146: 		Description: "Retrieve all RSS/Atom feeds from the OPML subscription list. Returns a complete list of feeds with their metadata including URLs, titles, folders, last fetch times, and error states. Use this to see all subscribed feeds before performing other operations.",
147: 		InputSchema: mcp.ToolInputSchema{
148: 			Type:       "object",
149: 			Properties: map[string]interface{}{},
150: 		},
151: 	}
152: 	s.mcpServer.AddTool(tool, s.handleListFeeds)
153: }
154: func (s *Server) registerAddFeedTool() {
155: 	tool := mcp.Tool{
156: 		Name:        "add_feed",
157: 		Description: "Add a new RSS/Atom feed to the subscription list. The feed is added to both the database and the OPML file. Optionally specify a title and folder for organization. If no title is provided, it will be fetched from the feed on first sync. Returns the created feed with its unique ID.",
158: 		InputSchema: mcp.ToolInputSchema{
159: 			Type: "object",
160: 			Properties: map[string]interface{}{
161: 				"url": map[string]interface{}{
162: 					"type":        "string",
163: 					"description": "The feed URL (RSS or Atom). Example: 'https://example.com/feed.xml'",
164: 				},
165: 				"title": map[string]interface{}{
166: 					"type":        "string",
167: 					"description": "Optional feed title. If not provided, will be fetched from the feed metadata. Example: 'My Favorite Blog'",
168: 				},
169: 				"folder": map[string]interface{}{
170: 					"type":        "string",
171: 					"description": "Optional folder/category for organization in OPML. Example: 'Tech Blogs'",
172: 				},
173: 			},
174: 			Required: []string{"url"},
175: 		},
176: 	}
177: 	s.mcpServer.AddTool(tool, s.handleAddFeed)
178: }
179: func (s *Server) registerRemoveFeedTool() {
180: 	tool := mcp.Tool{
181: 		Name:        "remove_feed",
182: 		Description: "Remove a feed from the subscription list. This removes the feed from both the database and the OPML file. All entries associated with this feed will also be deleted due to CASCADE constraints. This action cannot be undone.",
183: 		InputSchema: mcp.ToolInputSchema{
184: 			Type: "object",
185: 			Properties: map[string]interface{}{
186: 				"url": map[string]interface{}{
187: 					"type":        "string",
188: 					"description": "The feed URL to remove. Must match exactly. Example: 'https://example.com/feed.xml'",
189: 				},
190: 			},
191: 			Required: []string{"url"},
192: 		},
193: 	}
194: 	s.mcpServer.AddTool(tool, s.handleRemoveFeed)
195: }
196: func (s *Server) registerMoveFeedTool() {
197: 	tool := mcp.Tool{
198: 		Name:        "move_feed",
199: 		Description: "Move a feed to a different folder/category in the OPML file. Use this to reorganize feeds after they've been added. If the target folder doesn't exist, it will be created. Use an empty string for folder to move to the root level.",
200: 		InputSchema: mcp.ToolInputSchema{
201: 			Type: "object",
202: 			Properties: map[string]interface{}{
203: 				"url": map[string]interface{}{
204: 					"type":        "string",
205: 					"description": "The feed URL to move. Must match exactly. Example: 'https://example.com/feed.xml'",
206: 				},
207: 				"folder": map[string]interface{}{
208: 					"type":        "string",
209: 					"description": "Target folder name. Use empty string '' to move to root level. Example: 'Tech Blogs'",
210: 				},
211: 			},
212: 			Required: []string{"url", "folder"},
213: 		},
214: 	}
215: 	s.mcpServer.AddTool(tool, s.handleMoveFeed)
216: }
217: func (s *Server) registerSyncFeedsTool() {
218: 	tool := mcp.Tool{
219: 		Name:        "sync_feeds",
220: 		Description: "Fetch new entries from RSS/Atom feeds. If url is provided, syncs only that specific feed. Otherwise, syncs all subscribed feeds. Uses HTTP caching headers (ETag, Last-Modified) to avoid unnecessary downloads. Set force=true to ignore cache and fetch unconditionally. Returns a summary of new entries, cached responses, and any errors.",
221: 		InputSchema: mcp.ToolInputSchema{
222: 			Type: "object",
223: 			Properties: map[string]interface{}{
224: 				"url": map[string]interface{}{
225: 					"type":        "string",
226: 					"description": "Optional feed URL to sync only that specific feed. If omitted, syncs all feeds. Example: 'https://example.com/feed.xml'",
227: 				},
228: 				"force": map[string]interface{}{
229: 					"type":        "boolean",
230: 					"description": "If true, ignores HTTP cache headers and forces a fresh fetch. Default: false",
231: 				},
232: 			},
233: 		},
234: 	}
235: 	s.mcpServer.AddTool(tool, s.handleSyncFeeds)
236: }
237: func (s *Server) registerListEntriesTool() {
238: 	tool := mcp.Tool{
239: 		Name:        "list_entries",
240: 		Description: "Retrieve feed entries with optional filtering. Use 'since' with values like 'today', 'yesterday', 'week', 'month' to get recent entries (e.g., since='today' for today's entries). Filter by feed_id for a specific feed, unread_only for unread entries, and limit to control results. All filters are optional and can be combined. Returns entries sorted by published date (newest first). Use get_entry to read full article content.",
241: 		InputSchema: mcp.ToolInputSchema{
242: 			Type: "object",
243: 			Properties: map[string]interface{}{
244: 				"feed_id": map[string]interface{}{
245: 					"type":        "string",
246: 					"description": "Optional feed ID to filter entries. Only entries from this feed will be returned. Example: 'abc12345-1234-1234-1234-123456789abc'",
247: 				},
248: 				"unread_only": map[string]interface{}{
249: 					"type":        "boolean",
250: 					"description": "If true, returns only unread entries. If false or omitted, returns all entries. Example: true",
251: 				},
252: 				"since": map[string]interface{}{
253: 					"type":        "string",
254: 					"description": "Only return entries published on or after this date. Accepts shortcuts: 'today', 'yesterday', 'week', 'month', or ISO date (YYYY-MM-DD). Example: 'today' for today's entries",
255: 				},
256: 				"until": map[string]interface{}{
257: 					"type":        "string",
258: 					"description": "Only return entries published before this date. Accepts: 'today', 'yesterday', 'week', 'month', or ISO date (YYYY-MM-DD). Example: 'today' for yesterday and earlier",
259: 				},
260: 				"limit": map[string]interface{}{
261: 					"type":        "integer",
262: 					"description": "Maximum number of entries to return. If omitted, returns all matching entries. Example: 50",
263: 				},
264: 			},
265: 		},
266: 	}
267: 	s.mcpServer.AddTool(tool, s.handleListEntries)
268: }
269: func (s *Server) registerGetEntryTool() {
270: 	tool := mcp.Tool{
271: 		Name:        "get_entry",
272: 		Description: "Get the full details of a single entry including its content. Content is converted from HTML to Markdown for better readability. Use this after list_entries to read the full article. Supports both full entry IDs and ID prefixes (first 8 characters).",
273: 		InputSchema: mcp.ToolInputSchema{
274: 			Type: "object",
275: 			Properties: map[string]interface{}{
276: 				"entry_id": map[string]interface{}{
277: 					"type":        "string",
278: 					"description": "The entry ID or ID prefix. Example: 'abc12345' (prefix) or 'abc12345-1234-1234-1234-123456789abc' (full)",
279: 				},
280: 			},
281: 			Required: []string{"entry_id"},
282: 		},
283: 	}
284: 	s.mcpServer.AddTool(tool, s.handleGetEntry)
285: }
286: func (s *Server) registerMarkReadTool() {
287: 	tool := mcp.Tool{
288: 		Name:        "mark_read",
289: 		Description: "Mark an entry as read by its ID. Sets the read flag to true and records the current timestamp. Returns the updated entry. Use list_entries to find entry IDs.",
290: 		InputSchema: mcp.ToolInputSchema{
291: 			Type: "object",
292: 			Properties: map[string]interface{}{
293: 				"entry_id": map[string]interface{}{
294: 					"type":        "string",
295: 					"description": "The entry ID to mark as read. Example: 'abc12345-1234-1234-1234-123456789abc'",
296: 				},
297: 			},
298: 			Required: []string{"entry_id"},
299: 		},
300: 	}
301: 	s.mcpServer.AddTool(tool, s.handleMarkRead)
302: }
303: func (s *Server) registerMarkUnreadTool() {
304: 	tool := mcp.Tool{
305: 		Name:        "mark_unread",
306: 		Description: "Mark an entry as unread by its ID. Sets the read flag to false and clears the read timestamp. Returns the updated entry. Use list_entries to find entry IDs.",
307: 		InputSchema: mcp.ToolInputSchema{
308: 			Type: "object",
309: 			Properties: map[string]interface{}{
310: 				"entry_id": map[string]interface{}{
311: 					"type":        "string",
312: 					"description": "The entry ID to mark as unread. Example: 'abc12345-1234-1234-1234-123456789abc'",
313: 				},
314: 			},
315: 			Required: []string{"entry_id"},
316: 		},
317: 	}
318: 	s.mcpServer.AddTool(tool, s.handleMarkUnread)
319: }
320: func (s *Server) registerBulkMarkReadTool() {
321: 	tool := mcp.Tool{
322: 		Name:        "bulk_mark_read",
323: 		Description: "Mark all entries older than a specified period as read. Use this to catch up on older content. Accepts period names (yesterday, week, month) or ISO 8601 dates (YYYY-MM-DD). Returns the count of entries marked as read.",
324: 		InputSchema: mcp.ToolInputSchema{
325: 			Type: "object",
326: 			Properties: map[string]interface{}{
327: 				"before": map[string]interface{}{
328: 					"type":        "string",
329: 					"description": "Mark entries published before this date/period as read. Accepts: 'yesterday', 'week', 'month', or YYYY-MM-DD. Example: 'yesterday' or '2024-01-15'",
330: 				},
331: 			},
332: 			Required: []string{"before"},
333: 		},
334: 	}
335: 	s.mcpServer.AddTool(tool, s.handleBulkMarkRead)
336: }
337: // Handler implementations
338: func (s *Server) handleListFeeds(_ context.Context, req mcp.CallToolRequest) (*mcp.CallToolResult, error) {
339: 	// Get all feeds from OPML
340: 	opmlFeeds := s.opmlDoc.AllFeeds()
341: 	folders := s.opmlDoc.Folders()
342: 	// Get all feeds from database
343: 	dbFeeds, err := db.ListFeeds(s.db)
344: 	if err != nil {
345: 		return nil, fmt.Errorf("failed to list feeds from database: %w", err)
346: 	}
347: 	// Create map for quick lookup
348: 	dbFeedMap := make(map[string]*models.Feed)
349: 	for _, feed := range dbFeeds {
350: 		dbFeedMap[feed.URL] = feed
351: 	}
352: 	// Build output by combining OPML and DB data
353: 	feedOutputs := make([]FeedOutput, 0, len(opmlFeeds))
354: 	for _, opmlFeed := range opmlFeeds {
355: 		output := FeedOutput{
356: 			URL:    opmlFeed.URL,
357: 			Folder: opmlFeed.Folder,
358: 		}
359: 		// Add database info if available
360: 		if dbFeed, exists := dbFeedMap[opmlFeed.URL]; exists {
361: 			output.ID = dbFeed.ID
362: 			output.Title = dbFeed.Title
363: 			output.LastFetchedAt = dbFeed.LastFetchedAt
364: 			output.LastError = dbFeed.LastError
365: 			output.ErrorCount = dbFeed.ErrorCount
366: 			output.CreatedAt = dbFeed.CreatedAt
367: 		} else {
368: 			// Feed in OPML but not in DB
369: 			title := opmlFeed.Title
370: 			output.Title = &title
371: 		}
372: 		feedOutputs = append(feedOutputs, output)
373: 	}
374: 	result := ListFeedsOutput{
375: 		Feeds:   feedOutputs,
376: 		Count:   len(feedOutputs),
377: 		Folders: folders,
378: 	}
379: 	jsonBytes, err := json.MarshalIndent(result, "", "  ")
380: 	if err != nil {
381: 		return nil, fmt.Errorf("failed to marshal output: %w", err)
382: 	}
383: 	return mcp.NewToolResultText(string(jsonBytes)), nil
384: }
385: func (s *Server) handleAddFeed(_ context.Context, req mcp.CallToolRequest) (*mcp.CallToolResult, error) {
386: 	var input AddFeedInput
387: 	if err := req.BindArguments(&input); err != nil {
388: 		return nil, fmt.Errorf("invalid input: %w", err)
389: 	}
390: 	// Validate URL format
391: 	parsedURL, err := url.Parse(input.URL)
392: 	if err != nil {
393: 		return nil, fmt.Errorf("invalid feed URL: %w", err)
394: 	}
395: 	if parsedURL.Scheme != "http" && parsedURL.Scheme != "https" {
396: 		return nil, fmt.Errorf("feed URL must use http or https scheme, got: %s", parsedURL.Scheme)
397: 	}
398: 	if parsedURL.Host == "" {
399: 		return nil, fmt.Errorf("feed URL must have a host")
400: 	}
401: 	// Check if feed already exists in database
402: 	existingFeed, err := db.GetFeedByURL(s.db, input.URL)
403: 	if err == nil && existingFeed != nil {
404: 		return nil, fmt.Errorf("feed already exists: %s", input.URL)
405: 	}
406: 	// Create feed in database
407: 	feed := models.NewFeed(input.URL)
408: 	if input.Title != nil {
409: 		feed.Title = input.Title
410: 	}
411: 	if err := db.CreateFeed(s.db, feed); err != nil {
412: 		return nil, fmt.Errorf("failed to create feed in database: %w", err)
413: 	}
414: 	// Add to OPML
415: 	title := input.URL
416: 	if input.Title != nil {
417: 		title = *input.Title
418: 	}
419: 	folder := ""
420: 	if input.Folder != nil {
421: 		folder = *input.Folder
422: 	}
423: 	if err := s.opmlDoc.AddFeed(input.URL, title, folder); err != nil {
424: 		return nil, fmt.Errorf("failed to add feed to OPML: %w", err)
425: 	}
426: 	// Write OPML back to file
427: 	if err := s.opmlDoc.WriteFile(s.opmlPath); err != nil {
428: 		return nil, fmt.Errorf("failed to write OPML file: %w", err)
429: 	}
430: 	output := FeedOutput{
431: 		ID:         feed.ID,
432: 		URL:        feed.URL,
433: 		Title:      feed.Title,
434: 		Folder:     folder,
435: 		ErrorCount: feed.ErrorCount,
436: 		CreatedAt:  feed.CreatedAt,
437: 	}
438: 	jsonBytes, err := json.MarshalIndent(output, "", "  ")
439: 	if err != nil {
440: 		return nil, fmt.Errorf("failed to marshal output: %w", err)
441: 	}
442: 	return mcp.NewToolResultText(string(jsonBytes)), nil
443: }
444: func (s *Server) handleRemoveFeed(_ context.Context, req mcp.CallToolRequest) (*mcp.CallToolResult, error) {
445: 	var input RemoveFeedInput
446: 	if err := req.BindArguments(&input); err != nil {
447: 		return nil, fmt.Errorf("invalid input: %w", err)
448: 	}
449: 	// Get feed from database to get ID
450: 	feed, err := db.GetFeedByURL(s.db, input.URL)
451: 	if err != nil {
452: 		return nil, fmt.Errorf("feed not found: %s", input.URL)
453: 	}
454: 	// Delete from database (CASCADE will delete entries)
455: 	if err := db.DeleteFeed(s.db, feed.ID); err != nil {
456: 		return nil, fmt.Errorf("failed to delete feed from database: %w", err)
457: 	}
458: 	// Remove from OPML
459: 	if err := s.opmlDoc.RemoveFeed(input.URL); err != nil {
460: 		return nil, fmt.Errorf("failed to remove feed from OPML: %w", err)
461: 	}
462: 	// Write OPML back to file
463: 	if err := s.opmlDoc.WriteFile(s.opmlPath); err != nil {
464: 		return nil, fmt.Errorf("failed to write OPML file: %w", err)
465: 	}
466: 	output := RemoveFeedOutput{
467: 		Success: true,
468: 		Message: fmt.Sprintf("Feed '%s' and all its entries successfully removed", input.URL),
469: 		URL:     input.URL,
470: 	}
471: 	jsonBytes, err := json.MarshalIndent(output, "", "  ")
472: 	if err != nil {
473: 		return nil, fmt.Errorf("failed to marshal output: %w", err)
474: 	}
475: 	return mcp.NewToolResultText(string(jsonBytes)), nil
476: }
477: func (s *Server) handleMoveFeed(_ context.Context, req mcp.CallToolRequest) (*mcp.CallToolResult, error) {
478: 	var input MoveFeedInput
479: 	if err := req.BindArguments(&input); err != nil {
480: 		return nil, fmt.Errorf("invalid input: %w", err)
481: 	}
482: 	// Validate URL format (consistent with handleAddFeed)
483: 	parsedURL, err := url.Parse(input.URL)
484: 	if err != nil {
485: 		return nil, fmt.Errorf("invalid feed URL: %w", err)
486: 	}
487: 	if parsedURL.Scheme != "http" && parsedURL.Scheme != "https" {
488: 		return nil, fmt.Errorf("feed URL must use http or https scheme, got: %s", parsedURL.Scheme)
489: 	}
490: 	if parsedURL.Host == "" {
491: 		return nil, fmt.Errorf("feed URL must have a host")
492: 	}
493: 	// Verify feed exists in database (consistent with handleRemoveFeed)
494: 	if _, err := db.GetFeedByURL(s.db, input.URL); err != nil {
495: 		return nil, fmt.Errorf("feed not found: %s", input.URL)
496: 	}
497: 	// Find current folder for the feed
498: 	oldFolder := ""
499: 	found := false
500: 	for _, feed := range s.opmlDoc.AllFeeds() {
501: 		if feed.URL == input.URL {
502: 			oldFolder = feed.Folder
503: 			found = true
504: 			break
505: 		}
506: 	}
507: 	if !found {
508: 		return nil, fmt.Errorf("feed not found in OPML: %s", input.URL)
509: 	}
510: 	// Skip if already in target folder
511: 	if oldFolder == input.Folder {
512: 		output := MoveFeedOutput{
513: 			Success:   true,
514: 			Message:   fmt.Sprintf("Feed is already in %s", formatFolder(oldFolder)),
515: 			URL:       input.URL,
516: 			OldFolder: oldFolder,
517: 			NewFolder: input.Folder,
518: 		}
519: 		jsonBytes, err := json.MarshalIndent(output, "", "  ")
520: 		if err != nil {
521: 			return nil, fmt.Errorf("failed to marshal output: %w", err)
522: 		}
523: 		return mcp.NewToolResultText(string(jsonBytes)), nil
524: 	}
525: 	// Move the feed
526: 	if err := s.opmlDoc.MoveFeed(input.URL, input.Folder); err != nil {
527: 		return nil, fmt.Errorf("failed to move feed: %w", err)
528: 	}
529: 	// Write OPML back to file
530: 	if err := s.opmlDoc.WriteFile(s.opmlPath); err != nil {
531: 		return nil, fmt.Errorf("failed to write OPML file: %w", err)
532: 	}
533: 	output := MoveFeedOutput{
534: 		Success:   true,
535: 		Message:   fmt.Sprintf("Feed moved from %s to %s", formatFolder(oldFolder), formatFolder(input.Folder)),
536: 		URL:       input.URL,
537: 		OldFolder: oldFolder,
538: 		NewFolder: input.Folder,
539: 	}
540: 	jsonBytes, err := json.MarshalIndent(output, "", "  ")
541: 	if err != nil {
542: 		return nil, fmt.Errorf("failed to marshal output: %w", err)
543: 	}
544: 	return mcp.NewToolResultText(string(jsonBytes)), nil
545: }
546: func (s *Server) handleSyncFeeds(_ context.Context, req mcp.CallToolRequest) (*mcp.CallToolResult, error) {
547: 	var input SyncFeedsInput
548: 	if err := req.BindArguments(&input); err != nil {
549: 		return nil, fmt.Errorf("invalid input: %w", err)
550: 	}
551: 	force := false
552: 	if input.Force != nil {
553: 		force = *input.Force
554: 	}
555: 	// Get feeds to sync
556: 	feeds, err := db.ListFeeds(s.db)
557: 	if err != nil {
558: 		return nil, fmt.Errorf("failed to list feeds: %w", err)
559: 	}
560: 	if len(feeds) == 0 {
561: 		return nil, fmt.Errorf("no feeds found. Add a feed first using add_feed")
562: 	}
563: 	// Filter to specific URL if provided
564: 	if input.URL != nil {
565: 		filtered := []*models.Feed{}
566: 		for _, feed := range feeds {
567: 			if feed.URL == *input.URL {
568: 				filtered = append(filtered, feed)
569: 				break
570: 			}
571: 		}
572: 		if len(filtered) == 0 {
573: 			return nil, fmt.Errorf("feed not found: %s", *input.URL)
574: 		}
575: 		feeds = filtered
576: 	}
577: 	// Sync each feed
578: 	results := make([]SyncResult, 0, len(feeds))
579: 	totalNew := 0
580: 	totalCached := 0
581: 	totalErrors := 0
582: 	for _, feed := range feeds {
583: 		result := SyncResult{
584: 			FeedID: feed.ID,
585: 			FeedTitle: func() string {
586: 				if feed.Title != nil {
587: 					return *feed.Title
588: 				}
589: 				return feed.URL
590: 			}(),
591: 		}
592: 		newCount, wasCached, err := s.syncFeed(feed, force)
593: 		if err != nil {
594: 			errMsg := err.Error()
595: 			result.Error = &errMsg
596: 			totalErrors++
597: 		} else {
598: 			result.NewEntries = newCount
599: 			result.WasCached = wasCached
600: 			totalNew += newCount
601: 			if wasCached {
602: 				totalCached++
603: 			}
604: 		}
605: 		results = append(results, result)
606: 	}
607: 	output := SyncFeedsOutput{
608: 		Results:     results,
609: 		TotalFeeds:  len(feeds),
610: 		TotalNew:    totalNew,
611: 		TotalCached: totalCached,
612: 		TotalErrors: totalErrors,
613: 	}
614: 	jsonBytes, err := json.MarshalIndent(output, "", "  ")
615: 	if err != nil {
616: 		return nil, fmt.Errorf("failed to marshal output: %w", err)
617: 	}
618: 	return mcp.NewToolResultText(string(jsonBytes)), nil
619: }
620: func (s *Server) handleListEntries(_ context.Context, req mcp.CallToolRequest) (*mcp.CallToolResult, error) {
621: 	var input ListEntriesInput
622: 	if err := req.BindArguments(&input); err != nil {
623: 		return nil, fmt.Errorf("invalid input: %w", err)
624: 	}
625: 	// Parse since/until date strings
626: 	var since, until *time.Time
627: 	if input.Since != nil {
628: 		t, err := parseDateString(*input.Since)
629: 		if err != nil {
630: 			return nil, fmt.Errorf("invalid since value: %w", err)
631: 		}
632: 		since = &t
633: 	}
634: 	if input.Until != nil {
635: 		t, err := parseDateString(*input.Until)
636: 		if err != nil {
637: 			return nil, fmt.Errorf("invalid until value: %w", err)
638: 		}
639: 		until = &t
640: 	}
641: 	// List entries with filters
642: 	entries, err := db.ListEntries(s.db, input.FeedID, nil, input.UnreadOnly, since, until, input.Limit)
643: 	if err != nil {
644: 		return nil, fmt.Errorf("failed to list entries: %w", err)
645: 	}
646: 	// Build output
647: 	entryOutputs := make([]EntryOutput, 0, len(entries))
648: 	for _, entry := range entries {
649: 		entryOutputs = append(entryOutputs, EntryOutput{
650: 			ID:          entry.ID,
651: 			FeedID:      entry.FeedID,
652: 			Title:       entry.Title,
653: 			Link:        entry.Link,
654: 			Author:      entry.Author,
655: 			PublishedAt: entry.PublishedAt,
656: 			Read:        entry.Read,
657: 			ReadAt:      entry.ReadAt,
658: 			CreatedAt:   entry.CreatedAt,
659: 		})
660: 	}
661: 	// Build applied filters
662: 	filters := make(map[string]any)
663: 	if input.FeedID != nil {
664: 		filters["feed_id"] = *input.FeedID
665: 	}
666: 	if input.UnreadOnly != nil {
667: 		filters["unread_only"] = *input.UnreadOnly
668: 	}
669: 	if since != nil {
670: 		filters["since"] = *since
671: 	}
672: 	if until != nil {
673: 		filters["until"] = *until
674: 	}
675: 	if input.Limit != nil {
676: 		filters["limit"] = *input.Limit
677: 	}
678: 	output := ListEntriesOutput{
679: 		Entries: entryOutputs,
680: 		Count:   len(entryOutputs),
681: 		Filters: filters,
682: 	}
683: 	jsonBytes, err := json.MarshalIndent(output, "", "  ")
684: 	if err != nil {
685: 		return nil, fmt.Errorf("failed to marshal output: %w", err)
686: 	}
687: 	return mcp.NewToolResultText(string(jsonBytes)), nil
688: }
689: func (s *Server) handleGetEntry(_ context.Context, req mcp.CallToolRequest) (*mcp.CallToolResult, error) {
690: 	var input GetEntryInput
691: 	if err := req.BindArguments(&input); err != nil {
692: 		return nil, fmt.Errorf("invalid input: %w", err)
693: 	}
694: 	// Get entry by ID or prefix
695: 	entry, err := db.GetEntryByID(s.db, input.EntryID)
696: 	if err != nil {
697: 		// Try prefix match
698: 		entry, err = db.GetEntryByPrefix(s.db, input.EntryID)
699: 		if err != nil {
700: 			return nil, fmt.Errorf("entry not found: %s", input.EntryID)
701: 		}
702: 	}
703: 	// Get feed for context
704: 	feed, err := db.GetFeedByID(s.db, entry.FeedID)
705: 	if err != nil {
706: 		return nil, fmt.Errorf("failed to get feed: %w", err)
707: 	}
708: 	feedTitle := feed.URL
709: 	if feed.Title != nil {
710: 		feedTitle = *feed.Title
711: 	}
712: 	// Convert content to markdown if HTML
713: 	var contentPtr *string
714: 	if entry.Content != nil && *entry.Content != "" {
715: 		markdown := content.ToMarkdown(*entry.Content)
716: 		contentPtr = &markdown
717: 	}
718: 	output := GetEntryOutput{
719: 		ID:          entry.ID,
720: 		FeedID:      entry.FeedID,
721: 		FeedTitle:   feedTitle,
722: 		Title:       entry.Title,
723: 		Link:        entry.Link,
724: 		Author:      entry.Author,
725: 		PublishedAt: entry.PublishedAt,
726: 		Content:     contentPtr,
727: 		Read:        entry.Read,
728: 		ReadAt:      entry.ReadAt,
729: 		CreatedAt:   entry.CreatedAt,
730: 	}
731: 	jsonBytes, err := json.MarshalIndent(output, "", "  ")
732: 	if err != nil {
733: 		return nil, fmt.Errorf("failed to marshal output: %w", err)
734: 	}
735: 	return mcp.NewToolResultText(string(jsonBytes)), nil
736: }
737: func (s *Server) handleMarkRead(_ context.Context, req mcp.CallToolRequest) (*mcp.CallToolResult, error) {
738: 	var input MarkReadInput
739: 	if err := req.BindArguments(&input); err != nil {
740: 		return nil, fmt.Errorf("invalid input: %w", err)
741: 	}
742: 	// Verify entry exists
743: 	if _, err := db.GetEntryByID(s.db, input.EntryID); err != nil {
744: 		return nil, fmt.Errorf("entry not found: %s", input.EntryID)
745: 	}
746: 	// Mark as read
747: 	if err := db.MarkEntryRead(s.db, input.EntryID); err != nil {
748: 		return nil, fmt.Errorf("failed to mark entry as read: %w", err)
749: 	}
750: 	// Reload entry to get updated read_at
751: 	entry, err := db.GetEntryByID(s.db, input.EntryID)
752: 	if err != nil {
753: 		return nil, fmt.Errorf("failed to reload entry: %w", err)
754: 	}
755: 	output := EntryOutput{
756: 		ID:          entry.ID,
757: 		FeedID:      entry.FeedID,
758: 		Title:       entry.Title,
759: 		Link:        entry.Link,
760: 		Author:      entry.Author,
761: 		PublishedAt: entry.PublishedAt,
762: 		Read:        entry.Read,
763: 		ReadAt:      entry.ReadAt,
764: 		CreatedAt:   entry.CreatedAt,
765: 	}
766: 	jsonBytes, err := json.MarshalIndent(output, "", "  ")
767: 	if err != nil {
768: 		return nil, fmt.Errorf("failed to marshal output: %w", err)
769: 	}
770: 	return mcp.NewToolResultText(string(jsonBytes)), nil
771: }
772: func (s *Server) handleMarkUnread(_ context.Context, req mcp.CallToolRequest) (*mcp.CallToolResult, error) {
773: 	var input MarkUnreadInput
774: 	if err := req.BindArguments(&input); err != nil {
775: 		return nil, fmt.Errorf("invalid input: %w", err)
776: 	}
777: 	// Verify entry exists
778: 	if _, err := db.GetEntryByID(s.db, input.EntryID); err != nil {
779: 		return nil, fmt.Errorf("entry not found: %s", input.EntryID)
780: 	}
781: 	// Mark as unread
782: 	if err := db.MarkEntryUnread(s.db, input.EntryID); err != nil {
783: 		return nil, fmt.Errorf("failed to mark entry as unread: %w", err)
784: 	}
785: 	// Reload entry to get updated state
786: 	entry, err := db.GetEntryByID(s.db, input.EntryID)
787: 	if err != nil {
788: 		return nil, fmt.Errorf("failed to reload entry: %w", err)
789: 	}
790: 	output := EntryOutput{
791: 		ID:          entry.ID,
792: 		FeedID:      entry.FeedID,
793: 		Title:       entry.Title,
794: 		Link:        entry.Link,
795: 		Author:      entry.Author,
796: 		PublishedAt: entry.PublishedAt,
797: 		Read:        entry.Read,
798: 		ReadAt:      entry.ReadAt,
799: 		CreatedAt:   entry.CreatedAt,
800: 	}
801: 	jsonBytes, err := json.MarshalIndent(output, "", "  ")
802: 	if err != nil {
803: 		return nil, fmt.Errorf("failed to marshal output: %w", err)
804: 	}
805: 	return mcp.NewToolResultText(string(jsonBytes)), nil
806: }
807: // syncFeed is a helper that fetches and processes a single feed
808: // Returns (newCount, wasCached, error)
809: func (s *Server) syncFeed(feed *models.Feed, force bool) (int, bool, error) {
810: 	// Get cache headers from feed (skip if force)
811: 	var etag, lastModified *string
812: 	if !force {
813: 		etag = feed.ETag
814: 		lastModified = feed.LastModified
815: 	}
816: 	// Fetch the feed
817: 	result, err := fetch.Fetch(feed.URL, etag, lastModified)
818: 	if err != nil {
819: 		// Update error state in database
820: 		if updateErr := db.UpdateFeedError(s.db, feed.ID, err.Error()); updateErr != nil {
821: 			return 0, false, fmt.Errorf("fetch failed (%v) and error update failed: %w", err, updateErr)
822: 		}
823: 		return 0, false, err
824: 	}
825: 	// Handle 304 Not Modified
826: 	if result.NotModified {
827: 		return 0, true, nil
828: 	}
829: 	// Parse the feed
830: 	parsed, err := parse.Parse(result.Body)
831: 	if err != nil {
832: 		errMsg := fmt.Sprintf("failed to parse feed: %v", err)
833: 		if updateErr := db.UpdateFeedError(s.db, feed.ID, errMsg); updateErr != nil {
834: 			return 0, false, fmt.Errorf("parse failed (%v) and error update failed: %w", err, updateErr)
835: 		}
836: 		return 0, false, fmt.Errorf("failed to parse feed: %w", err)
837: 	}
838: 	// Update feed title if empty and persist to database
839: 	titleUpdated := false
840: 	if feed.Title == nil || *feed.Title == "" {
841: 		feed.Title = &parsed.Title
842: 		titleUpdated = true
843: 	}
844: 	// Process entries
845: 	newCount := 0
846: 	for _, parsedEntry := range parsed.Entries {
847: 		// Check if entry already exists
848: 		exists, err := db.EntryExists(s.db, feed.ID, parsedEntry.GUID)
849: 		if err != nil {
850: 			return newCount, false, fmt.Errorf("failed to check entry existence: %w", err)
851: 		}
852: 		if exists {
853: 			continue
854: 		}
855: 		// Create new entry
856: 		entry := models.NewEntry(feed.ID, parsedEntry.GUID, parsedEntry.Title)
857: 		entry.Link = &parsedEntry.Link
858: 		entry.Author = &parsedEntry.Author
859: 		entry.PublishedAt = parsedEntry.PublishedAt
860: 		entry.Content = &parsedEntry.Content
861: 		if err := db.CreateEntry(s.db, entry); err != nil {
862: 			return newCount, false, fmt.Errorf("failed to create entry: %w", err)
863: 		}
864: 		newCount++
865: 	}
866: 	// Update feed fetch state
867: 	fetchedAt := time.Now()
868: 	if err := db.UpdateFeedFetchState(s.db, feed.ID, &result.ETag, &result.LastModified, fetchedAt); err != nil {
869: 		return newCount, false, fmt.Errorf("failed to update feed state: %w", err)
870: 	}
871: 	// If title was updated, persist to database
872: 	if titleUpdated {
873: 		if err := db.UpdateFeed(s.db, feed); err != nil {
874: 			return newCount, false, fmt.Errorf("failed to update feed title: %w", err)
875: 		}
876: 	}
877: 	return newCount, false, nil
878: }
879: func (s *Server) handleBulkMarkRead(_ context.Context, req mcp.CallToolRequest) (*mcp.CallToolResult, error) {
880: 	var input BulkMarkReadInput
881: 	if err := req.BindArguments(&input); err != nil {
882: 		return nil, fmt.Errorf("invalid input: %w", err)
883: 	}
884: 	// Parse the before date
885: 	cutoff, err := parseDateString(input.Before)
886: 	if err != nil {
887: 		return nil, fmt.Errorf("invalid before value: %w", err)
888: 	}
889: 	// Mark entries as read
890: 	count, err := db.MarkEntriesReadBefore(s.db, cutoff)
891: 	if err != nil {
892: 		return nil, fmt.Errorf("failed to mark entries as read: %w", err)
893: 	}
894: 	output := BulkMarkReadOutput{
895: 		Count:  count,
896: 		Before: cutoff,
897: 	}
898: 	if count == 0 {
899: 		output.Message = "No entries to mark as read"
900: 	} else {
901: 		output.Message = fmt.Sprintf("Marked %d entries as read", count)
902: 	}
903: 	jsonBytes, err := json.MarshalIndent(output, "", "  ")
904: 	if err != nil {
905: 		return nil, fmt.Errorf("failed to marshal output: %w", err)
906: 	}
907: 	return mcp.NewToolResultText(string(jsonBytes)), nil
908: }
909: // parseDateString parses a date string that can be a period name or ISO date.
910: func parseDateString(s string) (time.Time, error) {
911: 	// Try period name first
912: 	if t, ok := timeutil.ParsePeriod(s); ok {
913: 		return t, nil
914: 	}
915: 	// Try ISO date format
916: 	if t, err := time.Parse("2006-01-02", s); err == nil {
917: 		return t, nil
918: 	}
919: 	// Try RFC3339
920: 	if t, err := time.Parse(time.RFC3339, s); err == nil {
921: 		return t, nil
922: 	}
923: 	return time.Time{}, fmt.Errorf("cannot parse date: use yesterday, week, month, today, or YYYY-MM-DD format")
924: }
925: // formatFolder returns a human-readable folder name for messages
926: func formatFolder(folder string) string {
927: 	if folder == "" {
928: 		return "root level"
929: 	}
930: 	return fmt.Sprintf("'%s'", folder)
931: }
</file>

<file path="go.mod">
 1: module github.com/harper/digest
 2:
 3: go 1.24.11
 4:
 5: require (
 6: 	github.com/JohannesKaufmann/html-to-markdown/v2 v2.5.0
 7: 	github.com/charmbracelet/glamour v0.10.0
 8: 	github.com/fatih/color v1.18.0
 9: 	github.com/google/uuid v1.6.0
10: 	github.com/mark3labs/mcp-go v0.43.2
11: 	github.com/mmcdole/gofeed v1.3.0
12: 	github.com/spf13/cobra v1.10.1
13: 	golang.org/x/net v0.48.0
14: 	modernc.org/sqlite v1.40.1
15: )
16:
17: require (
18: 	github.com/JohannesKaufmann/dom v0.2.0 // indirect
19: 	github.com/PuerkitoBio/goquery v1.11.0 // indirect
20: 	github.com/alecthomas/chroma/v2 v2.14.0 // indirect
21: 	github.com/andybalholm/cascadia v1.3.3 // indirect
22: 	github.com/aymanbagabas/go-osc52/v2 v2.0.1 // indirect
23: 	github.com/aymerick/douceur v0.2.0 // indirect
24: 	github.com/bahlo/generic-list-go v0.2.0 // indirect
25: 	github.com/buger/jsonparser v1.1.1 // indirect
26: 	github.com/charmbracelet/colorprofile v0.2.3-0.20250311203215-f60798e515dc // indirect
27: 	github.com/charmbracelet/lipgloss v1.1.1-0.20250404203927-76690c660834 // indirect
28: 	github.com/charmbracelet/x/ansi v0.8.0 // indirect
29: 	github.com/charmbracelet/x/cellbuf v0.0.13 // indirect
30: 	github.com/charmbracelet/x/exp/slice v0.0.0-20250327172914-2fdc97757edf // indirect
31: 	github.com/charmbracelet/x/term v0.2.1 // indirect
32: 	github.com/dlclark/regexp2 v1.11.0 // indirect
33: 	github.com/dustin/go-humanize v1.0.1 // indirect
34: 	github.com/gorilla/css v1.0.1 // indirect
35: 	github.com/inconshreveable/mousetrap v1.1.0 // indirect
36: 	github.com/invopop/jsonschema v0.13.0 // indirect
37: 	github.com/json-iterator/go v1.1.12 // indirect
38: 	github.com/lucasb-eyer/go-colorful v1.3.0 // indirect
39: 	github.com/mailru/easyjson v0.7.7 // indirect
40: 	github.com/mattn/go-colorable v0.1.13 // indirect
41: 	github.com/mattn/go-isatty v0.0.20 // indirect
42: 	github.com/mattn/go-runewidth v0.0.16 // indirect
43: 	github.com/microcosm-cc/bluemonday v1.0.27 // indirect
44: 	github.com/mmcdole/goxpp v1.1.1 // indirect
45: 	github.com/modern-go/concurrent v0.0.0-20180306012644-bacd9c7ef1dd // indirect
46: 	github.com/modern-go/reflect2 v1.0.2 // indirect
47: 	github.com/muesli/reflow v0.3.0 // indirect
48: 	github.com/muesli/termenv v0.16.0 // indirect
49: 	github.com/ncruces/go-strftime v1.0.0 // indirect
50: 	github.com/remyoudompheng/bigfft v0.0.0-20230129092748-24d4a6f8daec // indirect
51: 	github.com/rivo/uniseg v0.4.7 // indirect
52: 	github.com/spf13/cast v1.7.1 // indirect
53: 	github.com/spf13/pflag v1.0.9 // indirect
54: 	github.com/wk8/go-ordered-map/v2 v2.1.8 // indirect
55: 	github.com/xo/terminfo v0.0.0-20220910002029-abceb7e1c41e // indirect
56: 	github.com/yosida95/uritemplate/v3 v3.0.2 // indirect
57: 	github.com/yuin/goldmark v1.7.13 // indirect
58: 	github.com/yuin/goldmark-emoji v1.0.5 // indirect
59: 	golang.org/x/exp v0.0.0-20251209150349-8475f28825e9 // indirect
60: 	golang.org/x/sys v0.39.0 // indirect
61: 	golang.org/x/term v0.38.0 // indirect
62: 	golang.org/x/text v0.32.0 // indirect
63: 	gopkg.in/yaml.v3 v3.0.1 // indirect
64: 	modernc.org/libc v1.67.1 // indirect
65: 	modernc.org/mathutil v1.7.1 // indirect
66: 	modernc.org/memory v1.11.0 // indirect
67: )
</file>

</files>
